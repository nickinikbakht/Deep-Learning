{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW4_nikbakht.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UltnPIJvWOdb"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<h1>\r\n",
        "import موارد لازم\r\n",
        "</h1>\r\n",
        "<p>\r\n",
        "یک تعدادی از موارد که ایمپورت شده اند در تمرین دوباره فراخوانی شده اند از طرفی ممکن است برخی استفاده نشده باشند چرا که یک سری از سل های این تمرین پاک شده اند.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xo02O1JtJU5g",
        "outputId": "efeb9d75-9e03-466e-ff35-d50aab23a075"
      },
      "source": [
        "!pip install tensorflow-addons"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.6/dist-packages (0.8.3)\n",
            "Requirement already satisfied: typeguard in /usr/local/lib/python3.6/dist-packages (from tensorflow-addons) (2.7.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4ASKc-qyIbF",
        "outputId": "d5424bd2-9a0b-4aa2-a06b-007509d55f27"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "print(tf.__version__)\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "import unicodedata\n",
        "import urllib3\n",
        "import shutil\n",
        "import zipfile\n",
        "import itertools\n",
        "from google.colab import drive\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykdLYrhIXbMO"
      },
      "source": [
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dj4kDq33K8Kv"
      },
      "source": [
        "## Neural Machine Translation\n",
        "\n",
        "In this assignment you are going to practice sequence to sequence models.\n",
        "\n",
        "You will build a Neural Machine Translation (NMT) model to translate human readable dates (\"۲۰ دی ۱۳۹۹\") into machine readable dates (\"1399-10-20\").\n",
        "\n",
        "Let's import needed modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZfKIbP9K8K1"
      },
      "source": [
        "The dataset contains 20000 data for training. Run the code below, to load and see first 10 samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zv8cS5DIXywo"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<h1>\r\n",
        "آپلود دیتاست ها\r\n",
        "</h1>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erBPkDp_K8K1",
        "outputId": "5fdb5678-ffc6-4db7-bc61-11e7439b0b8a"
      },
      "source": [
        "# Path to 'TrainSet' file\n",
        "trainSetFile = \"TrainSet.pickle\"\n",
        "\n",
        "# Openning the file, and load contents\n",
        "with open(trainSetFile, 'rb') as file:\n",
        "    trainSet = pickle.load(file)\n",
        "\n",
        "# Samples\n",
        "print(\"Number of samples in train set:\", len(trainSet), \"\\n\")\n",
        "for i in range(10):\n",
        "    print(\"Human Input:\", trainSet[i][0], \"\\t\\tMachine Readable:\", trainSet[i][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples in train set: 20000 \n",
            "\n",
            "Human Input: شنبه مرداد ۹ ۱۳۷۸ \t\tMachine Readable: 1378-05-09\n",
            "Human Input: ۱۰ ۱۱ ۹۹ \t\tMachine Readable: 1399-11-10\n",
            "Human Input: ۱۰.۰۹.۵۰ \t\tMachine Readable: 1350-09-10\n",
            "Human Input: تیر ماه 28 هزار و سیصد و هفتاد \t\tMachine Readable: 1370-04-28\n",
            "Human Input: پنج شنبه فروردین ماه 26 هزار و سیصد و هفتاد و پنج \t\tMachine Readable: 1375-01-26\n",
            "Human Input: ۳/۷/۶۳ \t\tMachine Readable: 1363-03-07\n",
            "Human Input: مرداد ۲۲ هزار و سیصد و شصت و هشت \t\tMachine Readable: 1368-05-22\n",
            "Human Input: هشتم مهر ۱۳۸۸ \t\tMachine Readable: 1388-07-08\n",
            "Human Input: هشت آذر هزار و سیصد و هفتاد و نه \t\tMachine Readable: 1379-09-08\n",
            "Human Input: فروردین ماه ۱ ۱۳۶۱ \t\tMachine Readable: 1361-01-01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-lmjtOcK8K2",
        "outputId": "bedc48f3-3947-4456-9ed8-acb733a8c564"
      },
      "source": [
        "# Path to 'ValidationSet' file\n",
        "validationSetFile = \"ValidationSet.pickle\"\n",
        "\n",
        "# Openning the file, and load contents\n",
        "with open(validationSetFile, 'rb') as file:\n",
        "    validationSet = pickle.load(file)\n",
        "print(\"Number of samples in validation set:\", len(validationSet), \"\\n\")\n",
        "\n",
        "\n",
        "# Path to 'TestSet' file\n",
        "testSetFile = \"TestSet.pickle\"\n",
        "\n",
        "# Openning the file, and load contents\n",
        "with open(testSetFile, 'rb') as file:\n",
        "    testSet = pickle.load(file)\n",
        "print(\"Number of samples in test set:\", len(testSet))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples in validation set: 5000 \n",
            "\n",
            "Number of samples in test set: 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4q6Yn0Y18gac"
      },
      "source": [
        "### Preprocess File"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMPJRCXdjBQt"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<h1>\r\n",
        "پیش پردازش\r\n",
        "</h1>\r\n",
        "<p>\r\n",
        "با توجه به این که شبکه را چگونه می خواهیم بسازیم باید ابتدا پیش پردازشی روی داده ها انجام دهیم تا داده ها برای پردازش در شبکه آماده شوند.\r\n",
        "<br>\r\n",
        "ما می توانیم به دو صورت شبکه را بسازیم:\r\n",
        "</p>\r\n",
        "<ul>\r\n",
        "  <li>با پردازش لغات (حروفی که با space از یک دیگر جدا شده اند)</li>\r\n",
        "  <li>با پردازش حروف</li>\r\n",
        "</ul>\r\n",
        "<p>\r\n",
        "ما در این تمرین ابتدا شبکه ای می سازیم مبتنی بر کلمات، سپس شبکه ای مبتنی بر حروف می سازیم.<br> در این قسمت پیش پردازشی برای شبکه مبتنی برکلمات را مشاهده می کنیم:\r\n",
        "</p>\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVM5nnmb81UE"
      },
      "source": [
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3syZHkX83Lx"
      },
      "source": [
        "# For non-english characterset translations such as Hindi, Russian, etc. we keep unicode. \n",
        "def preprocess_sentence(s):\n",
        "    s = unicode_to_ascii(s.lower().strip())\n",
        "    s = s.lower().strip()\n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "    s = re.sub(r'[\" \"]+', \" \", s)\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    #s = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", s)\n",
        "    s = re.sub(r\"-\", \" \", s)\n",
        "    s = s.rstrip().strip()\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    s = '<start> ' + s + ' <end>'\n",
        "    return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "QUWzK3TQ8KeV",
        "outputId": "1e59fc7c-e34e-489b-944c-447c395a0fdc"
      },
      "source": [
        "preprocess_sentence(\"تیر ماه 28 هزار و سیصد و هفتاد\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> تیر ماه 28 هزار و سیصد و هفتاد <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 380
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2k9fujVls8s"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "چون شبکه مبتنی بر کلمه است و مبنای تشخیص کلمات(space) است بنابراین در خروجی انسانی مجموعه داده ها چون فرم بصورت 1384-01-5 است یک کلمه حساب می شود ما برای رفع این مشکل - را از میان اعداد حذف کردیم و بنابراین خروجی شامل پنج کلمه شد که کلمه اول و آخر start و end است.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "2xq3h8qe8Xsc",
        "outputId": "dc8c0897-cba8-45c1-8fe8-861b436a0838"
      },
      "source": [
        "preprocess_sentence('1378-05-09')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'<start> 1378 05 09 <end>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 381
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOGq0hRk9tad"
      },
      "source": [
        "### Create Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWBj8iis9192"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "تابع tokenize را برای تبدیل همه واژگان به یک فهرست واژگان، می سازیم. <br>\r\n",
        " که از توابع قبلا تعریف شده در کتابخانه ها استفاده می کنند.\r\n",
        " <br>\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbrbbW0uAUVe"
      },
      "source": [
        "def tokenize(input):\n",
        "   tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "   tokenizer.fit_on_texts(input)\n",
        "   sequences = tokenizer.texts_to_sequences(input)\n",
        "   sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, padding='post')\n",
        "   return  sequences, tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0vN7jT9Aijw"
      },
      "source": [
        "def max_len(tensor):\n",
        "    #print( np.argmax([len(t) for t in tensor]))\n",
        "    return max( len(t) for t in tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljv2IiLgnBA3"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>با استفاده از توابع تعریف شده در بالا\r\n",
        "مجموعه داده های ترین و ولید و تست را در این قسمت  می سازیم.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4og5aKKiz2Ub"
      },
      "source": [
        "h=[]\r\n",
        "m=[]\r\n",
        "da=[]\r\n",
        "for i in trainSet:\r\n",
        "   h1,m1= i\r\n",
        "   if h is not None:\r\n",
        "      h.append(preprocess_sentence(h1))\r\n",
        "      m.append(preprocess_sentence(m1))\r\n",
        "      da.append((h1, m1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hk3x5kdkhUHa",
        "outputId": "79693c52-2f02-4e56-ecb2-c4b6d87e9577"
      },
      "source": [
        "#total samples\n",
        "print(\"Total Samples : \", len(da))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Samples :  20000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYaBvTgtEK-z"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "داده های ترین\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c42RcXojAmdZ"
      },
      "source": [
        "# Tokenize each word into index and return the tokenized list and tokenizer\n",
        "X , X_tokenizer = tokenize(h)\n",
        "Y, Y_tokenizer = tokenize(m)\n",
        "\n",
        "Tx = max_len(X)\n",
        "Ty = max_len(Y)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muhXwcQ7JPMV",
        "outputId": "d2a0585f-2395-4755-9873-9ff86aa2e502"
      },
      "source": [
        "print(\"Max length human sequence denoted as Tx : \", Tx)\n",
        "print(\"Max length machine sequence denoted as Ty: \", Ty)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max length human sequence denoted as Tx :  14\n",
            "Max length machine sequence denoted as Ty:  5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGDAC5YgDk52"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "به هر کلمه با استفاده از تعدا تکرار که در دیتا ست دارد عددی تعلق می گیرد هر چه فراوانی بیشتر عدد کوچکتر است.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmCJzO38Apcn",
        "outputId": "ea52c222-0b81-474b-dbd4-0c9d42ea82a7"
      },
      "source": [
        "X_tokenizer.word_index['<start>'] #'<start>': 2   # tokenize by frequency\n",
        "input_vocab_size = len(X_tokenizer.word_index)+1  # add 1 for 0 sequence character\n",
        "output_vocab_size = len(Y_tokenizer.word_index)+ 1\n",
        "print(\"input_vocab_size : \", input_vocab_size)\n",
        "print(\"output_vocab_size : \" ,output_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_vocab_size :  1227\n",
            "output_vocab_size :  86\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SDQu5I6Ek_c"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "داده های ولیدیشن\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yg7njkSYnT_t"
      },
      "source": [
        "hv=[]\r\n",
        "mv=[]\r\n",
        "dav=[]\r\n",
        "for i in validationSet:\r\n",
        "   h1,m1= i\r\n",
        "   if h is not None:\r\n",
        "      hv.append(preprocess_sentence(h1))\r\n",
        "      mv.append(preprocess_sentence(m1))\r\n",
        "      dav.append((h1, m1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPKH1Sn_nT_-"
      },
      "source": [
        "# Tokenize each word into index and return the tokenized list and tokenizer\r\n",
        "Xv , Xv_tokenizer = tokenize(hv)\r\n",
        "Yv, Yv_tokenizer = tokenize(mv)\r\n",
        "#X_train,  X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.2)\r\n",
        "\r\n",
        "Txv = max_len(Xv)\r\n",
        "Tyv = max_len(Yv)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvXzJQ6xFOck"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "داده های تست\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS8Ia-u8h7Op"
      },
      "source": [
        "ht=[]\r\n",
        "mt=[]\r\n",
        "dat=[]\r\n",
        "for i in testSet:\r\n",
        "   h1,m1= i\r\n",
        "   if h is not None:\r\n",
        "      ht.append(preprocess_sentence(h1))\r\n",
        "      mt.append(preprocess_sentence(m1))\r\n",
        "      dat.append((h1, m1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc9JPtgQh7Oq"
      },
      "source": [
        "# Tokenize each word into index and return the tokenized list and tokenizer\r\n",
        "Xt , Xt_tokenizer = tokenize(ht)\r\n",
        "Yt, Yt_tokenizer = tokenize(mt)\r\n",
        "\r\n",
        "Txt = max_len(Xt)\r\n",
        "Tyt= max_len(Yt)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLXYtWdZBW6a"
      },
      "source": [
        "### Model Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBX7mjjzFbbb"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "تعریف پارامتر ها :\r\n",
        "<ul>\r\n",
        "  <li>بچ سایز: در درس متوجه شدیم که خوب است مینی بچ هایی تعریف شود و همه داده ها را هم زمان به مدل نشان ندهیم برای تعریف این مینی بچ ها بهتر است عدد مضربی از 2 باشد بنابر این کاندیدای ما 8 و 16 و 32 و 64 و 128 است چرا که اعداد بزرگتر و کوچکتر از این مقادیر مناسب نیستند. اگر عدد کوچکتر بگیریم شبیه خیلی عدد کم است و مقادیر 2 و 4 تایی برای داده های بسیار حجیم مثل تصاویر سه بعدی مناسب است. مقادیر بزرگتر از 128 هم برای داده های بسیار کم حجم مناسب است البته ممکن است مدل در آن شرایط هم خوب کار کند اما نیاز داریم برای این بررسی چندین بار دیگر مدل را ترین کنیم که با توجه به این که زمان بر است ما فرصت این کار را نداریم.\r\n",
        "  عدد 32 را که ما بین این اعداد بود را انتخاب کردیم و یک بار هم مدل را بری عدد 64 ترین کردیم تفاوت چشم گیر نبود اما نتایج برای عدد 32 بهتر بود  و بنا بر این ما عدد 32 را انتخاب کردیم. اما لازم به ذکر است که روش مناسب تر این است که برای اعداد ذکر شده هر کدام چند بار مدل را ترین کنیم و نتیجه را با میانگین گیری مشخص کنیم. البته باید توجه داشته باشیم که تغییر پایپر پارامتر های دیگر نیز این نتایج تاثیر می گذارد.  </li>\r\n",
        "  <li>\r\n",
        "  امبددینگ سایز : برای این مورد هیچ مقدار مشابه در پروژه های قبلی ندیدم و برای همین مقدار 256 که برای کاربرد ترجمه بود را امتحان کردم و جواب داد ولی آن چه که واضح است اینجا کاملا کاربرد متفاوت است و ابعاد باید متفاوت و کمتر باشند به همین دلیل عدد 10 و 20 را امتحان کردیم ولی نتایج روی سه مجموعه (آموزشی ، ولید و تست) به مقدار قابل ملاحظه ای بدتر بود. بنابر این همان عدد 256 را گرفتیم. البته عدد 100 را قبلا امتحان کردیم و نتایج خوبی داشت ولی چون نتایج اش پاک کردیم، تصمیم گرفتم که مقادیری که نتایج اش را دارم قرار دهم.\r\n",
        "  </li>\r\n",
        "\r\n",
        "  <li>rnn_units , dense_units: مقدار 1024 خیلی خیلی بزرگ است برای این سوال ولی ما مدل را اجرا کردیم و خطا خیلی زیاد نبود. ولی با توجه به این که به خروجی مرتبط است rnn_units و اینجا خروجی ثابت است (سه کلمه به همراه نقطه شروع و پایان) می توان انتظار داشت عدد 5 هم جواب خوبی برگرداند. با این حال عدد بسیار بزرگی مانند 1024 به مدل درجه آزادی بیشتر و محاسبات بیشتر تحمیل می کند اما محدودیتی برای مدل ایجاد نمی کند.\r\n",
        "  اما وقتی عدد کوچک تر بگیریم در تعداد اپوچ کمتر ولی زمان بیشتر به دقت مناسب می رسد\r\n",
        "  \r\n",
        "  در ادامه برای 5 با 10 اپوچ نتیجه را می بینیم برای 1024 با 2 اپوچ\r\n",
        "    </li>\r\n",
        "\r\n",
        "</ul>\r\n",
        "\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFUu1izSBVIy"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = len(h)\n",
        "steps_per_epoch = BUFFER_SIZE//BATCH_SIZE\n",
        "embedding_dims = 256\n",
        "rnn_units = 100\n",
        "dense_units = 100\n",
        "Dtype = tf.float32   #used to initialize DecoderCell Zero state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxQB7MaAh6tQ",
        "outputId": "42f320b9-d106-4170-8f28-a3912f6f832f"
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((X, Y)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "example_X, example_Y = next(iter(dataset))\r\n",
        "print(example_X.shape) \r\n",
        "print(example_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 14)\n",
            "(64, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wgJGrVth7Oq",
        "outputId": "4383ac63-e0b2-4c40-8123-ba059fddc96f"
      },
      "source": [
        "dataset_v = tf.data.Dataset.from_tensor_slices((Xv, Yv)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "example_Xv, example_Yv = next(iter(dataset_v))\r\n",
        "print(example_Xv.shape) \r\n",
        "print(example_Yv.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 14)\n",
            "(64, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzmgNcWAgZ2y",
        "outputId": "cce37557-b18b-4a78-ae41-05b3f249adfa"
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xt, Yt)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "example_Xt, example_Yt = next(iter(dataset_test))\n",
        "print(example_Xt.shape) \n",
        "print(example_Yt.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 14)\n",
            "(64, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmYJjp2igZ3A",
        "outputId": "0379163d-64ff-4d11-f49b-e155f406d6c1"
      },
      "source": [
        "dataset_v = tf.data.Dataset.from_tensor_slices((Xv, Yv)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\r\n",
        "example_Xv, example_Yv = next(iter(dataset_v))\r\n",
        "print(example_Xv.shape) \r\n",
        "print(example_Yv.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 14)\n",
            "(64, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqFYB-g1HM5o"
      },
      "source": [
        "### Creating Encoder-Decoder Model based on tfa.seq2seq module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Swb9mvflqsQ"
      },
      "source": [
        "### Define Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWxPNQirBtsh"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\n",
        "<p>\n",
        "با توجه به امکانات که کتابخانه های موجود به ما می دهند ما می توانیم در سه سطح متفاوت رگیولاریزیشن را برای جلوگیری از بیش برازش استفاده کنیم: \n",
        "<ul>\n",
        "  <li>kernel_regularizer</li>\n",
        "  <li>bias_regularizer</li>\n",
        "  <li>activity_regularizer</li>\n",
        "</ul>\n",
        "این موارد از اسمشان نیز مشخص است که رگیلاریزشن را در کدام لایه اعمال می کنند برای جلوگیری از بیش برازش می توانیم از این سه در مدل استفاده کنیم به عنوان مثال در lstm می توان هر سه را مقدار دهی کرد و نتیجه را بررسی کرد ما در مدل این موارد را اعمال نکردیم (به علت محدودیت زمان و سیستم)\n",
        "اما شما می توانید بررسی کنید.\n",
        "\n",
        "برای جلوگیری از بیش برازش در مدل در مقاله زیر با توجه به کاربرد توضیحاتی آمده است.\n",
        "https://arxiv.org/abs/1409.2329 <br>\n",
        "Recurrent Neural Network Regularization\n",
        "</p>\n",
        "<p>\n",
        "<h3>ساخت مدل<h3>\n",
        "<dl><ol>\n",
        "<li>\n",
        "  <dt>\n",
        "  شبکه encoder از یک لایه encoder embedding و یک لایه LSTM تشکیل شده است.</dt>\n",
        "  <dd>\n",
        "در امبددینگ از رگیلاریزیشن استفاده شده است  که جلوی بیش برازش را می گیرد اما مسئله ای که هست این است که باید این پارامتر را تنظیم کرد باز می توان مقادیر مختلف را به روش های گوناگون (انتخاب رندوم و امتحان اعداد به صورت لوگاریتمی 0.0001 و 0.0001 و 0.001 و 0.01 و 0.1 و 1و  10 و...  => هر بار سعی مکنیم بازه را کوچکتر کنیم تا به مقدار مناسب برسیم ) امتحان کرد. باید توجه شود که مقادیر دیگر هایپر پارامتر می توانند روی این مقادیر تاثیر بگذارند و بنا بر این باید با تغیبر یکی دیگر از آن ها اینجا نیز بار دیگر تنظیم شود. اما چون زمان ما محدود است ما به مقدار tf.keras.regularizers.L2(0.001) برای embeddings matrix که نتیجه خوب ارائه داده بسنده می کنیم.\n",
        "</dd>\n",
        "</li>\n",
        "<li>\n",
        "  <dt>شبکه decoder  هر دو decoder  و attention mechanism را در بر می گیرد.</dt>\n",
        "  <dd> برای پیاده سازی attention mechanism را می توان از مدل های مختلف استفاده کرد که دو مورد را در این قسمت امتحان کردیم و نهایتا یکی را انتخاب کردیم اما از آنجا که در درس اشاره نشده به این روش ها ما زمان زیادی رو این موارد نمی گذاریم. در این مدل ما  از LuongAtention و AttentionWrapper استفاده می کنم.\n",
        "  </dd>\n",
        "</dl>\n",
        "</ol>\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJXNQ4Vh26t9"
      },
      "source": [
        "#ENCODER\n",
        "class EncoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = tf.keras.layers.Embedding(input_dim=input_vocab_size,\n",
        "                                                           output_dim=embedding_dims,embeddings_regularizer=tf.keras.regularizers.L2(0.001))\n",
        "        self.encoder_rnnlayer = tf.keras.layers.LSTM(rnn_units,return_sequences=True, \n",
        "                                                     return_state=True )\n",
        "    \n",
        "#DECODER\n",
        "class DecoderNetwork(tf.keras.Model):\n",
        "    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n",
        "        super().__init__()\n",
        "        self.decoder_embedding = tf.keras.layers.Embedding(input_dim=output_vocab_size,\n",
        "                                                           output_dim=embedding_dims) \n",
        "        self.dense_layer = tf.keras.layers.Dense(output_vocab_size)\n",
        "        self.decoder_rnncell = tf.keras.layers.LSTMCell(rnn_units)\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n",
        "        self.rnn_cell =  self.build_rnn_cell(BATCH_SIZE)\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler= self.sampler,\n",
        "                                                output_layer=self.dense_layer)\n",
        "\n",
        "    def build_attention_mechanism(self, units,memory, memory_sequence_length):\n",
        "        return tfa.seq2seq.LuongAttention(units, memory = memory, \n",
        "                                          memory_sequence_length=memory_sequence_length)\n",
        "        #return tfa.seq2seq.BahdanauAttention(units, memory = memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    # wrap decodernn cell  \n",
        "    def build_rnn_cell(self, batch_size ):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, self.attention_mechanism,\n",
        "                                                attention_layer_size=dense_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_decoder_initial_state(self, batch_size, encoder_state,Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n",
        "                                                                dtype = Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "        return decoder_initial_state\n",
        "\n",
        "\n",
        "\n",
        "encoderNetwork = EncoderNetwork(input_vocab_size,embedding_dims, rnn_units)\n",
        "decoderNetwork = DecoderNetwork(output_vocab_size,embedding_dims, rnn_units)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfJTg36aCckr"
      },
      "source": [
        "### Optimizer and Custom Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc7yp0FWEedf"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00WV880hoVMc"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "تابع loss برای اندازه گیری خطا تعریف می کینم .<br>\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pT6hAEFEhVZ"
      },
      "source": [
        "def loss_function(y_pred, y):\n",
        "   \n",
        "    #shape of y [batch_size, ty]\n",
        "    #shape of y_pred [batch_size, Ty, output_vocab_size] \n",
        "    sparsecategoricalcrossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
        "                                                                                  reduction='none')\n",
        "    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlOVNNrCC5_-"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "برای شروع ، attention_mechanism بدون حافظه آغاز \r\n",
        "می شود.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdhtJ4YT46mh",
        "outputId": "184296aa-6f5f-41c4-b30a-2d30f9eaffd6"
      },
      "source": [
        "decoderNetwork.attention_mechanism.memory_initialized"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 401
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f8vdgaZFAah"
      },
      "source": [
        "### One step of training on a batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGIU4xhyb5uR"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "تعریف تابع برای ترین کردن\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm3HkY9nExNB"
      },
      "source": [
        "\n",
        "def train_step(input_batch, output_batch,encoder_initial_cell_state):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                        initial_state =encoder_initial_cell_state)\n",
        "\n",
        "        #[last step activations,last memory_state] of encoder passed as input to decoder Network\n",
        "        \n",
        "         \n",
        "        # Prepare correct Decoder input & output sequence data\n",
        "        decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "        #compare logits with timestepped +1 version of decoder_input\n",
        "        decoder_output = output_batch[:,1:] #ignore <start>\n",
        "\n",
        "\n",
        "        # Decoder Embeddings\n",
        "        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "        #Setting up decoder memory from encoder output and Zero State for AttentionWrapperState\n",
        "        decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                           encoder_state=[a_tx, c_tx],\n",
        "                                                                           Dtype=tf.float32)\n",
        "        \n",
        "        #BasicDecoderOutput        \n",
        "        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                               sequence_length=BATCH_SIZE*[Ty-1])\n",
        "\n",
        "        logits = outputs.rnn_output\n",
        "        #Calculate loss\n",
        "\n",
        "        loss = loss_function(logits, decoder_output)\n",
        "\n",
        "    #Returns the list of all layer variables / weights.\n",
        "    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n",
        "    # differentiate loss wrt variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    #grads_and_vars – List of(gradient, variable) pairs.\n",
        "    grads_and_vars = zip(gradients,variables)\n",
        "    optimizer.apply_gradients(grads_and_vars)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr_GRb1JHp6b"
      },
      "source": [
        "### Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVX6Hyx0S3tw"
      },
      "source": [
        " import tensorflow as tf\n",
        "#RNN LSTM hidden and memory state initializer\n",
        "def initialize_initial_state():\n",
        "        return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-DK641uFe1j"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "در پایین نتایج را برای <br>\r\n",
        "rnn_units = 5<br>\r\n",
        "dense_units = 5<br>\r\n",
        "می بینیم با این که 10 epoch بوده است اما نتایج چندان خوب نیست اما با افزایش تعداد اپوچ نتیجه خیلی خوب هم حاصل می شود.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blMQ9KDTE3s4",
        "outputId": "99a7221b-e5ec-4a73-e53b-0582f211e01b"
      },
      "source": [
        "epochs = 10\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%20 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 2.3450582027435303 epoch 1 batch 20 \n",
            "total loss: 2.30682110786438 epoch 1 batch 40 \n",
            "total loss: 2.3233470916748047 epoch 1 batch 60 \n",
            "total loss: 2.310060501098633 epoch 1 batch 80 \n",
            "total loss: 2.3372867107391357 epoch 1 batch 100 \n",
            "total loss: 2.2338645458221436 epoch 1 batch 120 \n",
            "total loss: 2.2833166122436523 epoch 1 batch 140 \n",
            "total loss: 2.2206034660339355 epoch 1 batch 160 \n",
            "total loss: 2.2111458778381348 epoch 1 batch 180 \n",
            "total loss: 2.2042877674102783 epoch 1 batch 200 \n",
            "total loss: 2.172658920288086 epoch 1 batch 220 \n",
            "total loss: 2.240938663482666 epoch 1 batch 240 \n",
            "total loss: 2.1468193531036377 epoch 1 batch 260 \n",
            "total loss: 2.165107011795044 epoch 1 batch 280 \n",
            "total loss: 2.1230263710021973 epoch 1 batch 300 \n",
            "total loss: 2.1133880615234375 epoch 2 batch 20 \n",
            "total loss: 2.114091396331787 epoch 2 batch 40 \n",
            "total loss: 2.1021080017089844 epoch 2 batch 60 \n",
            "total loss: 2.0851693153381348 epoch 2 batch 80 \n",
            "total loss: 2.036990165710449 epoch 2 batch 100 \n",
            "total loss: 2.04814076423645 epoch 2 batch 120 \n",
            "total loss: 2.066699981689453 epoch 2 batch 140 \n",
            "total loss: 2.027148962020874 epoch 2 batch 160 \n",
            "total loss: 2.012937068939209 epoch 2 batch 180 \n",
            "total loss: 2.0313258171081543 epoch 2 batch 200 \n",
            "total loss: 2.0185787677764893 epoch 2 batch 220 \n",
            "total loss: 1.9865641593933105 epoch 2 batch 240 \n",
            "total loss: 1.960594892501831 epoch 2 batch 260 \n",
            "total loss: 1.9580094814300537 epoch 2 batch 280 \n",
            "total loss: 1.9340500831604004 epoch 2 batch 300 \n",
            "total loss: 1.976649284362793 epoch 3 batch 20 \n",
            "total loss: 1.9037342071533203 epoch 3 batch 40 \n",
            "total loss: 1.9439529180526733 epoch 3 batch 60 \n",
            "total loss: 1.9312617778778076 epoch 3 batch 80 \n",
            "total loss: 1.9165130853652954 epoch 3 batch 100 \n",
            "total loss: 1.9269672632217407 epoch 3 batch 120 \n",
            "total loss: 1.8960684537887573 epoch 3 batch 140 \n",
            "total loss: 1.874267339706421 epoch 3 batch 160 \n",
            "total loss: 1.8852646350860596 epoch 3 batch 180 \n",
            "total loss: 1.896050214767456 epoch 3 batch 200 \n",
            "total loss: 1.8800787925720215 epoch 3 batch 220 \n",
            "total loss: 1.891770839691162 epoch 3 batch 240 \n",
            "total loss: 1.8608378171920776 epoch 3 batch 260 \n",
            "total loss: 1.8861215114593506 epoch 3 batch 280 \n",
            "total loss: 1.8496650457382202 epoch 3 batch 300 \n",
            "total loss: 1.849542260169983 epoch 4 batch 20 \n",
            "total loss: 1.845791220664978 epoch 4 batch 40 \n",
            "total loss: 1.847975492477417 epoch 4 batch 60 \n",
            "total loss: 1.8419899940490723 epoch 4 batch 80 \n",
            "total loss: 1.8516440391540527 epoch 4 batch 100 \n",
            "total loss: 1.830756425857544 epoch 4 batch 120 \n",
            "total loss: 1.807370662689209 epoch 4 batch 140 \n",
            "total loss: 1.8096764087677002 epoch 4 batch 160 \n",
            "total loss: 1.7938735485076904 epoch 4 batch 180 \n",
            "total loss: 1.8767274618148804 epoch 4 batch 200 \n",
            "total loss: 1.8164492845535278 epoch 4 batch 220 \n",
            "total loss: 1.7634391784667969 epoch 4 batch 240 \n",
            "total loss: 1.7529065608978271 epoch 4 batch 260 \n",
            "total loss: 1.802119493484497 epoch 4 batch 280 \n",
            "total loss: 1.7788608074188232 epoch 4 batch 300 \n",
            "total loss: 1.7813091278076172 epoch 5 batch 20 \n",
            "total loss: 1.7311973571777344 epoch 5 batch 40 \n",
            "total loss: 1.7134922742843628 epoch 5 batch 60 \n",
            "total loss: 1.709533452987671 epoch 5 batch 80 \n",
            "total loss: 1.7206811904907227 epoch 5 batch 100 \n",
            "total loss: 1.754095435142517 epoch 5 batch 120 \n",
            "total loss: 1.7338708639144897 epoch 5 batch 140 \n",
            "total loss: 1.7316150665283203 epoch 5 batch 160 \n",
            "total loss: 1.716718316078186 epoch 5 batch 180 \n",
            "total loss: 1.6802947521209717 epoch 5 batch 200 \n",
            "total loss: 1.6894747018814087 epoch 5 batch 220 \n",
            "total loss: 1.6852221488952637 epoch 5 batch 240 \n",
            "total loss: 1.7074294090270996 epoch 5 batch 260 \n",
            "total loss: 1.7212902307510376 epoch 5 batch 280 \n",
            "total loss: 1.6910326480865479 epoch 5 batch 300 \n",
            "total loss: 1.688018560409546 epoch 6 batch 20 \n",
            "total loss: 1.6663906574249268 epoch 6 batch 40 \n",
            "total loss: 1.6180360317230225 epoch 6 batch 60 \n",
            "total loss: 1.6327811479568481 epoch 6 batch 80 \n",
            "total loss: 1.6660444736480713 epoch 6 batch 100 \n",
            "total loss: 1.6778583526611328 epoch 6 batch 120 \n",
            "total loss: 1.6451822519302368 epoch 6 batch 140 \n",
            "total loss: 1.6627792119979858 epoch 6 batch 160 \n",
            "total loss: 1.6365121603012085 epoch 6 batch 180 \n",
            "total loss: 1.6513348817825317 epoch 6 batch 200 \n",
            "total loss: 1.63725745677948 epoch 6 batch 220 \n",
            "total loss: 1.6965245008468628 epoch 6 batch 240 \n",
            "total loss: 1.6147969961166382 epoch 6 batch 260 \n",
            "total loss: 1.6138997077941895 epoch 6 batch 280 \n",
            "total loss: 1.6357324123382568 epoch 6 batch 300 \n",
            "total loss: 1.6287624835968018 epoch 7 batch 20 \n",
            "total loss: 1.6376075744628906 epoch 7 batch 40 \n",
            "total loss: 1.576920509338379 epoch 7 batch 60 \n",
            "total loss: 1.6274231672286987 epoch 7 batch 80 \n",
            "total loss: 1.5889004468917847 epoch 7 batch 100 \n",
            "total loss: 1.635927438735962 epoch 7 batch 120 \n",
            "total loss: 1.5904284715652466 epoch 7 batch 140 \n",
            "total loss: 1.602360486984253 epoch 7 batch 160 \n",
            "total loss: 1.5789655447006226 epoch 7 batch 180 \n",
            "total loss: 1.5800732374191284 epoch 7 batch 200 \n",
            "total loss: 1.5829050540924072 epoch 7 batch 220 \n",
            "total loss: 1.5891145467758179 epoch 7 batch 240 \n",
            "total loss: 1.5614274740219116 epoch 7 batch 260 \n",
            "total loss: 1.5712945461273193 epoch 7 batch 280 \n",
            "total loss: 1.5898683071136475 epoch 7 batch 300 \n",
            "total loss: 1.5425426959991455 epoch 8 batch 20 \n",
            "total loss: 1.5590131282806396 epoch 8 batch 40 \n",
            "total loss: 1.5900874137878418 epoch 8 batch 60 \n",
            "total loss: 1.575141429901123 epoch 8 batch 80 \n",
            "total loss: 1.5196911096572876 epoch 8 batch 100 \n",
            "total loss: 1.580432653427124 epoch 8 batch 120 \n",
            "total loss: 1.5680358409881592 epoch 8 batch 140 \n",
            "total loss: 1.5339763164520264 epoch 8 batch 160 \n",
            "total loss: 1.5397478342056274 epoch 8 batch 180 \n",
            "total loss: 1.5193337202072144 epoch 8 batch 200 \n",
            "total loss: 1.5745781660079956 epoch 8 batch 220 \n",
            "total loss: 1.522308349609375 epoch 8 batch 240 \n",
            "total loss: 1.5051754713058472 epoch 8 batch 260 \n",
            "total loss: 1.5028852224349976 epoch 8 batch 280 \n",
            "total loss: 1.5550761222839355 epoch 8 batch 300 \n",
            "total loss: 1.5343563556671143 epoch 9 batch 20 \n",
            "total loss: 1.5151480436325073 epoch 9 batch 40 \n",
            "total loss: 1.5263864994049072 epoch 9 batch 60 \n",
            "total loss: 1.514538049697876 epoch 9 batch 80 \n",
            "total loss: 1.50618577003479 epoch 9 batch 100 \n",
            "total loss: 1.5810571908950806 epoch 9 batch 120 \n",
            "total loss: 1.531570315361023 epoch 9 batch 140 \n",
            "total loss: 1.5174787044525146 epoch 9 batch 160 \n",
            "total loss: 1.5258758068084717 epoch 9 batch 180 \n",
            "total loss: 1.500573992729187 epoch 9 batch 200 \n",
            "total loss: 1.5014488697052002 epoch 9 batch 220 \n",
            "total loss: 1.4641814231872559 epoch 9 batch 240 \n",
            "total loss: 1.4514551162719727 epoch 9 batch 260 \n",
            "total loss: 1.4965711832046509 epoch 9 batch 280 \n",
            "total loss: 1.4960063695907593 epoch 9 batch 300 \n",
            "total loss: 1.4872450828552246 epoch 10 batch 20 \n",
            "total loss: 1.4749122858047485 epoch 10 batch 40 \n",
            "total loss: 1.4814502000808716 epoch 10 batch 60 \n",
            "total loss: 1.5039572715759277 epoch 10 batch 80 \n",
            "total loss: 1.470786690711975 epoch 10 batch 100 \n",
            "total loss: 1.4826229810714722 epoch 10 batch 120 \n",
            "total loss: 1.4930559396743774 epoch 10 batch 140 \n",
            "total loss: 1.4806020259857178 epoch 10 batch 160 \n",
            "total loss: 1.4893556833267212 epoch 10 batch 180 \n",
            "total loss: 1.4595142602920532 epoch 10 batch 200 \n",
            "total loss: 1.472802758216858 epoch 10 batch 220 \n",
            "total loss: 1.4993274211883545 epoch 10 batch 240 \n",
            "total loss: 1.4721068143844604 epoch 10 batch 260 \n",
            "total loss: 1.4337265491485596 epoch 10 batch 280 \n",
            "total loss: 1.4308046102523804 epoch 10 batch 300 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxOZwQxDGyYE"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "در پایین نتایج را برای <br>\r\n",
        "rnn_units = 100<br>\r\n",
        "dense_units = 100<br>\r\n",
        "با 10 اپوچ نتیجه خوبی حاصل شده است. بهتر است تعدا اپوچ ها را کم کرد چرا که روی ترین خطا بشدت کاهش پیدا کرده است. و ممکن است نتیجه اور فیت باشد (باید بررسی شود)\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tsY2JO7E6g8",
        "outputId": "ae595cf1-295b-43a1-9438-43e748ce98b8"
      },
      "source": [
        "epochs = 10\r\n",
        "for i in range(1, epochs+1):\r\n",
        "\r\n",
        "    encoder_initial_cell_state = initialize_initial_state()\r\n",
        "    total_loss = 0.0\r\n",
        "\r\n",
        "\r\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\r\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\r\n",
        "        total_loss += batch_loss\r\n",
        "        if (batch+1)%20 == 0:\r\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 1.0135730504989624 epoch 1 batch 20 \n",
            "total loss: 1.0020941495895386 epoch 1 batch 40 \n",
            "total loss: 0.9875139594078064 epoch 1 batch 60 \n",
            "total loss: 0.9205904603004456 epoch 1 batch 80 \n",
            "total loss: 0.9719870090484619 epoch 1 batch 100 \n",
            "total loss: 0.8657976388931274 epoch 1 batch 120 \n",
            "total loss: 0.8795223236083984 epoch 1 batch 140 \n",
            "total loss: 0.8773382902145386 epoch 1 batch 160 \n",
            "total loss: 0.7717440128326416 epoch 1 batch 180 \n",
            "total loss: 0.7342590093612671 epoch 1 batch 200 \n",
            "total loss: 0.720573365688324 epoch 1 batch 220 \n",
            "total loss: 0.6764557957649231 epoch 1 batch 240 \n",
            "total loss: 0.5431921482086182 epoch 1 batch 260 \n",
            "total loss: 0.44333794713020325 epoch 1 batch 280 \n",
            "total loss: 0.47422754764556885 epoch 1 batch 300 \n",
            "total loss: 0.347017765045166 epoch 2 batch 20 \n",
            "total loss: 0.24115490913391113 epoch 2 batch 40 \n",
            "total loss: 0.2526314854621887 epoch 2 batch 60 \n",
            "total loss: 0.2959304451942444 epoch 2 batch 80 \n",
            "total loss: 0.16182588040828705 epoch 2 batch 100 \n",
            "total loss: 0.15489491820335388 epoch 2 batch 120 \n",
            "total loss: 0.09997710585594177 epoch 2 batch 140 \n",
            "total loss: 0.15745718777179718 epoch 2 batch 160 \n",
            "total loss: 0.1928272694349289 epoch 2 batch 180 \n",
            "total loss: 0.14377513527870178 epoch 2 batch 200 \n",
            "total loss: 0.2203490436077118 epoch 2 batch 220 \n",
            "total loss: 0.0621517188847065 epoch 2 batch 240 \n",
            "total loss: 0.057741280645132065 epoch 2 batch 260 \n",
            "total loss: 0.09660825878381729 epoch 2 batch 280 \n",
            "total loss: 0.08469123393297195 epoch 2 batch 300 \n",
            "total loss: 0.047719940543174744 epoch 3 batch 20 \n",
            "total loss: 0.09370694309473038 epoch 3 batch 40 \n",
            "total loss: 0.06255394965410233 epoch 3 batch 60 \n",
            "total loss: 0.056504759937524796 epoch 3 batch 80 \n",
            "total loss: 0.04332488030195236 epoch 3 batch 100 \n",
            "total loss: 0.03390539065003395 epoch 3 batch 120 \n",
            "total loss: 0.046307750046253204 epoch 3 batch 140 \n",
            "total loss: 0.10791318118572235 epoch 3 batch 160 \n",
            "total loss: 0.050938114523887634 epoch 3 batch 180 \n",
            "total loss: 0.024378281086683273 epoch 3 batch 200 \n",
            "total loss: 0.050781454890966415 epoch 3 batch 220 \n",
            "total loss: 0.05850539356470108 epoch 3 batch 240 \n",
            "total loss: 0.0864444300532341 epoch 3 batch 260 \n",
            "total loss: 0.06874316930770874 epoch 3 batch 280 \n",
            "total loss: 0.03346395492553711 epoch 3 batch 300 \n",
            "total loss: 0.05100604519248009 epoch 4 batch 20 \n",
            "total loss: 0.018052134662866592 epoch 4 batch 40 \n",
            "total loss: 0.03828248381614685 epoch 4 batch 60 \n",
            "total loss: 0.040871959179639816 epoch 4 batch 80 \n",
            "total loss: 0.04390415549278259 epoch 4 batch 100 \n",
            "total loss: 0.019725428894162178 epoch 4 batch 120 \n",
            "total loss: 0.02351485751569271 epoch 4 batch 140 \n",
            "total loss: 0.021615328267216682 epoch 4 batch 160 \n",
            "total loss: 0.02404315024614334 epoch 4 batch 180 \n",
            "total loss: 0.038395222276449203 epoch 4 batch 200 \n",
            "total loss: 0.03585483133792877 epoch 4 batch 220 \n",
            "total loss: 0.009728828445076942 epoch 4 batch 240 \n",
            "total loss: 0.007638529408723116 epoch 4 batch 260 \n",
            "total loss: 0.0161526408046484 epoch 4 batch 280 \n",
            "total loss: 0.028956905007362366 epoch 4 batch 300 \n",
            "total loss: 0.010410046204924583 epoch 5 batch 20 \n",
            "total loss: 0.026044171303510666 epoch 5 batch 40 \n",
            "total loss: 0.012092111632227898 epoch 5 batch 60 \n",
            "total loss: 0.016990771517157555 epoch 5 batch 80 \n",
            "total loss: 0.029698079451918602 epoch 5 batch 100 \n",
            "total loss: 0.029725756496191025 epoch 5 batch 120 \n",
            "total loss: 0.016709651798009872 epoch 5 batch 140 \n",
            "total loss: 0.06266076862812042 epoch 5 batch 160 \n",
            "total loss: 0.0074653420597314835 epoch 5 batch 180 \n",
            "total loss: 0.026700899004936218 epoch 5 batch 200 \n",
            "total loss: 0.020157286897301674 epoch 5 batch 220 \n",
            "total loss: 0.07338698953390121 epoch 5 batch 240 \n",
            "total loss: 0.014602843672037125 epoch 5 batch 260 \n",
            "total loss: 0.006717780139297247 epoch 5 batch 280 \n",
            "total loss: 0.011893886141479015 epoch 5 batch 300 \n",
            "total loss: 0.007097737397998571 epoch 6 batch 20 \n",
            "total loss: 0.013114998117089272 epoch 6 batch 40 \n",
            "total loss: 0.025051288306713104 epoch 6 batch 60 \n",
            "total loss: 0.00939452275633812 epoch 6 batch 80 \n",
            "total loss: 0.011389538645744324 epoch 6 batch 100 \n",
            "total loss: 0.0028911547269672155 epoch 6 batch 120 \n",
            "total loss: 0.009647632017731667 epoch 6 batch 140 \n",
            "total loss: 0.008210320957005024 epoch 6 batch 160 \n",
            "total loss: 0.010091522708535194 epoch 6 batch 180 \n",
            "total loss: 0.024536242708563805 epoch 6 batch 200 \n",
            "total loss: 0.005562834441661835 epoch 6 batch 220 \n",
            "total loss: 0.0019790232181549072 epoch 6 batch 240 \n",
            "total loss: 0.018610281869769096 epoch 6 batch 260 \n",
            "total loss: 0.007061297073960304 epoch 6 batch 280 \n",
            "total loss: 0.006158360280096531 epoch 6 batch 300 \n",
            "total loss: 0.0014234096743166447 epoch 7 batch 20 \n",
            "total loss: 0.005389857105910778 epoch 7 batch 40 \n",
            "total loss: 0.005463120993226767 epoch 7 batch 60 \n",
            "total loss: 0.0038578491657972336 epoch 7 batch 80 \n",
            "total loss: 0.014511974528431892 epoch 7 batch 100 \n",
            "total loss: 0.018504250794649124 epoch 7 batch 120 \n",
            "total loss: 0.005302045494318008 epoch 7 batch 140 \n",
            "total loss: 0.005047348793596029 epoch 7 batch 160 \n",
            "total loss: 0.012018518522381783 epoch 7 batch 180 \n",
            "total loss: 0.00399138405919075 epoch 7 batch 200 \n",
            "total loss: 0.001779874786734581 epoch 7 batch 220 \n",
            "total loss: 0.004133022855967283 epoch 7 batch 240 \n",
            "total loss: 0.008244745433330536 epoch 7 batch 260 \n",
            "total loss: 0.002849948825314641 epoch 7 batch 280 \n",
            "total loss: 0.0009597374591976404 epoch 7 batch 300 \n",
            "total loss: 0.017924552783370018 epoch 8 batch 20 \n",
            "total loss: 0.03710484877228737 epoch 8 batch 40 \n",
            "total loss: 0.029162736609578133 epoch 8 batch 60 \n",
            "total loss: 0.01612793654203415 epoch 8 batch 80 \n",
            "total loss: 0.012254491448402405 epoch 8 batch 100 \n",
            "total loss: 0.001562404097057879 epoch 8 batch 120 \n",
            "total loss: 0.005801124963909388 epoch 8 batch 140 \n",
            "total loss: 0.005115330219268799 epoch 8 batch 160 \n",
            "total loss: 0.014950274489820004 epoch 8 batch 180 \n",
            "total loss: 0.00576202105730772 epoch 8 batch 200 \n",
            "total loss: 0.002926845569163561 epoch 8 batch 220 \n",
            "total loss: 0.006858401000499725 epoch 8 batch 240 \n",
            "total loss: 0.005679421126842499 epoch 8 batch 260 \n",
            "total loss: 0.0013526674592867494 epoch 8 batch 280 \n",
            "total loss: 0.002752922475337982 epoch 8 batch 300 \n",
            "total loss: 0.014381367713212967 epoch 9 batch 20 \n",
            "total loss: 0.0018093183171004057 epoch 9 batch 40 \n",
            "total loss: 0.005798805505037308 epoch 9 batch 60 \n",
            "total loss: 0.001076955464668572 epoch 9 batch 80 \n",
            "total loss: 0.00346526806242764 epoch 9 batch 100 \n",
            "total loss: 0.016864843666553497 epoch 9 batch 120 \n",
            "total loss: 0.00171689095441252 epoch 9 batch 140 \n",
            "total loss: 0.0010113217867910862 epoch 9 batch 160 \n",
            "total loss: 0.0008984146406874061 epoch 9 batch 180 \n",
            "total loss: 0.0016196209471672773 epoch 9 batch 200 \n",
            "total loss: 0.007945898920297623 epoch 9 batch 220 \n",
            "total loss: 0.001984168076887727 epoch 9 batch 240 \n",
            "total loss: 0.004798497073352337 epoch 9 batch 260 \n",
            "total loss: 0.005063336342573166 epoch 9 batch 280 \n",
            "total loss: 0.0017093622591346502 epoch 9 batch 300 \n",
            "total loss: 0.0031414933037012815 epoch 10 batch 20 \n",
            "total loss: 0.0005671280669048429 epoch 10 batch 40 \n",
            "total loss: 0.0006865773466415703 epoch 10 batch 60 \n",
            "total loss: 0.001362132141366601 epoch 10 batch 80 \n",
            "total loss: 0.000886299938429147 epoch 10 batch 100 \n",
            "total loss: 0.0029477120842784643 epoch 10 batch 120 \n",
            "total loss: 0.0005268953391350806 epoch 10 batch 140 \n",
            "total loss: 0.001201340346597135 epoch 10 batch 160 \n",
            "total loss: 0.0011308303801342845 epoch 10 batch 180 \n",
            "total loss: 0.038819555193185806 epoch 10 batch 200 \n",
            "total loss: 0.0019243647111579776 epoch 10 batch 220 \n",
            "total loss: 0.00581542169675231 epoch 10 batch 240 \n",
            "total loss: 0.003673899918794632 epoch 10 batch 260 \n",
            "total loss: 0.0003606979444157332 epoch 10 batch 280 \n",
            "total loss: 0.0031204912811517715 epoch 10 batch 300 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aW-3Lxtz1NT",
        "outputId": "3a17e6d3-da48-4d5a-ea55-f50ff1b657c0"
      },
      "source": [
        "epochs = 2\r\n",
        "for i in range(1, epochs+1):\r\n",
        "\r\n",
        "    encoder_initial_cell_state = initialize_initial_state()\r\n",
        "    total_loss = 0.0\r\n",
        "\r\n",
        "\r\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\r\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\r\n",
        "        total_loss += batch_loss\r\n",
        "        if (batch+1)%20 == 0:\r\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 0.1784008890390396 epoch 1 batch 20 \n",
            "total loss: 0.09170575439929962 epoch 1 batch 40 \n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fc21d9b26a0>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 485, in body\n",
            "    next_sequence_lengths,  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 660, in map_structure\n",
            "    expand_composites=expand_composites)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\n",
            "    structure[0], [func(*x) for x in entries],  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 477, in <lambda>\n",
            "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
            "    error_in_function=error_in_function)\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fc22b8d13c8>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 485, in body\n",
            "    next_sequence_lengths,  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 660, in map_structure\n",
            "    expand_composites=expand_composites)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\n",
            "    structure[0], [func(*x) for x in entries],  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 477, in <lambda>\n",
            "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
            "    error_in_function=error_in_function)\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fc2264fc9e8>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 485, in body\n",
            "    next_sequence_lengths,  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 660, in map_structure\n",
            "    expand_composites=expand_composites)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\n",
            "    structure[0], [func(*x) for x in entries],  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 477, in <lambda>\n",
            "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
            "    error_in_function=error_in_function)\n",
            "==================================\n",
            "ERROR:tensorflow:==================================\n",
            "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
            "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x7fc2264fc860>\n",
            "If you want to mark it as used call its \"mark_used()\" method.\n",
            "It was originally created here:\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 485, in body\n",
            "    next_sequence_lengths,  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 660, in map_structure\n",
            "    expand_composites=expand_composites)  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/nest.py\", line 659, in <listcomp>\n",
            "    structure[0], [func(*x) for x in entries],  File \"/usr/local/lib/python3.6/dist-packages/tensorflow_addons/seq2seq/decoder.py\", line 477, in <lambda>\n",
            "    lambda ta, out: ta.write(time, out), outputs_ta, emit  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_should_use.py\", line 249, in wrapped\n",
            "    error_in_function=error_in_function)\n",
            "==================================\n",
            "total loss: 0.09979020059108734 epoch 1 batch 60 \n",
            "total loss: 0.06687803566455841 epoch 1 batch 80 \n",
            "total loss: 0.08247973024845123 epoch 1 batch 100 \n",
            "total loss: 0.10654767602682114 epoch 1 batch 120 \n",
            "total loss: 0.11287491768598557 epoch 1 batch 140 \n",
            "total loss: 0.06551770120859146 epoch 1 batch 160 \n",
            "total loss: 0.04349624365568161 epoch 1 batch 180 \n",
            "total loss: 0.12391290813684464 epoch 1 batch 200 \n",
            "total loss: 0.0369010828435421 epoch 1 batch 220 \n",
            "total loss: 0.10145465284585953 epoch 1 batch 240 \n",
            "total loss: 0.07458692044019699 epoch 1 batch 260 \n",
            "total loss: 0.0410027913749218 epoch 1 batch 280 \n",
            "total loss: 0.023463940247893333 epoch 1 batch 300 \n",
            "total loss: 0.060495324432849884 epoch 2 batch 20 \n",
            "total loss: 0.03952445089817047 epoch 2 batch 40 \n",
            "total loss: 0.04950904846191406 epoch 2 batch 60 \n",
            "total loss: 0.03878360986709595 epoch 2 batch 80 \n",
            "total loss: 0.05038127675652504 epoch 2 batch 100 \n",
            "total loss: 0.027376573532819748 epoch 2 batch 120 \n",
            "total loss: 0.0588197186589241 epoch 2 batch 140 \n",
            "total loss: 0.04422084242105484 epoch 2 batch 160 \n",
            "total loss: 0.05079570412635803 epoch 2 batch 180 \n",
            "total loss: 0.041784461587667465 epoch 2 batch 200 \n",
            "total loss: 0.08480838686227798 epoch 2 batch 220 \n",
            "total loss: 0.05981770530343056 epoch 2 batch 240 \n",
            "total loss: 0.03927254676818848 epoch 2 batch 260 \n",
            "total loss: 0.06728833168745041 epoch 2 batch 280 \n",
            "total loss: 0.0831434577703476 epoch 2 batch 300 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCmyyb5fHL6d"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "در پایین نتایج را برای <br>\r\n",
        "rnn_units = 1024<br>\r\n",
        "dense_units = 1024<br>\r\n",
        "با 2 اپوچ نتیجه خوبی حاصل شده است.\r\n",
        "اما زمان بسیار بیشتری طول کشیده برای تولید جواب.\r\n",
        "ممکن است بتوان گفت اورفیت شده است.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLWafclbC2Uk",
        "outputId": "c10100ea-2aae-4e0c-d7f7-24fac280cb61"
      },
      "source": [
        "epochs = 2\n",
        "for i in range(1, epochs+1):\n",
        "\n",
        "    encoder_initial_cell_state = initialize_initial_state()\n",
        "    total_loss = 0.0\n",
        "\n",
        "\n",
        "    for ( batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n",
        "        total_loss += batch_loss\n",
        "        if (batch+1)%20 == 0:\n",
        "            print(\"total loss: {} epoch {} batch {} \".format(batch_loss.numpy(), i, batch+1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total loss: 2.750474452972412 epoch 1 batch 20 \n",
            "total loss: 2.544995069503784 epoch 1 batch 40 \n",
            "total loss: 2.4033634662628174 epoch 1 batch 60 \n",
            "total loss: 1.9432857036590576 epoch 1 batch 80 \n",
            "total loss: 1.8823696374893188 epoch 1 batch 100 \n",
            "total loss: 1.8649293184280396 epoch 1 batch 120 \n",
            "total loss: 1.7328026294708252 epoch 1 batch 140 \n",
            "total loss: 1.4836058616638184 epoch 1 batch 160 \n",
            "total loss: 1.334524154663086 epoch 1 batch 180 \n",
            "total loss: 1.180592656135559 epoch 1 batch 200 \n",
            "total loss: 1.1910470724105835 epoch 1 batch 220 \n",
            "total loss: 0.9840564131736755 epoch 1 batch 240 \n",
            "total loss: 0.8954675793647766 epoch 1 batch 260 \n",
            "total loss: 0.6123999953269958 epoch 1 batch 280 \n",
            "total loss: 0.3736543655395508 epoch 1 batch 300 \n",
            "total loss: 0.3805537223815918 epoch 2 batch 20 \n",
            "total loss: 0.09562430530786514 epoch 2 batch 40 \n",
            "total loss: 0.2182844579219818 epoch 2 batch 60 \n",
            "total loss: 0.1920340210199356 epoch 2 batch 80 \n",
            "total loss: 0.15121276676654816 epoch 2 batch 100 \n",
            "total loss: 0.11396999657154083 epoch 2 batch 120 \n",
            "total loss: 0.05727274715900421 epoch 2 batch 140 \n",
            "total loss: 0.13307161629199982 epoch 2 batch 160 \n",
            "total loss: 0.0905832052230835 epoch 2 batch 180 \n",
            "total loss: 0.11611276865005493 epoch 2 batch 200 \n",
            "total loss: 0.24824649095535278 epoch 2 batch 220 \n",
            "total loss: 0.2715160846710205 epoch 2 batch 240 \n",
            "total loss: 0.1475110650062561 epoch 2 batch 260 \n",
            "total loss: 0.1509356051683426 epoch 2 batch 280 \n",
            "total loss: 0.11585617065429688 epoch 2 batch 300 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6t5fpSYbt32"
      },
      "source": [
        "### Inference\n",
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\n",
        "<p>\n",
        "توالی ورودی ایجاد را می کنیم تا به encoder منتقل شود.\n",
        "\n",
        "ورودی decoder در هر مرحله ، پیش بینی های قبلی آن همراه با hidden state و خروجی encoder است.\n",
        "\n",
        "وقتی مدل \\<end\\> را پیش بینی کرد ، پیش بینی را متوقف می کنیم.\n",
        "\n",
        "و وزن های توجه را برای هر time step ذخیره کنید.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "159gIIkrFxxF",
        "outputId": "7be0310c-ae2f-410b-89d3-7e57bd38ccbe"
      },
      "source": [
        "\n",
        "#if trained in same session else use checkpoint variable\n",
        "#decoder_embedding_matrix = tf.train.load_variable(checkpointdir, 'decoderNetwork/decoder_embedding/embeddings/.ATTRIBUTES/VARIABLE_VALUE')\n",
        "decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n",
        "print(decoderNetwork.decoder_embedding.variables[0].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(86, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVeCmlSQBPKk"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "در این قسمت از نمونهگیر گریدی برای پیش بینی جواب استفاده کردیم.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2aw0YCHbvo4",
        "outputId": "63f256fb-db5d-48dd-dfca-ec90930e57d3"
      },
      "source": [
        "\n",
        "input_raw=\"دوازده بهمن هزار و سیصد و هشتاد و دو\"\n",
        "#def inference(input_raw):\n",
        "input_lines = input_raw.split(\"\\n\")\n",
        "# We have a transcript file containing English-Hindi pairs\n",
        "# Preprocess X\n",
        "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
        "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "\n",
        "\n",
        "#output_sequences = []\n",
        "print('a_tx :',a_tx.shape)\n",
        "print('c_tx :', c_tx.shape)\n",
        "\n",
        "\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
        "#print(start_tokens)\n",
        "end_token = Y_tokenizer.word_index['<end>']\n",
        "\n",
        "greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "#finished,start_inputs = greedy_sampler.initialize(decoder_embedding_matrix,start_tokens,end_token)\n",
        "#print(finished.shape, start_inputs.shape)\n",
        "\n",
        "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, sampler = greedy_sampler,\n",
        "                                            output_layer=decoderNetwork.dense_layer)\n",
        "decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "#pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "print(\"decoder_initial_state = [a_tx, c_tx] :\",np.array([a_tx, c_tx]).shape)\n",
        "decoder_initial_state = decoderNetwork.build_decoder_initial_state(inference_batch_size,\n",
        "                                                                   encoder_state=[a_tx, c_tx],\n",
        "                                                                   Dtype=tf.float32)\n",
        "print(\"\\nCompared to simple encoder-decoder without attention, the decoder_initial_state \\\n",
        " is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \\n \")\n",
        "print(\"decoder initial state shape :\",np.array(decoder_initial_state).shape)\n",
        "print(\"decoder_initial_state tensor \\n\", decoder_initial_state)\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "\n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "print(\"start_index_emb_avg \", tf.reduce_sum(tf.reduce_mean(first_inputs, axis=0))) # mean along the batch\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                                             \n",
        "for j in range(maximum_iterations):\n",
        "    outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(outputs.sample_id,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)\n",
        "                                                                               "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_tx : (1, 1024)\n",
            "c_tx : (1, 1024)\n",
            "decoder_initial_state = [a_tx, c_tx] : (2, 1, 1024)\n",
            "\n",
            "Compared to simple encoder-decoder without attention, the decoder_initial_state  is an AttentionWrapperState object containing s_prev tensors and context and alignment vector \n",
            " \n",
            "decoder initial state shape : (6,)\n",
            "decoder_initial_state tensor \n",
            " AttentionWrapperState(cell_state=[<tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
            "array([[-0.03809679,  0.02568001,  0.01960622, ..., -0.07071487,\n",
            "        -0.17816448,  0.04117737]], dtype=float32)>, <tf.Tensor: shape=(1, 1024), dtype=float32, numpy=\n",
            "array([[-0.14047505,  0.08054069,  0.09260534, ..., -0.16887328,\n",
            "        -0.46367982,  0.20955126]], dtype=float32)>], attention=<tf.Tensor: shape=(1, 1024), dtype=float32, numpy=array([[0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>, time=<tf.Tensor: shape=(), dtype=int32, numpy=0>, alignments=<tf.Tensor: shape=(1, 14), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>, alignment_history=(), attention_state=<tf.Tensor: shape=(1, 14), dtype=float32, numpy=\n",
            "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
            "      dtype=float32)>)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (1, 256)\n",
            "start_index_emb_avg  tf.Tensor(0.9710251, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:49: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4QscK44lWG3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYZ-ENBooiNy",
        "outputId": "9b869a8c-6903-456c-99c2-510cfac0013a"
      },
      "source": [
        "\n",
        "print(\"ورودی:\")\n",
        "print(input_raw)\n",
        "print(\"\\nخروجی:\")\n",
        "for i in range(len(predictions)):\n",
        "    line = predictions[i,:]\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\n",
        "    print(\"-\".join( [Y_tokenizer.index_word[w] for w in seq]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ورودی:\n",
            "دوازده بهمن هزار و سیصد و هشتاد و دو\n",
            "\n",
            "خروجی:\n",
            "1382-11-12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMle86xGBiFj"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "در این قسمت از نمونه با استفاده از گریدی سرچ تولید شده است.\r\n",
        "\r\n",
        "برای ورودی \"دوازده بهمن هزار و سیصد و هشتاد و دو\" خروجی \r\n",
        "1382-11-12\r\n",
        "را داده است که درست است.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0FlH3sc8sXn"
      },
      "source": [
        "### Inference using Beam Search with beam_width = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOJe1v-X8u-n",
        "outputId": "e8342eec-52a1-4929-f7ae-9402efc4f68f"
      },
      "source": [
        "beam_width = 3\n",
        "#use with scope /cpu:0 for inferencing\n",
        "#restore from latest checkpoint for inferencing\n",
        "input_raw=\"دوازده بهمن هزار و سیصد و هشتاد و دو\"\n",
        "#input_raw=\"Wow!\"  #checking translation on training set record\n",
        "#def inference(input_raw):\n",
        "input_lines = input_raw.split(\"\\n\")\n",
        "# We have a transcript file containing English-Hindi pairs\n",
        "# Preprocess X\n",
        "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
        "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
        "#print(start_tokens)\n",
        "end_token = Y_tokenizer.word_index['<end>']\n",
        "\n",
        "\n",
        "\n",
        "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "\n",
        "#From official documentation\n",
        "#NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
        "\n",
        "#The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
        "#The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
        "#The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
        "encoder_memory = tfa.seq2seq.tile_batch(a, beam_width)\n",
        "decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
        "#set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "encoder_state = tfa.seq2seq.tile_batch([a_tx, c_tx], multiplier=beam_width)\n",
        "decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = 5\n",
        "#tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "\n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "for j in range(maximum_iterations):\n",
        "    beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "    scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)\n",
        "    beam_scores = np.append(beam_scores, scores, axis = -1)\n",
        "print(predictions.shape) \n",
        "print(beam_scores.shape)                                                                             "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (3, 14, 100)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (1, 3, 256)\n",
            "(1, 3, 5)\n",
            "(1, 3, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bdytbvNIB8O",
        "outputId": "188d1392-50c2-442b-a191-cc7e9a3d97b3"
      },
      "source": [
        "beam_width = 3\n",
        "#use with scope /cpu:0 for inferencing\n",
        "#restore from latest checkpoint for inferencing\n",
        "input_raw=\"دوازده بهمن هزار و سیصد و هشتاد و دو\"\n",
        "#input_raw=\"Wow!\"  #checking translation on training set record\n",
        "#def inference(input_raw):\n",
        "input_lines = input_raw.split(\"\\n\")\n",
        "# We have a transcript file containing English-Hindi pairs\n",
        "# Preprocess X\n",
        "input_lines = [preprocess_sentence(line) for line in input_lines]\n",
        "input_sequences = [[X_tokenizer.word_index[w] for w in line.split(' ')] for line in input_lines]\n",
        "input_sequences = tf.keras.preprocessing.sequence.pad_sequences(input_sequences,\n",
        "                                                                maxlen=Tx, padding='post')\n",
        "inp = tf.convert_to_tensor(input_sequences)\n",
        "#print(inp.shape)\n",
        "inference_batch_size = input_sequences.shape[0]\n",
        "encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n",
        "                              tf.zeros((inference_batch_size, rnn_units))]\n",
        "encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n",
        "a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n",
        "                                                initial_state =encoder_initial_cell_state)\n",
        "\n",
        "start_tokens = tf.fill([inference_batch_size],Y_tokenizer.word_index['<start>'])\n",
        "#print(start_tokens)\n",
        "end_token = Y_tokenizer.word_index['<end>']\n",
        "\n",
        "\n",
        "\n",
        "decoder_input = tf.expand_dims([Y_tokenizer.word_index['<start>']]* inference_batch_size,1)\n",
        "decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "\n",
        "\n",
        "#From official documentation\n",
        "#NOTE If you are using the BeamSearchDecoder with a cell wrapped in AttentionWrapper, then you must ensure that:\n",
        "\n",
        "#The encoder output has been tiled to beam_width via tfa.seq2seq.tile_batch (NOT tf.tile).\n",
        "#The batch_size argument passed to the get_initial_state method of this wrapper is equal to true_batch_size * beam_width.\n",
        "#The initial state created with get_initial_state above contains a cell_state value containing properly tiled final state from the encoder.\n",
        "encoder_memory = tfa.seq2seq.tile_batch(a, beam_width)\n",
        "decoderNetwork.attention_mechanism.setup_memory(encoder_memory)\n",
        "print(\"beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] :\", encoder_memory.shape)\n",
        "#set decoder_inital_state which is an AttentionWrapperState considering beam_width\n",
        "decoder_initial_state = decoderNetwork.rnn_cell.get_initial_state(batch_size = inference_batch_size* beam_width,dtype = Dtype)\n",
        "encoder_state = tfa.seq2seq.tile_batch([a_tx, c_tx], multiplier=beam_width)\n",
        "decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state) \n",
        "\n",
        "decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\n",
        "\n",
        "\n",
        "# Since we do not know the target sequence lengths in advance, we use maximum_iterations to limit the translation lengths.\n",
        "# One heuristic is to decode up to two times the source sentence lengths.\n",
        "maximum_iterations = 5\n",
        "#tf.round(tf.reduce_max(Tx) * 2)\n",
        "\n",
        "#initialize inference decoder\n",
        "\n",
        "(first_finished, first_inputs,first_state) = decoder_instance.initialize(decoder_embedding_matrix,\n",
        "                             start_tokens = start_tokens,\n",
        "                             end_token=end_token,\n",
        "                             initial_state = decoder_initial_state)\n",
        "#print( first_finished.shape)\n",
        "print(\"\\nfirst_inputs returns the same decoder_input i.e. embedding of  <start> :\",first_inputs.shape)\n",
        "\n",
        "inputs = first_inputs\n",
        "state = first_state  \n",
        "predictions = np.empty((inference_batch_size, beam_width,0), dtype = np.int32)\n",
        "beam_scores =  np.empty((inference_batch_size, beam_width,0), dtype = np.float32)                                                                            \n",
        "for j in range(maximum_iterations):\n",
        "    beam_search_outputs, next_state, next_inputs, finished = decoder_instance.step(j,inputs,state)\n",
        "    inputs = next_inputs\n",
        "    state = next_state\n",
        "    outputs = np.expand_dims(beam_search_outputs.predicted_ids,axis = -1)\n",
        "    scores = np.expand_dims(beam_search_outputs.scores,axis = -1)\n",
        "    predictions = np.append(predictions, outputs, axis = -1)\n",
        "    beam_scores = np.append(beam_scores, scores, axis = -1)\n",
        "print(predictions.shape) \n",
        "print(beam_scores.shape)                                                                             "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "beam_with * [batch_size, Tx, rnn_units] :  3 * [2, Tx, rnn_units]] : (3, 14, 100)\n",
            "\n",
            "first_inputs returns the same decoder_input i.e. embedding of  <start> : (1, 3, 256)\n",
            "(1, 3, 5)\n",
            "(1, 3, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdzgAX5JRP6j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a1345b0-527f-4893-9155-bf1dc0b0119d"
      },
      "source": [
        "print(\"-----------------\")\n",
        "print(\"ورودی:\")\n",
        "print(input_raw)\n",
        "print(\"-----------------\")\n",
        "print(\"\\nخروجی:\")\n",
        "for i in range(len(predictions)):\n",
        "    print(\"---------------------------------------------\")\n",
        "    output_beams_per_sample = predictions[i,:,:]\n",
        "    score_beams_per_sample = beam_scores[i,:,:]\n",
        "    for beam, score in zip(output_beams_per_sample,score_beams_per_sample) :\n",
        "        seq = list(itertools.takewhile( lambda index: index !=2, beam))\n",
        "        score_indexes = np.arange(len(seq))\n",
        "        beam_score = score[score_indexes].sum()\n",
        "        print(\" \".join( [Y_tokenizer.index_word[w] for w in seq]), \" beam score: \", beam_score)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------------\n",
            "ورودی:\n",
            "دوازده بهمن هزار و سیصد و هشتاد و دو\n",
            "-----------------\n",
            "\n",
            "خروجی:\n",
            "---------------------------------------------\n",
            "1382 11 12  beam score:  -0.00053067727\n",
            "1383 11 12  beam score:  -27.773975\n",
            "1362 11 12  beam score:  -29.092134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjRUCR3haImr"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "در این قسمت از نمونه با استفاده از بیم سرچ با اندازه beam 3 تولد شده است.\r\n",
        "\r\n",
        "برای ورودی \"دوازده بهمن هزار و سیصد و هشتاد و دو\" خروجی \r\n",
        "1382-11-12\r\n",
        "را داده است که درست است.\r\n",
        "\r\n",
        "اختلاف امتیاز دو خروجی اشتباه با خروجی درست را مشاهده می کنیم که بسیار زیاد است.\r\n",
        "\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKHJ26Rp4D6F"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "در مقایسه عملکرد دو روش بیم سرچ و گریدی باید گفت:\r\n",
        "یا باید هر دو دقت یکسانی داشته باشند یا دقت بیم سرچ باید بهتر باشد.<br>\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "*  گریدی : در هر گام تنها خروجی که ماکسیمم احتمال را دارد به خانه بعدی می دهیم و ممکن است جواب بهینه را از دست بدهیم \r\n",
        "*  بیم : در هر کام به اندازه پهنای بیم خروجی با احتمال بالا تر را انتخاب می کنیم و به مرحله بعدی می دهیم برای مثال اگر بیم 3 باشد در خروجی اول آن 3 مورد که احتمال بالاتری دارند را به خانه بعدی می دهیم و سپس در مرحله بعد پس از بررسی هر کدام از میان تمام مقدیر تولید شده که 3 برابر قبل است 3 تا باحتمال بیشتر را انتخاب می کنیم و به بعدی می دهیم از اینجا به بعد تمام خروجی ها به اندازه 3 برابر اولین خروجی است که هر بار 3 محتملترین را انتخاب می کنیم.\r\n",
        "\r\n",
        "برای بررسی دقت یک مدل شبکه انکودر و دکودر را در آن مدل باید بررسی کرد از آنجا که ما در این جا شبکه انکودر را یکسان گرفتیم اگر تفاوت وجود داشته باشد باد در دکودر مدل ها بررسی شود.<br>\r\n",
        "\r\n",
        "اگر بطور خاص به دکودر در بیم سرچ نگاه کنیم و  به دنبال بررسی این باشیم که آیا دکودر در این مدل خوب عمل کرده است یا خیر و اگر قرار است برای بهبود مدل کاری انجام دهیم خوب است در دکودر (همان شبکه بیم) تغییر ایجاد کنیم یا باید در شبکه انکودر (RNN) تغییر ایجاد کنیم.\r\n",
        "\r\n",
        "با بررسی p(y|x) و p(y^|x) می توان یافت باید در کدام تغییر ایجاد کرد که در اینجا p(y|x) مقدار مطلوب ما است و p(y^|x) خروجی مدل است \r\n",
        "\r\n",
        "اگر p(y|x)>p(y^|x) ینی اگر y را به مدل RNN بدهیم احتمال بیشتری می دهد بنابراین در نتیجه شبکه RNN درست بوده و بیم ما اشتباه عمل کرده است اگر برعکس بود و p(y^|x) بیشتر بود نساد می دهد RNN خوب عمل نکرده است و بیم مطابق خروجی RNNعمل کرده است .\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnDD7_FqQP2e"
      },
      "source": [
        "### Evaluate Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RpQFnMTQR9o"
      },
      "source": [
        "def eval_greedy(input_batch, output_batch,encoder_initial_cell_state, BATCH_SIZE):\n",
        "    #initialize loss = 0\n",
        "    loss = 0\n",
        "\n",
        "    # we can do initialization in outer block\n",
        "    #encoder_initial_cell_state = encoder.initialize_initial_state()\n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n",
        "    a, h_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n",
        "                                                    initial_state =encoder_initial_cell_state)\n",
        "\n",
        "\n",
        "\n",
        "    decoder_input = output_batch[:,:-1] # ignore <end>\n",
        "    #compare logits with timestepped +1 version of decoder_input\n",
        "    decoder_output = output_batch[:,1:] #ignore <start>\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n",
        "    decoder_instance = tfa.seq2seq.BasicDecoder(decoderNetwork.rnn_cell, \n",
        "                                                greedy_sampler,\n",
        "                                                decoderNetwork.dense_layer)\n",
        "    #BasicDecoderOutput\n",
        "\n",
        "    decoderNetwork.attention_mechanism.setup_memory(a)\n",
        "    #pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\n",
        "    \n",
        "    decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n",
        "                                                                       encoder_state=[h_tx, c_tx],\n",
        "                                                                       Dtype=tf.float32)\n",
        "    outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n",
        "                                           sequence_length=BATCH_SIZE*[Ty-1])\n",
        "    logits = outputs.rnn_output\n",
        "    sample_id = outputs.sample_id\n",
        "    #Calculate loss\n",
        "    loss = loss_function(logits, decoder_output)\n",
        "    return loss, sample_id, logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsWt4I4ezqiU"
      },
      "source": [
        "def eval_beam(input_batch, output_batch,encoder_initial_cell_state, BATCH_SIZE):\r\n",
        "    #initialize loss = 0\r\n",
        "    loss = 0\r\n",
        "\r\n",
        "    # we can do initialization in outer block\r\n",
        "    #encoder_initial_cell_state = encoder.initialize_initial_state()\r\n",
        "    encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\r\n",
        "    a, h_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \r\n",
        "                                                    initial_state =encoder_initial_cell_state)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "    decoder_input = output_batch[:,:-1] # ignore <end>\r\n",
        "    #compare logits with timestepped +1 version of decoder_input\r\n",
        "    decoder_output = output_batch[:,1:] #ignore <start>\r\n",
        "    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\r\n",
        "    decoder_instance = tfa.seq2seq.BeamSearchDecoder(decoderNetwork.rnn_cell,beam_width=beam_width,\r\n",
        "                                                 output_layer=decoderNetwork.dense_layer)\r\n",
        "    #BasicDecoderOutput\r\n",
        "\r\n",
        "    decoderNetwork.attention_mechanism.setup_memory(a)\r\n",
        "    #pass [ last step activations , encoder memory_state ] as input to decoder for LSTM\r\n",
        "    \r\n",
        "    decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\r\n",
        "                                                                       encoder_state=[h_tx, c_tx],\r\n",
        "                                                                       Dtype=tf.float32)\r\n",
        "    outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\r\n",
        "                                           sequence_length=BATCH_SIZE*[Ty-1])\r\n",
        "    \r\n",
        "    \r\n",
        "    logits = outputs.rnn_output\r\n",
        "    sample_id = outputs.sample_id\r\n",
        "    #Calculate loss\r\n",
        "    loss = loss_function(logits, decoder_output)\r\n",
        "    return loss, sample_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOJtMbjZsAuh"
      },
      "source": [
        "### Evaluation Loss on Entire Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh2fnCtjbk24"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "از میزان خطایی که داده های ترین در آخرین اپوچ (10) دیدیم مشخص بود که چون خطا خیلی خیلی کم شده اور فیت شده است .\r\n",
        "\r\n",
        "در این قسمت هم با مقایسه خطای روی ولید و تست با ترین کاملا مشخص است که خطا تست با این که حدود 15 درصد است اما نسبت به ترین که حدودا صفر است خیلی خیلی زیاد است.\r\n",
        "می توانیم تعدا اپوچ ها را کم کنیم یا این که تعداد یونیت های rnn را مثلا به 50 کاهش دهیم و باز مدل را ترین کنیم و نتایج را ببینیم.\r\n",
        "</p>\r\n",
        "<p>\r\n",
        "ما به علت کمبود وقت دوباره ترین را انجام ندادیم.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LpYul_FpW2Pa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFgeMs5agTFL",
        "outputId": "5cf80f99-cf0b-482b-e304-cac4c6795c00"
      },
      "source": [
        "dataset_train = tf.data.Dataset.from_tensor_slices((X, Y)).batch(len(X))\r\n",
        "a=[]\r\n",
        "for (input_batch, output_batch) in dataset_train.take(-1):\r\n",
        "    batch_size = len(input_batch)\r\n",
        "    print(input_batch.shape)\r\n",
        "    encoder_initial_cell_state = [tf.zeros((batch_size, rnn_units)),\r\n",
        "                                  tf.zeros((batch_size, rnn_units))]\r\n",
        "    loss,_ = eval_greedy(input_batch, output_batch, encoder_initial_cell_state, batch_size)\r\n",
        "    loss = tf.reduce_mean(loss)\r\n",
        "    print(\"Training loss {}\".format(loss) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20000, 14)\n",
            "Training loss 0.0016933688893914223\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmRmm_uxz6y-",
        "outputId": "c90e08fa-0ca6-4ca1-b077-c1a2c9bc36b2"
      },
      "source": [
        "dataset_v = tf.data.Dataset.from_tensor_slices((Xv, Yv)).batch(len(Xv))\r\n",
        "a=[]\r\n",
        "for (input_batch, output_batch) in dataset_v.take(-1):\r\n",
        "    batch_size = len(input_batch)\r\n",
        "    print(input_batch.shape)\r\n",
        "    encoder_initial_cell_state = [tf.zeros((batch_size, rnn_units)),\r\n",
        "                                  tf.zeros((batch_size, rnn_units))]\r\n",
        "    loss,_ = eval_greedy(input_batch, output_batch, encoder_initial_cell_state, batch_size)\r\n",
        "    a.append((_.numpy()))\r\n",
        "    loss = tf.reduce_mean(loss)\r\n",
        "    print(\"Training loss {}\".format(loss) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 14)\n",
            "Training loss 15.325642585754395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7smHQEFXfjnZ",
        "outputId": "a824bccc-279b-4421-c849-9a1536dbe8d5"
      },
      "source": [
        "dataset_v = tf.data.Dataset.from_tensor_slices((Xv, Yv)).batch(len(Xv))\r\n",
        "a=[]\r\n",
        "for (input_batch, output_batch) in dataset_v.take(-1):\r\n",
        "    batch_size = len(input_batch)\r\n",
        "    print(input_batch.shape)\r\n",
        "    encoder_initial_cell_state = [tf.zeros((batch_size, rnn_units)),\r\n",
        "                                  tf.zeros((batch_size, rnn_units))]\r\n",
        "    loss,_ = eval_beam(input_batch, output_batch, encoder_initial_cell_state, batch_size)\r\n",
        "    a.append((_.numpy()))\r\n",
        "    loss = tf.reduce_mean(loss)\r\n",
        "    print(\"Training loss {}\".format(loss) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 14)\n",
            "Training loss 15.325642585754395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnBx8hI9ixDH"
      },
      "source": [
        "b=[]\r\n",
        "for i in a:\r\n",
        "  for j in range(5000):\r\n",
        "    line = i[j]\r\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\r\n",
        "    b.append(\"-\".join( [Y_tokenizer.index_word[w] for w in seq]))\r\n",
        "   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOQcNyqLwFy8"
      },
      "source": [
        "hv1=[]\r\n",
        "mv1=[]\r\n",
        "dav1=[]\r\n",
        "for i in validationSet:\r\n",
        "   h1,m1= i\r\n",
        "   if h1 is not None:\r\n",
        "      hv1.append(h1)\r\n",
        "      mv1.append(m1)\r\n",
        "      dav1.append((h1, m1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkmAtIIrV9-l",
        "outputId": "565fb8ff-48b4-4d52-893c-6264d9ce1959"
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xt, Yt)).batch(len(Xt))\n",
        "a=[]\n",
        "for (input_batch, output_batch) in dataset_test.take(-1):\n",
        "    batch_size = len(input_batch)\n",
        "    print(input_batch.shape)\n",
        "    encoder_initial_cell_state = [tf.zeros((batch_size, rnn_units)),\n",
        "                                  tf.zeros((batch_size, rnn_units))]\n",
        "    loss,_ = eval_greedy(input_batch, output_batch, encoder_initial_cell_state, batch_size)\n",
        "    a.append((_.numpy()))\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    print(\"Training loss {}\".format(loss) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 14)\n",
            "Training loss 14.608162879943848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiA4swbYy2sF"
      },
      "source": [
        "b=[]\r\n",
        "for i in a:\r\n",
        "  for j in range(5000):\r\n",
        "    line = i[j]\r\n",
        "    seq = list(itertools.takewhile( lambda index: index !=2, line))\r\n",
        "    b.append(\"-\".join( [Y_tokenizer.index_word[w] for w in seq]))\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXPvv3rty2sd"
      },
      "source": [
        "ht1=[]\r\n",
        "mt1=[]\r\n",
        "dat1=[]\r\n",
        "for i in testSet:\r\n",
        "   h1,m1= i\r\n",
        "   if h is not None:\r\n",
        "      ht1.append(h1)\r\n",
        "      mt1.append(m1)\r\n",
        "      dat1.append((h1, m1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOrW_Ycwz2bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f988514-a8ce-49b0-d716-1bc7f15ca8b9"
      },
      "source": [
        "dataset_test = tf.data.Dataset.from_tensor_slices((Xt, Yt)).batch(len(Xt))\r\n",
        "a=[]\r\n",
        "for (input_batch, output_batch) in dataset_test.take(-1):\r\n",
        "    batch_size = len(input_batch)\r\n",
        "    print(input_batch.shape)\r\n",
        "    encoder_initial_cell_state = [tf.zeros((batch_size, rnn_units)),\r\n",
        "                                  tf.zeros((batch_size, rnn_units))]\r\n",
        "    loss,_ = eval_greedy(input_batch, output_batch, encoder_initial_cell_state, batch_size)\r\n",
        "    a.append((_.numpy()))\r\n",
        "    loss = tf.reduce_mean(loss)\r\n",
        "    print(\"Training loss {}\".format(loss) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5000, 14)\n",
            "Training loss 14.608162879943848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBLR4YgtFjg2"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "بررسی نشان می دهد برای ورودی هایی که عدد بیشتر داشته است خوب عمل نکرده است با این که ما سعی کردیم با حذف . و / تاریخ هایی که اسپیس نداشتند را به سه کلمه تبدیل کنیم باز هم مواردی داشتیم که خوب عمل نکرده بودند.\r\n",
        "</p>\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKdgSd_ry2se"
      },
      "source": [
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C14po7TNy2sf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHGvT_1g0pPI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6J9zdiXcdY4v"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<h1>مدل مبتنی بر حروف</h1>\r\n",
        "<p>\r\n",
        "در قسمت قبل ما مدلی مبتی بر کلمه ساختیم و نتایج را بررسی کردم مدل ما اورفیت شده بود اما با این حال برای داده هایی که خودمان وارد کردیم نتیجه خوبی داشت (البته برای داده های تست تا حدود 15 درصد خطا داشت)\r\n",
        "<br>\r\n",
        "در این قسمت مدلی مبتنی بر حروف می سازیم و میزان دقت و belu ان را بررسی می کنیم.\r\n",
        "\r\n",
        "</p>\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgZCFAiULWg8"
      },
      "source": [
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import load_model, Model\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "\n",
        "#from faker import Faker\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from babel.dates import format_date\n",
        "#from nmt_utils import *\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDJhKqkWMF6b"
      },
      "source": [
        "\r\n",
        "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply\r\n",
        "from keras.layers import RepeatVector, Dense, Activation, Lambda\r\n",
        "from keras.optimizers import Adam\r\n",
        "from keras.utils import to_categorical\r\n",
        "from keras.models import load_model, Model\r\n",
        "import keras.backend as K\r\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "from tqdm import tqdm\r\n",
        "from babel.dates import format_date\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU8NsEuIfdVn"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "\r\n",
        "<h2>پیش پردازش</h2>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3upQG_4-im8a"
      },
      "source": [
        "human_vocab=set()\r\n",
        "machine_vocab=set()\r\n",
        "dataset=[]\r\n",
        "for i in trainSet:\r\n",
        "   h,m= i\r\n",
        "   if h is not None:\r\n",
        "      dataset.append((h, m))\r\n",
        "      human_vocab.update(tuple(h))\r\n",
        "      machine_vocab.update(tuple(m))\r\n",
        "human = dict(zip(sorted(human_vocab) + ['<unk>', '<pad>'], list(range(len(human_vocab) + 2))))\r\n",
        "inv_machine = dict(enumerate(sorted(machine_vocab)))\r\n",
        "machine = {v: k for k, v in inv_machine.items()}\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efmZvJazA9bQ",
        "outputId": "a537fa58-0d26-41af-b7ae-0ef2db98a296"
      },
      "source": [
        "machine"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'-': 0,\n",
              " '0': 1,\n",
              " '1': 2,\n",
              " '2': 3,\n",
              " '3': 4,\n",
              " '4': 5,\n",
              " '5': 6,\n",
              " '6': 7,\n",
              " '7': 8,\n",
              " '8': 9,\n",
              " '9': 10}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 307
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-HxPiu7A8ho",
        "outputId": "534b4304-31ac-46b2-df7b-519d8124fe6b"
      },
      "source": [
        "inv_machine"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '-',\n",
              " 1: '0',\n",
              " 2: '1',\n",
              " 3: '2',\n",
              " 4: '3',\n",
              " 5: '4',\n",
              " 6: '5',\n",
              " 7: '6',\n",
              " 8: '7',\n",
              " 9: '8',\n",
              " 10: '9'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 308
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orQrecux0vLi",
        "outputId": "370dab8b-1615-4a56-aaf4-cf4d189dff5c"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 309
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKOYCP9Qut4L"
      },
      "source": [
        "vhuman_vocab=set()\r\n",
        "vmachine_vocab=set()\r\n",
        "vdataset=[]\r\n",
        "for i in validationSet:\r\n",
        "   h,m= i\r\n",
        "   if h is not None:\r\n",
        "      vdataset.append((h, m))\r\n",
        "      vhuman_vocab.update(tuple(h))\r\n",
        "      vmachine_vocab.update(tuple(m))\r\n",
        "vhuman = dict(zip(sorted(vhuman_vocab) + ['<unk>', '<pad>'], list(range(len(vhuman_vocab) + 2))))\r\n",
        "vinv_machine = dict(enumerate(sorted(vmachine_vocab)))\r\n",
        "vmachine = {v: k for k, v in vinv_machine.items()}\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW88n90Lu_9h"
      },
      "source": [
        "thuman_vocab=set()\n",
        "tmachine_vocab=set()\n",
        "tdataset=[]\n",
        "for i in testSet:\n",
        "   h,m= i\n",
        "   if h is not None:\n",
        "      tdataset.append((h, m))\n",
        "      thuman_vocab.update(tuple(h))\n",
        "      tmachine_vocab.update(tuple(m))\n",
        "thuman = dict(zip(sorted(thuman_vocab) + ['<unk>', '<pad>'], list(range(len(thuman_vocab) + 2))))\n",
        "tinv_machine = dict(enumerate(sorted(tmachine_vocab)))\n",
        "tmachine = {v: k for k, v in tinv_machine.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXVWaPeGv9-N"
      },
      "source": [
        "def preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty):\r\n",
        "    X, Y = zip(*dataset)\r\n",
        "    \r\n",
        "    X = np.array([string_to_int(i, Tx, human_vocab) for i in X])\r\n",
        "    Y = [string_to_int(t, Ty, machine_vocab) for t in Y]\r\n",
        "    \r\n",
        "    Xoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human_vocab)), X)))\r\n",
        "    Yoh = np.array(list(map(lambda x: to_categorical(x, num_classes=len(machine_vocab)), Y)))\r\n",
        "\r\n",
        "    return X, np.array(Y), Xoh, Yoh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsvuD9PEBiFR"
      },
      "source": [
        "def string_to_int(string, length, vocab):\r\n",
        "    #string = string.lower()\r\n",
        "    string = string.replace(',','')\r\n",
        "    \r\n",
        "    if len(string) > length:\r\n",
        "        string = string[:length]\r\n",
        "        \r\n",
        "    rep = list(map(lambda x: vocab.get(x, '<unk>'), string))\r\n",
        "    \r\n",
        "    if len(string) < length:\r\n",
        "        rep += [vocab['<pad>']] * (length - len(string))\r\n",
        "    \r\n",
        "    return rep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHAwKkrtGSD-"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "بزرگترین ورودی 52 کاراکتر بود اما با در نظر گرفتن ورودی 52 برای مدل دقت مدل به شدت افت کرد بنا بر این ما 50 گرفتیم.\r\n",
        "\r\n",
        "برای 50 نتایج خیلی بهتر بود.\r\n",
        "\r\n",
        "خروجی هم ثابت است (10)\r\n",
        "</p>\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76cfvlUuBmde",
        "outputId": "d80b9323-66ee-4005-d4dd-873d48337d8b"
      },
      "source": [
        "string_to_int('شنبه مرداد ۹ ۱۳۷۸', 52, human)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[24,\n",
              " 30,\n",
              " 15,\n",
              " 31,\n",
              " 0,\n",
              " 29,\n",
              " 21,\n",
              " 19,\n",
              " 14,\n",
              " 19,\n",
              " 0,\n",
              " 46,\n",
              " 0,\n",
              " 38,\n",
              " 40,\n",
              " 44,\n",
              " 45,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49,\n",
              " 49]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 314
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpAxG1UrB390",
        "outputId": "f754d7e4-3bae-4559-e0a7-20c89963ce14"
      },
      "source": [
        "Tx = 50\r\n",
        "Ty = 10\r\n",
        "X, Y, Xoh, Yoh = preprocess_data(dataset, human, machine, Tx, Ty)\r\n",
        "vX, vY, vXoh, vYoh = preprocess_data(vdataset, vhuman, vmachine, Tx, Ty)\r\n",
        "tX, tY, tXoh, tYoh = preprocess_data(tdataset, thuman, tmachine, Tx, Ty)\r\n",
        "\r\n",
        "print(\"X.shape:\", X.shape)\r\n",
        "print(\"Y.shape:\", Y.shape)\r\n",
        "print(\"Xoh.shape:\", Xoh.shape)\r\n",
        "print(\"Yoh.shape:\", Yoh.shape)\r\n",
        "print()\r\n",
        "print(\"vX.shape:\", vX.shape)\r\n",
        "print(\"vY.shape:\", vY.shape)\r\n",
        "print(\"vXoh.shape:\", vXoh.shape)\r\n",
        "print(\"vYoh.shape:\", vYoh.shape)\r\n",
        "print()\r\n",
        "print(\"tX.shape:\", tX.shape)\r\n",
        "print(\"tY.shape:\", tY.shape)\r\n",
        "print(\"tXoh.shape:\", tXoh.shape)\r\n",
        "print(\"tYoh.shape:\", tYoh.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X.shape: (20000, 50)\n",
            "Y.shape: (20000, 10)\n",
            "Xoh.shape: (20000, 50, 50)\n",
            "Yoh.shape: (20000, 10, 11)\n",
            "\n",
            "vX.shape: (5000, 50)\n",
            "vY.shape: (5000, 10)\n",
            "vXoh.shape: (5000, 50, 50)\n",
            "vYoh.shape: (5000, 10, 11)\n",
            "\n",
            "tX.shape: (5000, 50)\n",
            "tY.shape: (5000, 10)\n",
            "tXoh.shape: (5000, 50, 50)\n",
            "tYoh.shape: (5000, 10, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfD9cwQ0EKRR",
        "outputId": "6fadaec8-1ce2-4475-b98b-7bf8c9dae1d5"
      },
      "source": [
        "index = 0\r\n",
        "print(\"Source date:\", dataset[index][0])\r\n",
        "print(\"Target date:\", dataset[index][1])\r\n",
        "print()\r\n",
        "print(\"Source after preprocessing (indices):\", X[index])\r\n",
        "print(\"Target after preprocessing (indices):\", Y[index])\r\n",
        "print()\r\n",
        "print(\"Source after preprocessing (one-hot):\", Xoh[index])\r\n",
        "print(\"Target after preprocessing (one-hot):\", Yoh[index])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Source date: شنبه مرداد ۹ ۱۳۷۸\n",
            "Target date: 1378-05-09\n",
            "\n",
            "Source after preprocessing (indices): [24 30 15 31  0 29 21 19 14 19  0 46  0 38 40 44 45 49 49 49 49 49 49 49\n",
            " 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49 49\n",
            " 49 49]\n",
            "Target after preprocessing (indices): [ 2  4  8  9  0  1  6  0  1 10]\n",
            "\n",
            "Source after preprocessing (one-hot): [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n",
            "Target after preprocessing (one-hot): [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we_R-trpfpcm"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<h1>ساخت مدل</h1>\r\n",
        "<p>\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgXLQJVTFD_8"
      },
      "source": [
        "repeator = RepeatVector(Tx)\r\n",
        "concatenator = Concatenate(axis=-1)\r\n",
        "densor1 = Dense(10, activation = \"tanh\")\r\n",
        "densor2 = Dense(1, activation = \"relu\")\r\n",
        "activator = Activation('softmax', name='attention_weights')\r\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKLsn3XvFN3Y"
      },
      "source": [
        "def one_step_attention(a, s_prev):\r\n",
        "    s_prev = repeator(s_prev)\r\n",
        "    concat = concatenator([a, s_prev])\r\n",
        "    e = densor1(concat)\r\n",
        "    energies = densor2(e)\r\n",
        "    alphas = activator(energies)\r\n",
        "    context = dotor([alphas, a])\r\n",
        "    \r\n",
        "    return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAlJkZ85FRM0"
      },
      "source": [
        "n_a = 32\r\n",
        "n_s = 64\r\n",
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)\r\n",
        "output_layer = Dense(len(machine_vocab), activation='softmax')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP42VEJsFTG7"
      },
      "source": [
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\r\n",
        "    X = Input(shape=(Tx, human_vocab_size))\r\n",
        "    s0 = Input(shape=(n_s,), name='s0')\r\n",
        "    c0 = Input(shape=(n_s,), name='c0')\r\n",
        "    s = s0\r\n",
        "    c = c0\r\n",
        "    \r\n",
        "    outputs = []\r\n",
        "    \r\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences = True))(X)\r\n",
        "    \r\n",
        "    for t in range(Ty):\r\n",
        "        context = one_step_attention(a, s)\r\n",
        "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\r\n",
        "        out = output_layer(s)\r\n",
        "        outputs.append(out)\r\n",
        "    \r\n",
        "    model = Model([X, s0, c0], outputs)\r\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxBTBsC1FWO1"
      },
      "source": [
        "mod = model(Tx, Ty, n_a, n_s, len(human), len(machine))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwl9EHLBFYxC",
        "outputId": "477a156a-30c5-47d1-b3df-814bf945e88b"
      },
      "source": [
        "mod.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 50, 50)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "s0 (InputLayer)                 [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_4 (Bidirectional) (None, 50, 64)       21248       input_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "repeat_vector_3 (RepeatVector)  (None, 50, 64)       0           s0[0][0]                         \n",
            "                                                                 lstm_13[0][0]                    \n",
            "                                                                 lstm_13[1][0]                    \n",
            "                                                                 lstm_13[2][0]                    \n",
            "                                                                 lstm_13[3][0]                    \n",
            "                                                                 lstm_13[4][0]                    \n",
            "                                                                 lstm_13[5][0]                    \n",
            "                                                                 lstm_13[6][0]                    \n",
            "                                                                 lstm_13[7][0]                    \n",
            "                                                                 lstm_13[8][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_3 (Concatenate)     (None, 50, 128)      0           bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[0][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[1][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[2][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[3][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[4][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[5][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[6][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[7][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[8][0]            \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 repeat_vector_3[9][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_15 (Dense)                (None, 50, 10)       1290        concatenate_3[0][0]              \n",
            "                                                                 concatenate_3[1][0]              \n",
            "                                                                 concatenate_3[2][0]              \n",
            "                                                                 concatenate_3[3][0]              \n",
            "                                                                 concatenate_3[4][0]              \n",
            "                                                                 concatenate_3[5][0]              \n",
            "                                                                 concatenate_3[6][0]              \n",
            "                                                                 concatenate_3[7][0]              \n",
            "                                                                 concatenate_3[8][0]              \n",
            "                                                                 concatenate_3[9][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_16 (Dense)                (None, 50, 1)        11          dense_15[0][0]                   \n",
            "                                                                 dense_15[1][0]                   \n",
            "                                                                 dense_15[2][0]                   \n",
            "                                                                 dense_15[3][0]                   \n",
            "                                                                 dense_15[4][0]                   \n",
            "                                                                 dense_15[5][0]                   \n",
            "                                                                 dense_15[6][0]                   \n",
            "                                                                 dense_15[7][0]                   \n",
            "                                                                 dense_15[8][0]                   \n",
            "                                                                 dense_15[9][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "attention_weights (Activation)  (None, 50, 1)        0           dense_16[0][0]                   \n",
            "                                                                 dense_16[1][0]                   \n",
            "                                                                 dense_16[2][0]                   \n",
            "                                                                 dense_16[3][0]                   \n",
            "                                                                 dense_16[4][0]                   \n",
            "                                                                 dense_16[5][0]                   \n",
            "                                                                 dense_16[6][0]                   \n",
            "                                                                 dense_16[7][0]                   \n",
            "                                                                 dense_16[8][0]                   \n",
            "                                                                 dense_16[9][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dot_3 (Dot)                     (None, 1, 64)        0           attention_weights[0][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 attention_weights[1][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 attention_weights[2][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 attention_weights[3][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 attention_weights[4][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 attention_weights[5][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 attention_weights[6][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 attention_weights[7][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 attention_weights[8][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "                                                                 attention_weights[9][0]          \n",
            "                                                                 bidirectional_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "c0 (InputLayer)                 [(None, 64)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lstm_13 (LSTM)                  [(None, 64), (None,  33024       dot_3[0][0]                      \n",
            "                                                                 s0[0][0]                         \n",
            "                                                                 c0[0][0]                         \n",
            "                                                                 dot_3[1][0]                      \n",
            "                                                                 lstm_13[0][0]                    \n",
            "                                                                 lstm_13[0][2]                    \n",
            "                                                                 dot_3[2][0]                      \n",
            "                                                                 lstm_13[1][0]                    \n",
            "                                                                 lstm_13[1][2]                    \n",
            "                                                                 dot_3[3][0]                      \n",
            "                                                                 lstm_13[2][0]                    \n",
            "                                                                 lstm_13[2][2]                    \n",
            "                                                                 dot_3[4][0]                      \n",
            "                                                                 lstm_13[3][0]                    \n",
            "                                                                 lstm_13[3][2]                    \n",
            "                                                                 dot_3[5][0]                      \n",
            "                                                                 lstm_13[4][0]                    \n",
            "                                                                 lstm_13[4][2]                    \n",
            "                                                                 dot_3[6][0]                      \n",
            "                                                                 lstm_13[5][0]                    \n",
            "                                                                 lstm_13[5][2]                    \n",
            "                                                                 dot_3[7][0]                      \n",
            "                                                                 lstm_13[6][0]                    \n",
            "                                                                 lstm_13[6][2]                    \n",
            "                                                                 dot_3[8][0]                      \n",
            "                                                                 lstm_13[7][0]                    \n",
            "                                                                 lstm_13[7][2]                    \n",
            "                                                                 dot_3[9][0]                      \n",
            "                                                                 lstm_13[8][0]                    \n",
            "                                                                 lstm_13[8][2]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_17 (Dense)                (None, 11)           715         lstm_13[0][0]                    \n",
            "                                                                 lstm_13[1][0]                    \n",
            "                                                                 lstm_13[2][0]                    \n",
            "                                                                 lstm_13[3][0]                    \n",
            "                                                                 lstm_13[4][0]                    \n",
            "                                                                 lstm_13[5][0]                    \n",
            "                                                                 lstm_13[6][0]                    \n",
            "                                                                 lstm_13[7][0]                    \n",
            "                                                                 lstm_13[8][0]                    \n",
            "                                                                 lstm_13[9][0]                    \n",
            "==================================================================================================\n",
            "Total params: 56,288\n",
            "Trainable params: 56,288\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2qLs2XWFbcu"
      },
      "source": [
        "opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)\r\n",
        "mod.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc7OJhi-gfT_"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<h1>آموزش مدل</h1>\r\n",
        "\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10WeWmY5Ffvn",
        "outputId": "049a9b29-6052-4ef2-f296-e134e0ac3da0"
      },
      "source": [
        "s0 = np.zeros((len(trainSet), n_s))\r\n",
        "c0 = np.zeros((len(trainSet), n_s))\r\n",
        "outputs = list(Yoh.swapaxes(0,1))\r\n",
        "s0v = np.zeros((5000, n_s))\r\n",
        "c0v = np.zeros((5000, n_s))\r\n",
        "outputsv = list(vYoh.swapaxes(0,1))\r\n",
        "mod.fit([Xoh, s0, c0], outputs, epochs=40, batch_size=100 ,validation_data=([vXoh, s0v, c0v], outputsv))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40\n",
            "200/200 [==============================] - 42s 129ms/step - loss: 17.8905 - dense_17_loss: 0.6439 - dense_17_1_loss: 0.9966 - dense_17_2_loss: 2.4340 - dense_17_3_loss: 2.5127 - dense_17_4_loss: 1.2248 - dense_17_5_loss: 1.6683 - dense_17_6_loss: 2.5530 - dense_17_7_loss: 1.1866 - dense_17_8_loss: 1.9605 - dense_17_9_loss: 2.7100 - dense_17_accuracy: 0.9421 - dense_17_1_accuracy: 0.5774 - dense_17_2_accuracy: 0.1679 - dense_17_3_accuracy: 0.0798 - dense_17_4_accuracy: 0.9021 - dense_17_5_accuracy: 0.1608 - dense_17_6_accuracy: 0.0502 - dense_17_7_accuracy: 0.9288 - dense_17_8_accuracy: 0.1074 - dense_17_9_accuracy: 0.0401 - val_loss: 8.0610 - val_dense_17_loss: 0.0252 - val_dense_17_1_loss: 0.0972 - val_dense_17_2_loss: 0.9519 - val_dense_17_3_loss: 1.6598 - val_dense_17_4_loss: 0.0344 - val_dense_17_5_loss: 0.4146 - val_dense_17_6_loss: 1.7597 - val_dense_17_7_loss: 0.0580 - val_dense_17_8_loss: 1.1333 - val_dense_17_9_loss: 1.9269 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9806 - val_dense_17_2_accuracy: 0.6838 - val_dense_17_3_accuracy: 0.4138 - val_dense_17_4_accuracy: 0.9994 - val_dense_17_5_accuracy: 0.8642 - val_dense_17_6_accuracy: 0.3816 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.5826 - val_dense_17_9_accuracy: 0.2970\n",
            "Epoch 2/40\n",
            "200/200 [==============================] - 21s 107ms/step - loss: 7.4648 - dense_17_loss: 0.0204 - dense_17_1_loss: 0.0824 - dense_17_2_loss: 0.8148 - dense_17_3_loss: 1.5486 - dense_17_4_loss: 0.0262 - dense_17_5_loss: 0.3662 - dense_17_6_loss: 1.6493 - dense_17_7_loss: 0.0425 - dense_17_8_loss: 1.0332 - dense_17_9_loss: 1.8812 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9819 - dense_17_2_accuracy: 0.7237 - dense_17_3_accuracy: 0.4316 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.8841 - dense_17_6_accuracy: 0.4078 - dense_17_7_accuracy: 0.9993 - dense_17_8_accuracy: 0.6247 - dense_17_9_accuracy: 0.3064 - val_loss: 6.0410 - val_dense_17_loss: 0.0105 - val_dense_17_1_loss: 0.0428 - val_dense_17_2_loss: 0.6245 - val_dense_17_3_loss: 1.2214 - val_dense_17_4_loss: 0.0124 - val_dense_17_5_loss: 0.2455 - val_dense_17_6_loss: 1.3324 - val_dense_17_7_loss: 0.0213 - val_dense_17_8_loss: 0.7950 - val_dense_17_9_loss: 1.7353 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9908 - val_dense_17_2_accuracy: 0.7816 - val_dense_17_3_accuracy: 0.5294 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9238 - val_dense_17_6_accuracy: 0.5310 - val_dense_17_7_accuracy: 0.9992 - val_dense_17_8_accuracy: 0.6774 - val_dense_17_9_accuracy: 0.3582\n",
            "Epoch 3/40\n",
            "200/200 [==============================] - 21s 107ms/step - loss: 5.6835 - dense_17_loss: 0.0084 - dense_17_1_loss: 0.0432 - dense_17_2_loss: 0.5836 - dense_17_3_loss: 1.1245 - dense_17_4_loss: 0.0127 - dense_17_5_loss: 0.2148 - dense_17_6_loss: 1.2261 - dense_17_7_loss: 0.0200 - dense_17_8_loss: 0.7773 - dense_17_9_loss: 1.6728 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9919 - dense_17_2_accuracy: 0.7917 - dense_17_3_accuracy: 0.5801 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9507 - dense_17_6_accuracy: 0.5625 - dense_17_7_accuracy: 0.9986 - dense_17_8_accuracy: 0.7099 - dense_17_9_accuracy: 0.3772 - val_loss: 4.8270 - val_dense_17_loss: 0.0055 - val_dense_17_1_loss: 0.0308 - val_dense_17_2_loss: 0.5320 - val_dense_17_3_loss: 0.9115 - val_dense_17_4_loss: 0.0091 - val_dense_17_5_loss: 0.1740 - val_dense_17_6_loss: 1.0103 - val_dense_17_7_loss: 0.0120 - val_dense_17_8_loss: 0.6600 - val_dense_17_9_loss: 1.4818 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9962 - val_dense_17_2_accuracy: 0.7958 - val_dense_17_3_accuracy: 0.6656 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9650 - val_dense_17_6_accuracy: 0.6518 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.7632 - val_dense_17_9_accuracy: 0.4558\n",
            "Epoch 4/40\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 4.5782 - dense_17_loss: 0.0050 - dense_17_1_loss: 0.0305 - dense_17_2_loss: 0.4940 - dense_17_3_loss: 0.8341 - dense_17_4_loss: 0.0107 - dense_17_5_loss: 0.1582 - dense_17_6_loss: 0.9442 - dense_17_7_loss: 0.0126 - dense_17_8_loss: 0.6644 - dense_17_9_loss: 1.4244 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9943 - dense_17_2_accuracy: 0.8190 - dense_17_3_accuracy: 0.6937 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9678 - dense_17_6_accuracy: 0.6847 - dense_17_7_accuracy: 0.9992 - dense_17_8_accuracy: 0.7586 - dense_17_9_accuracy: 0.4795 - val_loss: 4.0411 - val_dense_17_loss: 0.0039 - val_dense_17_1_loss: 0.0242 - val_dense_17_2_loss: 0.4699 - val_dense_17_3_loss: 0.6949 - val_dense_17_4_loss: 0.0078 - val_dense_17_5_loss: 0.1362 - val_dense_17_6_loss: 0.8137 - val_dense_17_7_loss: 0.0120 - val_dense_17_8_loss: 0.5978 - val_dense_17_9_loss: 1.2806 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9968 - val_dense_17_2_accuracy: 0.8146 - val_dense_17_3_accuracy: 0.7652 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9720 - val_dense_17_6_accuracy: 0.7426 - val_dense_17_7_accuracy: 0.9996 - val_dense_17_8_accuracy: 0.7798 - val_dense_17_9_accuracy: 0.5404\n",
            "Epoch 5/40\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 3.8243 - dense_17_loss: 0.0034 - dense_17_1_loss: 0.0243 - dense_17_2_loss: 0.4396 - dense_17_3_loss: 0.6417 - dense_17_4_loss: 0.0092 - dense_17_5_loss: 0.1259 - dense_17_6_loss: 0.7772 - dense_17_7_loss: 0.0092 - dense_17_8_loss: 0.5808 - dense_17_9_loss: 1.2131 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9956 - dense_17_2_accuracy: 0.8326 - dense_17_3_accuracy: 0.7822 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9756 - dense_17_6_accuracy: 0.7517 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.7905 - dense_17_9_accuracy: 0.5727 - val_loss: 3.4177 - val_dense_17_loss: 0.0028 - val_dense_17_1_loss: 0.0187 - val_dense_17_2_loss: 0.4203 - val_dense_17_3_loss: 0.5377 - val_dense_17_4_loss: 0.0088 - val_dense_17_5_loss: 0.1185 - val_dense_17_6_loss: 0.6940 - val_dense_17_7_loss: 0.0079 - val_dense_17_8_loss: 0.5167 - val_dense_17_9_loss: 1.0923 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9966 - val_dense_17_2_accuracy: 0.8356 - val_dense_17_3_accuracy: 0.8344 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9728 - val_dense_17_6_accuracy: 0.7914 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8128 - val_dense_17_9_accuracy: 0.6076\n",
            "Epoch 6/40\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 3.2645 - dense_17_loss: 0.0026 - dense_17_1_loss: 0.0200 - dense_17_2_loss: 0.3904 - dense_17_3_loss: 0.5146 - dense_17_4_loss: 0.0078 - dense_17_5_loss: 0.1068 - dense_17_6_loss: 0.6626 - dense_17_7_loss: 0.0081 - dense_17_8_loss: 0.5194 - dense_17_9_loss: 1.0323 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9966 - dense_17_2_accuracy: 0.8523 - dense_17_3_accuracy: 0.8391 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9775 - dense_17_6_accuracy: 0.8056 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.8162 - dense_17_9_accuracy: 0.6434 - val_loss: 2.9819 - val_dense_17_loss: 0.0023 - val_dense_17_1_loss: 0.0165 - val_dense_17_2_loss: 0.3834 - val_dense_17_3_loss: 0.4482 - val_dense_17_4_loss: 0.0063 - val_dense_17_5_loss: 0.1001 - val_dense_17_6_loss: 0.5996 - val_dense_17_7_loss: 0.0073 - val_dense_17_8_loss: 0.4731 - val_dense_17_9_loss: 0.9451 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9970 - val_dense_17_2_accuracy: 0.8468 - val_dense_17_3_accuracy: 0.8752 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9754 - val_dense_17_6_accuracy: 0.8216 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8344 - val_dense_17_9_accuracy: 0.6736\n",
            "Epoch 7/40\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 2.8864 - dense_17_loss: 0.0021 - dense_17_1_loss: 0.0190 - dense_17_2_loss: 0.3605 - dense_17_3_loss: 0.4328 - dense_17_4_loss: 0.0063 - dense_17_5_loss: 0.0979 - dense_17_6_loss: 0.5692 - dense_17_7_loss: 0.0079 - dense_17_8_loss: 0.4867 - dense_17_9_loss: 0.9039 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9964 - dense_17_2_accuracy: 0.8678 - dense_17_3_accuracy: 0.8799 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9787 - dense_17_6_accuracy: 0.8409 - dense_17_7_accuracy: 0.9997 - dense_17_8_accuracy: 0.8272 - dense_17_9_accuracy: 0.6897 - val_loss: 2.6563 - val_dense_17_loss: 0.0020 - val_dense_17_1_loss: 0.0140 - val_dense_17_2_loss: 0.3541 - val_dense_17_3_loss: 0.3807 - val_dense_17_4_loss: 0.0057 - val_dense_17_5_loss: 0.0871 - val_dense_17_6_loss: 0.5214 - val_dense_17_7_loss: 0.0078 - val_dense_17_8_loss: 0.4362 - val_dense_17_9_loss: 0.8472 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9974 - val_dense_17_2_accuracy: 0.8646 - val_dense_17_3_accuracy: 0.9050 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9792 - val_dense_17_6_accuracy: 0.8570 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8444 - val_dense_17_9_accuracy: 0.7132\n",
            "Epoch 8/40\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 2.5749 - dense_17_loss: 0.0019 - dense_17_1_loss: 0.0153 - dense_17_2_loss: 0.3362 - dense_17_3_loss: 0.3684 - dense_17_4_loss: 0.0057 - dense_17_5_loss: 0.0842 - dense_17_6_loss: 0.5011 - dense_17_7_loss: 0.0071 - dense_17_8_loss: 0.4521 - dense_17_9_loss: 0.8030 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9971 - dense_17_2_accuracy: 0.8701 - dense_17_3_accuracy: 0.9083 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9807 - dense_17_6_accuracy: 0.8697 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.8401 - dense_17_9_accuracy: 0.7347 - val_loss: 2.4303 - val_dense_17_loss: 0.0017 - val_dense_17_1_loss: 0.0133 - val_dense_17_2_loss: 0.3279 - val_dense_17_3_loss: 0.3358 - val_dense_17_4_loss: 0.0054 - val_dense_17_5_loss: 0.0794 - val_dense_17_6_loss: 0.4757 - val_dense_17_7_loss: 0.0067 - val_dense_17_8_loss: 0.4106 - val_dense_17_9_loss: 0.7737 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9976 - val_dense_17_2_accuracy: 0.8804 - val_dense_17_3_accuracy: 0.9234 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9790 - val_dense_17_6_accuracy: 0.8716 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8536 - val_dense_17_9_accuracy: 0.7340\n",
            "Epoch 9/40\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 2.3560 - dense_17_loss: 0.0017 - dense_17_1_loss: 0.0143 - dense_17_2_loss: 0.3055 - dense_17_3_loss: 0.3141 - dense_17_4_loss: 0.0051 - dense_17_5_loss: 0.0815 - dense_17_6_loss: 0.4723 - dense_17_7_loss: 0.0068 - dense_17_8_loss: 0.4197 - dense_17_9_loss: 0.7351 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9973 - dense_17_2_accuracy: 0.8894 - dense_17_3_accuracy: 0.9328 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9812 - dense_17_6_accuracy: 0.8787 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.8506 - dense_17_9_accuracy: 0.7531 - val_loss: 2.2564 - val_dense_17_loss: 0.0016 - val_dense_17_1_loss: 0.0122 - val_dense_17_2_loss: 0.3022 - val_dense_17_3_loss: 0.2982 - val_dense_17_4_loss: 0.0056 - val_dense_17_5_loss: 0.0776 - val_dense_17_6_loss: 0.4313 - val_dense_17_7_loss: 0.0060 - val_dense_17_8_loss: 0.3989 - val_dense_17_9_loss: 0.7229 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9972 - val_dense_17_2_accuracy: 0.8864 - val_dense_17_3_accuracy: 0.9366 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9770 - val_dense_17_6_accuracy: 0.8846 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8536 - val_dense_17_9_accuracy: 0.7528\n",
            "Epoch 10/40\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 2.1433 - dense_17_loss: 0.0015 - dense_17_1_loss: 0.0132 - dense_17_2_loss: 0.2811 - dense_17_3_loss: 0.2806 - dense_17_4_loss: 0.0046 - dense_17_5_loss: 0.0735 - dense_17_6_loss: 0.4112 - dense_17_7_loss: 0.0064 - dense_17_8_loss: 0.4007 - dense_17_9_loss: 0.6706 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9976 - dense_17_2_accuracy: 0.9007 - dense_17_3_accuracy: 0.9404 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9822 - dense_17_6_accuracy: 0.8976 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.8541 - dense_17_9_accuracy: 0.7751 - val_loss: 2.0966 - val_dense_17_loss: 0.0014 - val_dense_17_1_loss: 0.0109 - val_dense_17_2_loss: 0.2855 - val_dense_17_3_loss: 0.2664 - val_dense_17_4_loss: 0.0041 - val_dense_17_5_loss: 0.0716 - val_dense_17_6_loss: 0.4074 - val_dense_17_7_loss: 0.0065 - val_dense_17_8_loss: 0.3711 - val_dense_17_9_loss: 0.6717 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9986 - val_dense_17_2_accuracy: 0.8968 - val_dense_17_3_accuracy: 0.9470 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9808 - val_dense_17_6_accuracy: 0.8852 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8690 - val_dense_17_9_accuracy: 0.7780\n",
            "Epoch 11/40\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 2.0070 - dense_17_loss: 0.0014 - dense_17_1_loss: 0.0121 - dense_17_2_loss: 0.2644 - dense_17_3_loss: 0.2552 - dense_17_4_loss: 0.0042 - dense_17_5_loss: 0.0709 - dense_17_6_loss: 0.3892 - dense_17_7_loss: 0.0059 - dense_17_8_loss: 0.3733 - dense_17_9_loss: 0.6305 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9980 - dense_17_2_accuracy: 0.9087 - dense_17_3_accuracy: 0.9531 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9831 - dense_17_6_accuracy: 0.9006 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.8661 - dense_17_9_accuracy: 0.7887 - val_loss: 1.9658 - val_dense_17_loss: 0.0013 - val_dense_17_1_loss: 0.0099 - val_dense_17_2_loss: 0.2652 - val_dense_17_3_loss: 0.2380 - val_dense_17_4_loss: 0.0041 - val_dense_17_5_loss: 0.0710 - val_dense_17_6_loss: 0.3773 - val_dense_17_7_loss: 0.0060 - val_dense_17_8_loss: 0.3551 - val_dense_17_9_loss: 0.6379 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9034 - val_dense_17_3_accuracy: 0.9552 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9794 - val_dense_17_6_accuracy: 0.8980 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8726 - val_dense_17_9_accuracy: 0.7846\n",
            "Epoch 12/40\n",
            "200/200 [==============================] - 22s 109ms/step - loss: 1.8789 - dense_17_loss: 0.0013 - dense_17_1_loss: 0.0115 - dense_17_2_loss: 0.2465 - dense_17_3_loss: 0.2268 - dense_17_4_loss: 0.0037 - dense_17_5_loss: 0.0654 - dense_17_6_loss: 0.3628 - dense_17_7_loss: 0.0054 - dense_17_8_loss: 0.3583 - dense_17_9_loss: 0.5973 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9982 - dense_17_2_accuracy: 0.9156 - dense_17_3_accuracy: 0.9577 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9846 - dense_17_6_accuracy: 0.9063 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.8696 - dense_17_9_accuracy: 0.7968 - val_loss: 1.8460 - val_dense_17_loss: 0.0011 - val_dense_17_1_loss: 0.0094 - val_dense_17_2_loss: 0.2495 - val_dense_17_3_loss: 0.2167 - val_dense_17_4_loss: 0.0036 - val_dense_17_5_loss: 0.0649 - val_dense_17_6_loss: 0.3631 - val_dense_17_7_loss: 0.0050 - val_dense_17_8_loss: 0.3404 - val_dense_17_9_loss: 0.5923 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9988 - val_dense_17_2_accuracy: 0.9144 - val_dense_17_3_accuracy: 0.9616 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9814 - val_dense_17_6_accuracy: 0.8996 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8802 - val_dense_17_9_accuracy: 0.8000\n",
            "Epoch 13/40\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 1.7722 - dense_17_loss: 0.0012 - dense_17_1_loss: 0.0110 - dense_17_2_loss: 0.2279 - dense_17_3_loss: 0.2089 - dense_17_4_loss: 0.0035 - dense_17_5_loss: 0.0629 - dense_17_6_loss: 0.3400 - dense_17_7_loss: 0.0050 - dense_17_8_loss: 0.3465 - dense_17_9_loss: 0.5655 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9977 - dense_17_2_accuracy: 0.9231 - dense_17_3_accuracy: 0.9632 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9840 - dense_17_6_accuracy: 0.9129 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.8763 - dense_17_9_accuracy: 0.8125 - val_loss: 1.7575 - val_dense_17_loss: 0.0011 - val_dense_17_1_loss: 0.0087 - val_dense_17_2_loss: 0.2335 - val_dense_17_3_loss: 0.2002 - val_dense_17_4_loss: 0.0034 - val_dense_17_5_loss: 0.0639 - val_dense_17_6_loss: 0.3460 - val_dense_17_7_loss: 0.0056 - val_dense_17_8_loss: 0.3286 - val_dense_17_9_loss: 0.5664 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9988 - val_dense_17_2_accuracy: 0.9202 - val_dense_17_3_accuracy: 0.9656 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9804 - val_dense_17_6_accuracy: 0.9046 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8838 - val_dense_17_9_accuracy: 0.8078\n",
            "Epoch 14/40\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 1.6993 - dense_17_loss: 0.0011 - dense_17_1_loss: 0.0105 - dense_17_2_loss: 0.2149 - dense_17_3_loss: 0.1991 - dense_17_4_loss: 0.0032 - dense_17_5_loss: 0.0619 - dense_17_6_loss: 0.3315 - dense_17_7_loss: 0.0049 - dense_17_8_loss: 0.3319 - dense_17_9_loss: 0.5403 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9980 - dense_17_2_accuracy: 0.9316 - dense_17_3_accuracy: 0.9699 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9846 - dense_17_6_accuracy: 0.9130 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.8829 - dense_17_9_accuracy: 0.8161 - val_loss: 1.6666 - val_dense_17_loss: 0.0010 - val_dense_17_1_loss: 0.0085 - val_dense_17_2_loss: 0.2233 - val_dense_17_3_loss: 0.1871 - val_dense_17_4_loss: 0.0034 - val_dense_17_5_loss: 0.0608 - val_dense_17_6_loss: 0.3249 - val_dense_17_7_loss: 0.0051 - val_dense_17_8_loss: 0.3113 - val_dense_17_9_loss: 0.5411 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9256 - val_dense_17_3_accuracy: 0.9708 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9812 - val_dense_17_6_accuracy: 0.9106 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8884 - val_dense_17_9_accuracy: 0.8172\n",
            "Epoch 15/40\n",
            "200/200 [==============================] - 22s 111ms/step - loss: 1.6143 - dense_17_loss: 0.0010 - dense_17_1_loss: 0.0106 - dense_17_2_loss: 0.2105 - dense_17_3_loss: 0.1841 - dense_17_4_loss: 0.0031 - dense_17_5_loss: 0.0620 - dense_17_6_loss: 0.3209 - dense_17_7_loss: 0.0044 - dense_17_8_loss: 0.3126 - dense_17_9_loss: 0.5050 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9980 - dense_17_2_accuracy: 0.9348 - dense_17_3_accuracy: 0.9721 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9838 - dense_17_6_accuracy: 0.9138 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.8915 - dense_17_9_accuracy: 0.8290 - val_loss: 1.5961 - val_dense_17_loss: 9.3698e-04 - val_dense_17_1_loss: 0.0082 - val_dense_17_2_loss: 0.2097 - val_dense_17_3_loss: 0.1728 - val_dense_17_4_loss: 0.0030 - val_dense_17_5_loss: 0.0588 - val_dense_17_6_loss: 0.3106 - val_dense_17_7_loss: 0.0038 - val_dense_17_8_loss: 0.3050 - val_dense_17_9_loss: 0.5232 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9304 - val_dense_17_3_accuracy: 0.9732 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9822 - val_dense_17_6_accuracy: 0.9158 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8892 - val_dense_17_9_accuracy: 0.8210\n",
            "Epoch 16/40\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 1.5141 - dense_17_loss: 9.3866e-04 - dense_17_1_loss: 0.0095 - dense_17_2_loss: 0.1918 - dense_17_3_loss: 0.1695 - dense_17_4_loss: 0.0028 - dense_17_5_loss: 0.0560 - dense_17_6_loss: 0.3051 - dense_17_7_loss: 0.0041 - dense_17_8_loss: 0.2947 - dense_17_9_loss: 0.4796 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9984 - dense_17_2_accuracy: 0.9414 - dense_17_3_accuracy: 0.9753 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9859 - dense_17_6_accuracy: 0.9178 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.8953 - dense_17_9_accuracy: 0.8409 - val_loss: 1.5332 - val_dense_17_loss: 8.7684e-04 - val_dense_17_1_loss: 0.0080 - val_dense_17_2_loss: 0.2028 - val_dense_17_3_loss: 0.1656 - val_dense_17_4_loss: 0.0026 - val_dense_17_5_loss: 0.0579 - val_dense_17_6_loss: 0.2982 - val_dense_17_7_loss: 0.0035 - val_dense_17_8_loss: 0.2904 - val_dense_17_9_loss: 0.5033 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9344 - val_dense_17_3_accuracy: 0.9764 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9822 - val_dense_17_6_accuracy: 0.9184 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.8968 - val_dense_17_9_accuracy: 0.8288\n",
            "Epoch 17/40\n",
            "200/200 [==============================] - 22s 110ms/step - loss: 1.4481 - dense_17_loss: 8.8880e-04 - dense_17_1_loss: 0.0089 - dense_17_2_loss: 0.1796 - dense_17_3_loss: 0.1551 - dense_17_4_loss: 0.0027 - dense_17_5_loss: 0.0560 - dense_17_6_loss: 0.2894 - dense_17_7_loss: 0.0044 - dense_17_8_loss: 0.2877 - dense_17_9_loss: 0.4636 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9988 - dense_17_2_accuracy: 0.9447 - dense_17_3_accuracy: 0.9796 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9859 - dense_17_6_accuracy: 0.9247 - dense_17_7_accuracy: 0.9998 - dense_17_8_accuracy: 0.8990 - dense_17_9_accuracy: 0.8468 - val_loss: 1.4737 - val_dense_17_loss: 8.2973e-04 - val_dense_17_1_loss: 0.0074 - val_dense_17_2_loss: 0.1902 - val_dense_17_3_loss: 0.1511 - val_dense_17_4_loss: 0.0025 - val_dense_17_5_loss: 0.0538 - val_dense_17_6_loss: 0.2911 - val_dense_17_7_loss: 0.0043 - val_dense_17_8_loss: 0.2840 - val_dense_17_9_loss: 0.4884 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9410 - val_dense_17_3_accuracy: 0.9788 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9830 - val_dense_17_6_accuracy: 0.9180 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.9006 - val_dense_17_9_accuracy: 0.8356\n",
            "Epoch 18/40\n",
            "200/200 [==============================] - 22s 111ms/step - loss: 1.3923 - dense_17_loss: 8.0332e-04 - dense_17_1_loss: 0.0083 - dense_17_2_loss: 0.1668 - dense_17_3_loss: 0.1433 - dense_17_4_loss: 0.0026 - dense_17_5_loss: 0.0524 - dense_17_6_loss: 0.2799 - dense_17_7_loss: 0.0038 - dense_17_8_loss: 0.2801 - dense_17_9_loss: 0.4544 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9989 - dense_17_2_accuracy: 0.9490 - dense_17_3_accuracy: 0.9828 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9857 - dense_17_6_accuracy: 0.9251 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9004 - dense_17_9_accuracy: 0.8502 - val_loss: 1.4187 - val_dense_17_loss: 8.1997e-04 - val_dense_17_1_loss: 0.0072 - val_dense_17_2_loss: 0.1793 - val_dense_17_3_loss: 0.1424 - val_dense_17_4_loss: 0.0023 - val_dense_17_5_loss: 0.0536 - val_dense_17_6_loss: 0.2831 - val_dense_17_7_loss: 0.0046 - val_dense_17_8_loss: 0.2760 - val_dense_17_9_loss: 0.4693 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9470 - val_dense_17_3_accuracy: 0.9800 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9816 - val_dense_17_6_accuracy: 0.9204 - val_dense_17_7_accuracy: 0.9996 - val_dense_17_8_accuracy: 0.9046 - val_dense_17_9_accuracy: 0.8400\n",
            "Epoch 19/40\n",
            "200/200 [==============================] - 22s 111ms/step - loss: 1.3692 - dense_17_loss: 7.7747e-04 - dense_17_1_loss: 0.0079 - dense_17_2_loss: 0.1659 - dense_17_3_loss: 0.1427 - dense_17_4_loss: 0.0025 - dense_17_5_loss: 0.0538 - dense_17_6_loss: 0.2771 - dense_17_7_loss: 0.0036 - dense_17_8_loss: 0.2736 - dense_17_9_loss: 0.4413 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9991 - dense_17_2_accuracy: 0.9519 - dense_17_3_accuracy: 0.9811 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9856 - dense_17_6_accuracy: 0.9256 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.9041 - dense_17_9_accuracy: 0.8520 - val_loss: 1.3678 - val_dense_17_loss: 7.2550e-04 - val_dense_17_1_loss: 0.0070 - val_dense_17_2_loss: 0.1727 - val_dense_17_3_loss: 0.1340 - val_dense_17_4_loss: 0.0023 - val_dense_17_5_loss: 0.0537 - val_dense_17_6_loss: 0.2738 - val_dense_17_7_loss: 0.0038 - val_dense_17_8_loss: 0.2657 - val_dense_17_9_loss: 0.4540 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9494 - val_dense_17_3_accuracy: 0.9820 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9832 - val_dense_17_6_accuracy: 0.9232 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.9060 - val_dense_17_9_accuracy: 0.8460\n",
            "Epoch 20/40\n",
            "200/200 [==============================] - 22s 111ms/step - loss: 1.2944 - dense_17_loss: 7.3132e-04 - dense_17_1_loss: 0.0071 - dense_17_2_loss: 0.1532 - dense_17_3_loss: 0.1282 - dense_17_4_loss: 0.0023 - dense_17_5_loss: 0.0493 - dense_17_6_loss: 0.2596 - dense_17_7_loss: 0.0036 - dense_17_8_loss: 0.2550 - dense_17_9_loss: 0.4353 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9990 - dense_17_2_accuracy: 0.9572 - dense_17_3_accuracy: 0.9831 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9870 - dense_17_6_accuracy: 0.9296 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.9095 - dense_17_9_accuracy: 0.8530 - val_loss: 1.3376 - val_dense_17_loss: 6.9033e-04 - val_dense_17_1_loss: 0.0064 - val_dense_17_2_loss: 0.1651 - val_dense_17_3_loss: 0.1285 - val_dense_17_4_loss: 0.0021 - val_dense_17_5_loss: 0.0519 - val_dense_17_6_loss: 0.2671 - val_dense_17_7_loss: 0.0032 - val_dense_17_8_loss: 0.2617 - val_dense_17_9_loss: 0.4509 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9994 - val_dense_17_2_accuracy: 0.9526 - val_dense_17_3_accuracy: 0.9822 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9818 - val_dense_17_6_accuracy: 0.9236 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.9072 - val_dense_17_9_accuracy: 0.8470\n",
            "Epoch 21/40\n",
            "200/200 [==============================] - 22s 113ms/step - loss: 1.2663 - dense_17_loss: 6.7820e-04 - dense_17_1_loss: 0.0072 - dense_17_2_loss: 0.1484 - dense_17_3_loss: 0.1254 - dense_17_4_loss: 0.0022 - dense_17_5_loss: 0.0502 - dense_17_6_loss: 0.2572 - dense_17_7_loss: 0.0032 - dense_17_8_loss: 0.2564 - dense_17_9_loss: 0.4154 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9990 - dense_17_2_accuracy: 0.9601 - dense_17_3_accuracy: 0.9854 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9865 - dense_17_6_accuracy: 0.9277 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9099 - dense_17_9_accuracy: 0.8609 - val_loss: 1.2928 - val_dense_17_loss: 7.4327e-04 - val_dense_17_1_loss: 0.0064 - val_dense_17_2_loss: 0.1583 - val_dense_17_3_loss: 0.1223 - val_dense_17_4_loss: 0.0019 - val_dense_17_5_loss: 0.0521 - val_dense_17_6_loss: 0.2645 - val_dense_17_7_loss: 0.0032 - val_dense_17_8_loss: 0.2537 - val_dense_17_9_loss: 0.4297 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9544 - val_dense_17_3_accuracy: 0.9842 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9834 - val_dense_17_6_accuracy: 0.9244 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.9100 - val_dense_17_9_accuracy: 0.8542\n",
            "Epoch 22/40\n",
            "200/200 [==============================] - 22s 112ms/step - loss: 1.2238 - dense_17_loss: 6.6548e-04 - dense_17_1_loss: 0.0077 - dense_17_2_loss: 0.1413 - dense_17_3_loss: 0.1192 - dense_17_4_loss: 0.0020 - dense_17_5_loss: 0.0486 - dense_17_6_loss: 0.2549 - dense_17_7_loss: 0.0031 - dense_17_8_loss: 0.2435 - dense_17_9_loss: 0.4027 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9988 - dense_17_2_accuracy: 0.9608 - dense_17_3_accuracy: 0.9870 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9864 - dense_17_6_accuracy: 0.9288 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.9135 - dense_17_9_accuracy: 0.8666 - val_loss: 1.2773 - val_dense_17_loss: 5.9813e-04 - val_dense_17_1_loss: 0.0066 - val_dense_17_2_loss: 0.1526 - val_dense_17_3_loss: 0.1196 - val_dense_17_4_loss: 0.0022 - val_dense_17_5_loss: 0.0528 - val_dense_17_6_loss: 0.2558 - val_dense_17_7_loss: 0.0025 - val_dense_17_8_loss: 0.2474 - val_dense_17_9_loss: 0.4374 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9558 - val_dense_17_3_accuracy: 0.9834 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9818 - val_dense_17_6_accuracy: 0.9268 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9148 - val_dense_17_9_accuracy: 0.8482\n",
            "Epoch 23/40\n",
            "200/200 [==============================] - 22s 113ms/step - loss: 1.1904 - dense_17_loss: 6.0788e-04 - dense_17_1_loss: 0.0068 - dense_17_2_loss: 0.1345 - dense_17_3_loss: 0.1131 - dense_17_4_loss: 0.0020 - dense_17_5_loss: 0.0458 - dense_17_6_loss: 0.2426 - dense_17_7_loss: 0.0029 - dense_17_8_loss: 0.2427 - dense_17_9_loss: 0.3995 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9992 - dense_17_2_accuracy: 0.9650 - dense_17_3_accuracy: 0.9872 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9871 - dense_17_6_accuracy: 0.9335 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.9124 - dense_17_9_accuracy: 0.8628 - val_loss: 1.2169 - val_dense_17_loss: 5.8588e-04 - val_dense_17_1_loss: 0.0062 - val_dense_17_2_loss: 0.1450 - val_dense_17_3_loss: 0.1094 - val_dense_17_4_loss: 0.0019 - val_dense_17_5_loss: 0.0500 - val_dense_17_6_loss: 0.2452 - val_dense_17_7_loss: 0.0028 - val_dense_17_8_loss: 0.2383 - val_dense_17_9_loss: 0.4174 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9582 - val_dense_17_3_accuracy: 0.9868 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9832 - val_dense_17_6_accuracy: 0.9284 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.9176 - val_dense_17_9_accuracy: 0.8590\n",
            "Epoch 24/40\n",
            "200/200 [==============================] - 22s 112ms/step - loss: 1.1535 - dense_17_loss: 5.7227e-04 - dense_17_1_loss: 0.0069 - dense_17_2_loss: 0.1268 - dense_17_3_loss: 0.1060 - dense_17_4_loss: 0.0019 - dense_17_5_loss: 0.0473 - dense_17_6_loss: 0.2388 - dense_17_7_loss: 0.0028 - dense_17_8_loss: 0.2360 - dense_17_9_loss: 0.3865 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9991 - dense_17_2_accuracy: 0.9654 - dense_17_3_accuracy: 0.9888 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9868 - dense_17_6_accuracy: 0.9329 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9155 - dense_17_9_accuracy: 0.8662 - val_loss: 1.1905 - val_dense_17_loss: 6.1582e-04 - val_dense_17_1_loss: 0.0060 - val_dense_17_2_loss: 0.1398 - val_dense_17_3_loss: 0.1065 - val_dense_17_4_loss: 0.0017 - val_dense_17_5_loss: 0.0495 - val_dense_17_6_loss: 0.2433 - val_dense_17_7_loss: 0.0026 - val_dense_17_8_loss: 0.2332 - val_dense_17_9_loss: 0.4073 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9612 - val_dense_17_3_accuracy: 0.9872 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9838 - val_dense_17_6_accuracy: 0.9296 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9192 - val_dense_17_9_accuracy: 0.8600\n",
            "Epoch 25/40\n",
            "200/200 [==============================] - 22s 112ms/step - loss: 1.1248 - dense_17_loss: 5.6268e-04 - dense_17_1_loss: 0.0070 - dense_17_2_loss: 0.1260 - dense_17_3_loss: 0.1043 - dense_17_4_loss: 0.0018 - dense_17_5_loss: 0.0439 - dense_17_6_loss: 0.2304 - dense_17_7_loss: 0.0026 - dense_17_8_loss: 0.2299 - dense_17_9_loss: 0.3782 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9989 - dense_17_2_accuracy: 0.9666 - dense_17_3_accuracy: 0.9887 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9871 - dense_17_6_accuracy: 0.9370 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.9192 - dense_17_9_accuracy: 0.8716 - val_loss: 1.1661 - val_dense_17_loss: 5.1256e-04 - val_dense_17_1_loss: 0.0060 - val_dense_17_2_loss: 0.1372 - val_dense_17_3_loss: 0.1054 - val_dense_17_4_loss: 0.0016 - val_dense_17_5_loss: 0.0481 - val_dense_17_6_loss: 0.2391 - val_dense_17_7_loss: 0.0032 - val_dense_17_8_loss: 0.2293 - val_dense_17_9_loss: 0.3958 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9620 - val_dense_17_3_accuracy: 0.9860 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9834 - val_dense_17_6_accuracy: 0.9310 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.9182 - val_dense_17_9_accuracy: 0.8624\n",
            "Epoch 26/40\n",
            "200/200 [==============================] - 23s 113ms/step - loss: 1.0886 - dense_17_loss: 5.2915e-04 - dense_17_1_loss: 0.0069 - dense_17_2_loss: 0.1186 - dense_17_3_loss: 0.0973 - dense_17_4_loss: 0.0018 - dense_17_5_loss: 0.0444 - dense_17_6_loss: 0.2264 - dense_17_7_loss: 0.0026 - dense_17_8_loss: 0.2219 - dense_17_9_loss: 0.3682 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9991 - dense_17_2_accuracy: 0.9703 - dense_17_3_accuracy: 0.9905 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9867 - dense_17_6_accuracy: 0.9358 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9218 - dense_17_9_accuracy: 0.8739 - val_loss: 1.1399 - val_dense_17_loss: 5.3540e-04 - val_dense_17_1_loss: 0.0058 - val_dense_17_2_loss: 0.1309 - val_dense_17_3_loss: 0.0988 - val_dense_17_4_loss: 0.0016 - val_dense_17_5_loss: 0.0461 - val_dense_17_6_loss: 0.2339 - val_dense_17_7_loss: 0.0029 - val_dense_17_8_loss: 0.2271 - val_dense_17_9_loss: 0.3922 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9662 - val_dense_17_3_accuracy: 0.9886 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9836 - val_dense_17_6_accuracy: 0.9324 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.9198 - val_dense_17_9_accuracy: 0.8674\n",
            "Epoch 27/40\n",
            "200/200 [==============================] - 22s 112ms/step - loss: 1.0462 - dense_17_loss: 5.2162e-04 - dense_17_1_loss: 0.0077 - dense_17_2_loss: 0.1113 - dense_17_3_loss: 0.0932 - dense_17_4_loss: 0.0017 - dense_17_5_loss: 0.0408 - dense_17_6_loss: 0.2170 - dense_17_7_loss: 0.0023 - dense_17_8_loss: 0.2186 - dense_17_9_loss: 0.3531 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9981 - dense_17_2_accuracy: 0.9736 - dense_17_3_accuracy: 0.9916 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9882 - dense_17_6_accuracy: 0.9387 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9209 - dense_17_9_accuracy: 0.8831 - val_loss: 1.1142 - val_dense_17_loss: 5.0114e-04 - val_dense_17_1_loss: 0.0056 - val_dense_17_2_loss: 0.1246 - val_dense_17_3_loss: 0.0949 - val_dense_17_4_loss: 0.0016 - val_dense_17_5_loss: 0.0503 - val_dense_17_6_loss: 0.2307 - val_dense_17_7_loss: 0.0020 - val_dense_17_8_loss: 0.2168 - val_dense_17_9_loss: 0.3873 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9658 - val_dense_17_3_accuracy: 0.9888 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9834 - val_dense_17_6_accuracy: 0.9302 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9244 - val_dense_17_9_accuracy: 0.8664\n",
            "Epoch 28/40\n",
            "200/200 [==============================] - 23s 113ms/step - loss: 1.0672 - dense_17_loss: 4.8294e-04 - dense_17_1_loss: 0.0073 - dense_17_2_loss: 0.1173 - dense_17_3_loss: 0.1032 - dense_17_4_loss: 0.0015 - dense_17_5_loss: 0.0434 - dense_17_6_loss: 0.2238 - dense_17_7_loss: 0.0022 - dense_17_8_loss: 0.2150 - dense_17_9_loss: 0.3529 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9987 - dense_17_2_accuracy: 0.9694 - dense_17_3_accuracy: 0.9903 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9872 - dense_17_6_accuracy: 0.9350 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.9247 - dense_17_9_accuracy: 0.8796 - val_loss: 1.0854 - val_dense_17_loss: 4.7337e-04 - val_dense_17_1_loss: 0.0055 - val_dense_17_2_loss: 0.1214 - val_dense_17_3_loss: 0.0902 - val_dense_17_4_loss: 0.0015 - val_dense_17_5_loss: 0.0454 - val_dense_17_6_loss: 0.2274 - val_dense_17_7_loss: 0.0030 - val_dense_17_8_loss: 0.2143 - val_dense_17_9_loss: 0.3762 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9680 - val_dense_17_3_accuracy: 0.9896 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9834 - val_dense_17_6_accuracy: 0.9318 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.9274 - val_dense_17_9_accuracy: 0.8698\n",
            "Epoch 29/40\n",
            "200/200 [==============================] - 23s 113ms/step - loss: 1.0092 - dense_17_loss: 4.7059e-04 - dense_17_1_loss: 0.0061 - dense_17_2_loss: 0.1088 - dense_17_3_loss: 0.0894 - dense_17_4_loss: 0.0015 - dense_17_5_loss: 0.0414 - dense_17_6_loss: 0.2085 - dense_17_7_loss: 0.0025 - dense_17_8_loss: 0.2026 - dense_17_9_loss: 0.3480 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9992 - dense_17_2_accuracy: 0.9716 - dense_17_3_accuracy: 0.9924 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9884 - dense_17_6_accuracy: 0.9412 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9284 - dense_17_9_accuracy: 0.8821 - val_loss: 1.0681 - val_dense_17_loss: 4.5873e-04 - val_dense_17_1_loss: 0.0055 - val_dense_17_2_loss: 0.1189 - val_dense_17_3_loss: 0.0878 - val_dense_17_4_loss: 0.0014 - val_dense_17_5_loss: 0.0454 - val_dense_17_6_loss: 0.2234 - val_dense_17_7_loss: 0.0028 - val_dense_17_8_loss: 0.2108 - val_dense_17_9_loss: 0.3717 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9682 - val_dense_17_3_accuracy: 0.9904 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9848 - val_dense_17_6_accuracy: 0.9320 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9266 - val_dense_17_9_accuracy: 0.8702\n",
            "Epoch 30/40\n",
            "200/200 [==============================] - 23s 113ms/step - loss: 1.0003 - dense_17_loss: 4.5203e-04 - dense_17_1_loss: 0.0063 - dense_17_2_loss: 0.1062 - dense_17_3_loss: 0.0868 - dense_17_4_loss: 0.0015 - dense_17_5_loss: 0.0415 - dense_17_6_loss: 0.2112 - dense_17_7_loss: 0.0023 - dense_17_8_loss: 0.2016 - dense_17_9_loss: 0.3424 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9991 - dense_17_2_accuracy: 0.9744 - dense_17_3_accuracy: 0.9922 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9874 - dense_17_6_accuracy: 0.9397 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9293 - dense_17_9_accuracy: 0.8822 - val_loss: 1.0457 - val_dense_17_loss: 4.4012e-04 - val_dense_17_1_loss: 0.0053 - val_dense_17_2_loss: 0.1140 - val_dense_17_3_loss: 0.0848 - val_dense_17_4_loss: 0.0014 - val_dense_17_5_loss: 0.0467 - val_dense_17_6_loss: 0.2197 - val_dense_17_7_loss: 0.0022 - val_dense_17_8_loss: 0.2040 - val_dense_17_9_loss: 0.3671 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9692 - val_dense_17_3_accuracy: 0.9912 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9838 - val_dense_17_6_accuracy: 0.9336 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9312 - val_dense_17_9_accuracy: 0.8714\n",
            "Epoch 31/40\n",
            "200/200 [==============================] - 23s 113ms/step - loss: 0.9659 - dense_17_loss: 4.3381e-04 - dense_17_1_loss: 0.0057 - dense_17_2_loss: 0.0987 - dense_17_3_loss: 0.0849 - dense_17_4_loss: 0.0014 - dense_17_5_loss: 0.0403 - dense_17_6_loss: 0.2039 - dense_17_7_loss: 0.0020 - dense_17_8_loss: 0.1998 - dense_17_9_loss: 0.3289 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9993 - dense_17_2_accuracy: 0.9773 - dense_17_3_accuracy: 0.9927 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9879 - dense_17_6_accuracy: 0.9411 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9315 - dense_17_9_accuracy: 0.8901 - val_loss: 1.0216 - val_dense_17_loss: 3.9299e-04 - val_dense_17_1_loss: 0.0053 - val_dense_17_2_loss: 0.1105 - val_dense_17_3_loss: 0.0827 - val_dense_17_4_loss: 0.0013 - val_dense_17_5_loss: 0.0432 - val_dense_17_6_loss: 0.2151 - val_dense_17_7_loss: 0.0022 - val_dense_17_8_loss: 0.2039 - val_dense_17_9_loss: 0.3571 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9704 - val_dense_17_3_accuracy: 0.9920 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9842 - val_dense_17_6_accuracy: 0.9322 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9290 - val_dense_17_9_accuracy: 0.8770\n",
            "Epoch 32/40\n",
            "200/200 [==============================] - 23s 114ms/step - loss: 0.9663 - dense_17_loss: 4.1243e-04 - dense_17_1_loss: 0.0063 - dense_17_2_loss: 0.0988 - dense_17_3_loss: 0.0869 - dense_17_4_loss: 0.0013 - dense_17_5_loss: 0.0420 - dense_17_6_loss: 0.2048 - dense_17_7_loss: 0.0020 - dense_17_8_loss: 0.1951 - dense_17_9_loss: 0.3287 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9993 - dense_17_2_accuracy: 0.9769 - dense_17_3_accuracy: 0.9921 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9862 - dense_17_6_accuracy: 0.9400 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9330 - dense_17_9_accuracy: 0.8869 - val_loss: 1.0065 - val_dense_17_loss: 3.8184e-04 - val_dense_17_1_loss: 0.0052 - val_dense_17_2_loss: 0.1102 - val_dense_17_3_loss: 0.0798 - val_dense_17_4_loss: 0.0012 - val_dense_17_5_loss: 0.0424 - val_dense_17_6_loss: 0.2122 - val_dense_17_7_loss: 0.0022 - val_dense_17_8_loss: 0.2006 - val_dense_17_9_loss: 0.3523 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9724 - val_dense_17_3_accuracy: 0.9920 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9846 - val_dense_17_6_accuracy: 0.9336 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9296 - val_dense_17_9_accuracy: 0.8784\n",
            "Epoch 33/40\n",
            "200/200 [==============================] - 23s 114ms/step - loss: 0.9461 - dense_17_loss: 3.9597e-04 - dense_17_1_loss: 0.0058 - dense_17_2_loss: 0.0964 - dense_17_3_loss: 0.0831 - dense_17_4_loss: 0.0012 - dense_17_5_loss: 0.0384 - dense_17_6_loss: 0.2010 - dense_17_7_loss: 0.0019 - dense_17_8_loss: 0.1926 - dense_17_9_loss: 0.3253 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9992 - dense_17_2_accuracy: 0.9768 - dense_17_3_accuracy: 0.9937 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9880 - dense_17_6_accuracy: 0.9404 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.9335 - dense_17_9_accuracy: 0.8881 - val_loss: 0.9853 - val_dense_17_loss: 3.6510e-04 - val_dense_17_1_loss: 0.0051 - val_dense_17_2_loss: 0.1052 - val_dense_17_3_loss: 0.0790 - val_dense_17_4_loss: 0.0012 - val_dense_17_5_loss: 0.0413 - val_dense_17_6_loss: 0.2073 - val_dense_17_7_loss: 0.0020 - val_dense_17_8_loss: 0.1971 - val_dense_17_9_loss: 0.3468 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9720 - val_dense_17_3_accuracy: 0.9914 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9860 - val_dense_17_6_accuracy: 0.9346 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9324 - val_dense_17_9_accuracy: 0.8778\n",
            "Epoch 34/40\n",
            "200/200 [==============================] - 23s 114ms/step - loss: 0.9314 - dense_17_loss: 3.8253e-04 - dense_17_1_loss: 0.0062 - dense_17_2_loss: 0.0930 - dense_17_3_loss: 0.0831 - dense_17_4_loss: 0.0012 - dense_17_5_loss: 0.0417 - dense_17_6_loss: 0.2010 - dense_17_7_loss: 0.0017 - dense_17_8_loss: 0.1866 - dense_17_9_loss: 0.3165 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9991 - dense_17_2_accuracy: 0.9795 - dense_17_3_accuracy: 0.9925 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9862 - dense_17_6_accuracy: 0.9418 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9372 - dense_17_9_accuracy: 0.8951 - val_loss: 0.9847 - val_dense_17_loss: 3.5254e-04 - val_dense_17_1_loss: 0.0050 - val_dense_17_2_loss: 0.1030 - val_dense_17_3_loss: 0.0763 - val_dense_17_4_loss: 0.0012 - val_dense_17_5_loss: 0.0421 - val_dense_17_6_loss: 0.2158 - val_dense_17_7_loss: 0.0022 - val_dense_17_8_loss: 0.1961 - val_dense_17_9_loss: 0.3427 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9754 - val_dense_17_3_accuracy: 0.9930 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9836 - val_dense_17_6_accuracy: 0.9334 - val_dense_17_7_accuracy: 0.9998 - val_dense_17_8_accuracy: 0.9322 - val_dense_17_9_accuracy: 0.8804\n",
            "Epoch 35/40\n",
            "200/200 [==============================] - 22s 112ms/step - loss: 0.9028 - dense_17_loss: 3.6960e-04 - dense_17_1_loss: 0.0060 - dense_17_2_loss: 0.0859 - dense_17_3_loss: 0.0715 - dense_17_4_loss: 0.0012 - dense_17_5_loss: 0.0408 - dense_17_6_loss: 0.1964 - dense_17_7_loss: 0.0017 - dense_17_8_loss: 0.1867 - dense_17_9_loss: 0.3123 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9991 - dense_17_2_accuracy: 0.9807 - dense_17_3_accuracy: 0.9942 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9870 - dense_17_6_accuracy: 0.9433 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9351 - dense_17_9_accuracy: 0.8926 - val_loss: 0.9537 - val_dense_17_loss: 3.5716e-04 - val_dense_17_1_loss: 0.0049 - val_dense_17_2_loss: 0.1005 - val_dense_17_3_loss: 0.0739 - val_dense_17_4_loss: 0.0011 - val_dense_17_5_loss: 0.0417 - val_dense_17_6_loss: 0.2056 - val_dense_17_7_loss: 0.0020 - val_dense_17_8_loss: 0.1898 - val_dense_17_9_loss: 0.3340 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9738 - val_dense_17_3_accuracy: 0.9938 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9846 - val_dense_17_6_accuracy: 0.9356 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9356 - val_dense_17_9_accuracy: 0.8832\n",
            "Epoch 36/40\n",
            "200/200 [==============================] - 23s 114ms/step - loss: 0.8919 - dense_17_loss: 3.5396e-04 - dense_17_1_loss: 0.0065 - dense_17_2_loss: 0.0874 - dense_17_3_loss: 0.0758 - dense_17_4_loss: 0.0012 - dense_17_5_loss: 0.0382 - dense_17_6_loss: 0.1927 - dense_17_7_loss: 0.0017 - dense_17_8_loss: 0.1800 - dense_17_9_loss: 0.3080 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9990 - dense_17_2_accuracy: 0.9808 - dense_17_3_accuracy: 0.9938 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9878 - dense_17_6_accuracy: 0.9418 - dense_17_7_accuracy: 0.9999 - dense_17_8_accuracy: 0.9393 - dense_17_9_accuracy: 0.8962 - val_loss: 0.9414 - val_dense_17_loss: 3.6339e-04 - val_dense_17_1_loss: 0.0048 - val_dense_17_2_loss: 0.0974 - val_dense_17_3_loss: 0.0717 - val_dense_17_4_loss: 0.0010 - val_dense_17_5_loss: 0.0413 - val_dense_17_6_loss: 0.2009 - val_dense_17_7_loss: 0.0018 - val_dense_17_8_loss: 0.1883 - val_dense_17_9_loss: 0.3337 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9738 - val_dense_17_3_accuracy: 0.9938 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9834 - val_dense_17_6_accuracy: 0.9364 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9346 - val_dense_17_9_accuracy: 0.8826\n",
            "Epoch 37/40\n",
            "200/200 [==============================] - 23s 115ms/step - loss: 0.8756 - dense_17_loss: 3.4581e-04 - dense_17_1_loss: 0.0058 - dense_17_2_loss: 0.0854 - dense_17_3_loss: 0.0702 - dense_17_4_loss: 0.0011 - dense_17_5_loss: 0.0390 - dense_17_6_loss: 0.1939 - dense_17_7_loss: 0.0018 - dense_17_8_loss: 0.1775 - dense_17_9_loss: 0.3006 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9991 - dense_17_2_accuracy: 0.9813 - dense_17_3_accuracy: 0.9947 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9882 - dense_17_6_accuracy: 0.9413 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9399 - dense_17_9_accuracy: 0.8993 - val_loss: 0.9276 - val_dense_17_loss: 3.6053e-04 - val_dense_17_1_loss: 0.0046 - val_dense_17_2_loss: 0.0957 - val_dense_17_3_loss: 0.0709 - val_dense_17_4_loss: 9.6618e-04 - val_dense_17_5_loss: 0.0408 - val_dense_17_6_loss: 0.2015 - val_dense_17_7_loss: 0.0019 - val_dense_17_8_loss: 0.1829 - val_dense_17_9_loss: 0.3279 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9750 - val_dense_17_3_accuracy: 0.9940 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9848 - val_dense_17_6_accuracy: 0.9388 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9386 - val_dense_17_9_accuracy: 0.8846\n",
            "Epoch 38/40\n",
            "200/200 [==============================] - 23s 114ms/step - loss: 0.8836 - dense_17_loss: 3.4357e-04 - dense_17_1_loss: 0.0052 - dense_17_2_loss: 0.0843 - dense_17_3_loss: 0.0711 - dense_17_4_loss: 9.8161e-04 - dense_17_5_loss: 0.0400 - dense_17_6_loss: 0.1935 - dense_17_7_loss: 0.0016 - dense_17_8_loss: 0.1825 - dense_17_9_loss: 0.3040 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9993 - dense_17_2_accuracy: 0.9809 - dense_17_3_accuracy: 0.9944 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9865 - dense_17_6_accuracy: 0.9404 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9355 - dense_17_9_accuracy: 0.8954 - val_loss: 0.9098 - val_dense_17_loss: 3.3911e-04 - val_dense_17_1_loss: 0.0047 - val_dense_17_2_loss: 0.0930 - val_dense_17_3_loss: 0.0685 - val_dense_17_4_loss: 0.0011 - val_dense_17_5_loss: 0.0406 - val_dense_17_6_loss: 0.1953 - val_dense_17_7_loss: 0.0019 - val_dense_17_8_loss: 0.1802 - val_dense_17_9_loss: 0.3243 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9766 - val_dense_17_3_accuracy: 0.9944 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9846 - val_dense_17_6_accuracy: 0.9392 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9398 - val_dense_17_9_accuracy: 0.8862\n",
            "Epoch 39/40\n",
            "200/200 [==============================] - 23s 115ms/step - loss: 0.8424 - dense_17_loss: 3.3164e-04 - dense_17_1_loss: 0.0057 - dense_17_2_loss: 0.0823 - dense_17_3_loss: 0.0684 - dense_17_4_loss: 0.0010 - dense_17_5_loss: 0.0372 - dense_17_6_loss: 0.1843 - dense_17_7_loss: 0.0017 - dense_17_8_loss: 0.1727 - dense_17_9_loss: 0.2887 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9993 - dense_17_2_accuracy: 0.9829 - dense_17_3_accuracy: 0.9948 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9894 - dense_17_6_accuracy: 0.9424 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9410 - dense_17_9_accuracy: 0.9021 - val_loss: 0.9011 - val_dense_17_loss: 3.3078e-04 - val_dense_17_1_loss: 0.0046 - val_dense_17_2_loss: 0.0920 - val_dense_17_3_loss: 0.0670 - val_dense_17_4_loss: 8.7158e-04 - val_dense_17_5_loss: 0.0397 - val_dense_17_6_loss: 0.1973 - val_dense_17_7_loss: 0.0018 - val_dense_17_8_loss: 0.1779 - val_dense_17_9_loss: 0.3196 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9990 - val_dense_17_2_accuracy: 0.9774 - val_dense_17_3_accuracy: 0.9944 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9838 - val_dense_17_6_accuracy: 0.9384 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9378 - val_dense_17_9_accuracy: 0.8910\n",
            "Epoch 40/40\n",
            "200/200 [==============================] - 23s 115ms/step - loss: 0.8370 - dense_17_loss: 3.2077e-04 - dense_17_1_loss: 0.0053 - dense_17_2_loss: 0.0792 - dense_17_3_loss: 0.0666 - dense_17_4_loss: 9.7578e-04 - dense_17_5_loss: 0.0368 - dense_17_6_loss: 0.1865 - dense_17_7_loss: 0.0015 - dense_17_8_loss: 0.1702 - dense_17_9_loss: 0.2896 - dense_17_accuracy: 1.0000 - dense_17_1_accuracy: 0.9992 - dense_17_2_accuracy: 0.9832 - dense_17_3_accuracy: 0.9957 - dense_17_4_accuracy: 1.0000 - dense_17_5_accuracy: 0.9876 - dense_17_6_accuracy: 0.9428 - dense_17_7_accuracy: 1.0000 - dense_17_8_accuracy: 0.9414 - dense_17_9_accuracy: 0.9003 - val_loss: 0.8978 - val_dense_17_loss: 3.0531e-04 - val_dense_17_1_loss: 0.0048 - val_dense_17_2_loss: 0.0908 - val_dense_17_3_loss: 0.0669 - val_dense_17_4_loss: 7.8379e-04 - val_dense_17_5_loss: 0.0403 - val_dense_17_6_loss: 0.1985 - val_dense_17_7_loss: 0.0020 - val_dense_17_8_loss: 0.1792 - val_dense_17_9_loss: 0.3143 - val_dense_17_accuracy: 1.0000 - val_dense_17_1_accuracy: 0.9992 - val_dense_17_2_accuracy: 0.9774 - val_dense_17_3_accuracy: 0.9950 - val_dense_17_4_accuracy: 1.0000 - val_dense_17_5_accuracy: 0.9836 - val_dense_17_6_accuracy: 0.9396 - val_dense_17_7_accuracy: 1.0000 - val_dense_17_8_accuracy: 0.9380 - val_dense_17_9_accuracy: 0.8930\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fc22a058518>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 324
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rK5IhcVzguWX"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "در سلول زیر ما خروجی تعدادی از ورودی که خودمان دادیم به مدل را مشاهده می کنیم.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ao5CLnMFFhQt",
        "outputId": "3e612a55-a346-4284-db29-11b1f62365bb"
      },
      "source": [
        "Example_dates = ['۳/۱/۶۱', '۱۳ دی ۸۴', 'چهارشنبه دی ماه 14 هزار و چهارصد', 'مهر ماه ۱ ۱۳۶۱', \r\n",
        "                 \"بهمن هزار و سیصد و هشتاد و دو ۱۲\", 'دوازده آذر هزار و سیصد و هفتاد', \r\n",
        "                 \" سه شهریور هزار و سیصد و هفتاد و پنج\", 'هجده آذر 99']\r\n",
        "s0 = np.zeros((1, n_s))\r\n",
        "c0 = np.zeros((1, n_s))\r\n",
        "for example in Example_dates:\r\n",
        "    \r\n",
        "    source = string_to_int(example, Tx, human)\r\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human)), source)))\r\n",
        "    source = source.reshape((1, ) + source.shape)\r\n",
        "    prediction = mod.predict([source, s0, c0])\r\n",
        "    prediction = np.argmax(prediction, axis = -1)\r\n",
        "    output = [inv_machine[int(i)] for i in prediction]\r\n",
        "    \r\n",
        "    print(\"source:\", example)\r\n",
        "    print(\"output:\", ''.join(output)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "source: ۳/۱/۶۱\n",
            "output: 1361-03-33\n",
            "source: ۱۳ دی ۸۴\n",
            "output: 1384-10-03\n",
            "source: چهارشنبه دی ماه 14 هزار و چهارصد\n",
            "output: 1400-10-14\n",
            "source: مهر ماه ۱ ۱۳۶۱\n",
            "output: 1361-07-01\n",
            "source: بهمن هزار و سیصد و هشتاد و دو ۱۲\n",
            "output: 1382-11-02\n",
            "source: دوازده آذر هزار و سیصد و هفتاد\n",
            "output: 1370-09-12\n",
            "source:  سه شهریور هزار و سیصد و هفتاد و پنج\n",
            "output: 1375-06-03\n",
            "source: هجده آذر 99\n",
            "output: 1399-09-08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHe1vlVwWpJt",
        "outputId": "a72b3c8d-3702-4305-e357-d4358e57ff46"
      },
      "source": [
        "prediction = mod.predict([source, s0, c0])\n",
        "prediction = np.argmax(prediction, axis = -1)\n",
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2],\n",
              "       [ 4],\n",
              "       [10],\n",
              "       [10],\n",
              "       [ 0],\n",
              "       [ 1],\n",
              "       [10],\n",
              "       [ 0],\n",
              "       [ 1],\n",
              "       [ 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 326
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwJsfOag3PLJ",
        "outputId": "55c79aa3-83db-4ce9-dbdf-c7ada134e5b2"
      },
      "source": [
        "pip install git+https://github.com/Maluuba/nlg-eval.git@master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/Maluuba/nlg-eval.git@master\n",
            "  Cloning https://github.com/Maluuba/nlg-eval.git (to revision master) to /tmp/pip-req-build-w8trpqjd\n",
            "  Running command git clone -q https://github.com/Maluuba/nlg-eval.git /tmp/pip-req-build-w8trpqjd\n",
            "Requirement already satisfied (use --upgrade to upgrade): nlg-eval==2.3 from git+https://github.com/Maluuba/nlg-eval.git@master in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: click>=6.3 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (7.1.2)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (3.2.5)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.19.5)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (5.8.0)\n",
            "Requirement already satisfied: requests>=2.19 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (2.23.0)\n",
            "Requirement already satisfied: six>=1.11 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.15.0)\n",
            "Requirement already satisfied: Cython>=0.28.5 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (0.29.21)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.17 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (0.22.2.post1)\n",
            "Requirement already satisfied: gensim>=3 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (3.6.0)\n",
            "Requirement already satisfied: Theano>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (1.0.5)\n",
            "Requirement already satisfied: tqdm>=4.24 in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (4.41.1)\n",
            "Requirement already satisfied: xdg in /usr/local/lib/python3.6/dist-packages (from nlg-eval==2.3) (5.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19->nlg-eval==2.3) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.17->nlg-eval==2.3) (1.0.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3->nlg-eval==2.3) (4.1.2)\n",
            "Building wheels for collected packages: nlg-eval\n",
            "  Building wheel for nlg-eval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlg-eval: filename=nlg_eval-2.3-cp36-none-any.whl size=68175149 sha256=66f118e67f5c5c4f7141fcc1e91616a44b61b6e77e9e8bbd23e6405d2a7b8576\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mf969t0n/wheels/a5/7c/fd/f312beca2adcc3f49cb40570730658dad37bb5709f5d237a56\n",
            "Successfully built nlg-eval\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75-1QTQKherX"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<h1>ارزیابی مدل</h1>\r\n",
        "<p>\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5M0zcYIAMpH"
      },
      "source": [
        "h=[]\n",
        "m=[]\n",
        "for i in testSet:\n",
        "  h.append(i[0])\n",
        "  m.append(i[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhBGO1Hl4AaM"
      },
      "source": [
        "import  nltk.translate.bleu_score as bleu\n",
        "pr=[]\n",
        "Example_dates=h\n",
        "s0 = np.zeros((1, n_s))\n",
        "c0 = np.zeros((1, n_s))\n",
        "for example in Example_dates:\n",
        "    \n",
        "    source = string_to_int(example, Tx, human)\n",
        "    source = np.array(list(map(lambda x: to_categorical(x, num_classes=len(human)), source)))\n",
        "    source = source.reshape((1, ) + source.shape)\n",
        "    prediction = mod.predict([source, s0, c0])\n",
        "    prediction = np.argmax(prediction, axis = -1)\n",
        "    output = [inv_machine[int(i)] for i in prediction]\n",
        "    pr.append(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGuDvSrjJHo7"
      },
      "source": [
        "str =[]\r\n",
        "for j in range(len(pr)):\r\n",
        "    str1 = \"\"    \r\n",
        "    for ele in pr[j]:  \r\n",
        "        str1 += ele\r\n",
        "    str.append(str1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8vTengasinq"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "belu اسکور نشان می دهد خروجی مدل تا چه اندازه با خروجی واقعی تفاوت داشته است که در این تمرین ما مقدار وزن برای همه نوع گرام برابر 25 صدم قرار دادیم.\r\n",
        "بطور کلی بررسی که انجام می دهد روی یک حرف تا 4 حرف متوالی است که آیا درست بوده یا خیر که تا حدود 94 درصد است و نشان می دهد توالی خروجی تا حد خوبی با توالی اصلی برابر است و این بسیار خوب است.\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2hl6nUs7g1z",
        "outputId": "e9a37daf-1e2f-442a-fc5a-d0d5238854d8"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "a=0\n",
        "for i in range(len(h)):\n",
        "  reference = (m[i]).split()\n",
        "  candidate = pr[i]\n",
        "  f=sentence_bleu(reference, candidate ,weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  a=a+f\n",
        "  #print(f)\n",
        "\n",
        "bl=a/(len(h))\n",
        "bl"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9477709872024812"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 331
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNSuefixk-M3"
      },
      "source": [
        "<div dir='rtl'style=\"text-align: right;font-family:verdana;\">\r\n",
        "<p>\r\n",
        "دقت و بررسی داده هایی که نتایج خوب ندارند:\r\n",
        "\r\n",
        "\r\n",
        "*   برای بررسی دقت راهکار دقیقی در کلاس معرفی نشده است و می توانیم روش های مختلفی را داشته باشیم مثلا تابع لاس که در قسمت قبل معرفی شد میزاین خطا را با توجه به تابع SparseCategoricalCrossentropy برای بردار احتمالات حروجی بر می گرداند و در هر پچ میانگیم می گیرد یا می توان آن ها که کاملا درست اند را در نظر گرفت و بررسی کرد چه تعدا خروجی کاملا درست داریم. ما می توانیم تمام روش ها را اعمال کنیم ولی با توجه به محدودیت وقت از  بررسی دقت با تمام روش ها صرف نظر می کنیم. فقط آن ها که کاملا با خروجی اصلی برابر بودند را می شماریم حدود 79.5 بوده است و این مقدار خوبی است .\r\n",
        "*   ما آن ها که درست نبوده اند را چاپ کردیم و بنظر می رسد آن هایی که اعداد پشت سر هم بودند مدل بدتر عمل کرده است. البته در اکثر موارد تشخیص مدل کاملا غلط نبوده است و برای مثال در مواجه با 01 که عدد یک را برای روز مشخص کرده است عدد را 10 در نظر گرفته\r\n",
        "\r\n",
        "۰۱ اسفند ۱۳۵۵<br>\r\n",
        "input:  1355-12-01   output:  1355-12-10\r\n",
        "\r\n",
        "7/22/86<br>\r\n",
        "input:  1386-07-22   output:  1386-02-27\r\n",
        "\r\n",
        "</p>\r\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJuPhQoaxp8d",
        "outputId": "8965e2a7-ccd7-4edb-db43-06d574cd0fdb"
      },
      "source": [
        "c=0\r\n",
        "for i in range(len(h)):\r\n",
        "  reference = m[i]\r\n",
        "  candidate = str[i]\r\n",
        "  if reference==candidate:\r\n",
        "    c=c+1\r\n",
        "  else:\r\n",
        "    print(h[i])\r\n",
        "    print(\"input: \", reference,\"  output: \",candidate)\r\n",
        "\r\n",
        "ac=c/len(h)\r\n",
        "ac"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "سهشنبه اردیبهشت ماه ۱۲ هزار و سیصد و هشتاد و هشت\n",
            "input:  1388-02-12   output:  1388-02-21\n",
            "۱/۲۲/۶۹\n",
            "input:  1369-01-22   output:  1369-02-22\n",
            "30.07.86\n",
            "input:  1386-07-30   output:  1388-07-00\n",
            "۱۲ آذر ۱۳۵۹\n",
            "input:  1359-09-12   output:  1359-09-21\n",
            "۱۳ آذر ۱۳۹۸\n",
            "input:  1398-09-13   output:  1398-09-33\n",
            "بیست و ششم دی هزار و چهارصد\n",
            "input:  1400-10-26   output:  1400-16-26\n",
            "شنبه آبان ماه 13 هزار و سیصد و نود و یک\n",
            "input:  1391-08-13   output:  1391-08-31\n",
            "۰۹.۱۰.۹۹\n",
            "input:  1399-10-09   output:  1399-09-09\n",
            "18.12.71\n",
            "input:  1371-12-18   output:  1371-11-18\n",
            "سی مرداد پنجاه\n",
            "input:  1350-05-30   output:  1350-05-00\n",
            "8/15/60\n",
            "input:  1360-08-15   output:  1360-08-18\n",
            "چهارشنبه خرداد ماه 25 1395\n",
            "input:  1395-03-25   output:  1395-03-23\n",
            "سوم دی ماه هزار و سیصد و نود و هفت\n",
            "input:  1397-10-03   output:  1397-11-33\n",
            "دو شنبه دی ماه ۳۱ شصت و سه\n",
            "input:  1363-10-31   output:  1363-10-11\n",
            "سهشنبه شهریور ماه 2 1350\n",
            "input:  1350-06-02   output:  1350-06-20\n",
            "مرداد ۳۱ ۱۳۶۵\n",
            "input:  1365-05-31   output:  1365-05-33\n",
            "6/1/81\n",
            "input:  1381-06-01   output:  1381-01-03\n",
            "1/12/93\n",
            "input:  1393-01-12   output:  1393-11-01\n",
            "دو‌شنبه آذر ماه ۲ ۱۳۸۲\n",
            "input:  1382-09-02   output:  1382-09-22\n",
            "۳۱ مهر هزار و سیصد و هشتاد و شش\n",
            "input:  1386-07-31   output:  1386-07-13\n",
            "چهار‌شنبه آذر ماه ۱ هزار و سیصد و هشتاد و چهار\n",
            "input:  1384-09-01   output:  1384-09-13\n",
            "دهم مرداد هزار و سیصد و شصت\n",
            "input:  1360-05-10   output:  1360-05-11\n",
            "۲۴ ۰۱ ۸۸\n",
            "input:  1388-01-24   output:  1388-04-24\n",
            "چهارشنبه آبان ماه 31 1391\n",
            "input:  1391-08-31   output:  1391-08-33\n",
            "31 07 71\n",
            "input:  1371-07-31   output:  1371-07-00\n",
            "دوشنبه دی ماه ۲ هزار و سیصد و پنجاه و دو\n",
            "input:  1352-10-02   output:  1352-10-22\n",
            "۰۷.۰۲.۹۷\n",
            "input:  1397-02-07   output:  1377-07-07\n",
            "دو‌شنبه بهمن ماه ۲۱ هزار و سیصد و پنجاه و هفت\n",
            "input:  1357-11-21   output:  1357-11-22\n",
            "یک شنبه آذر ماه ۱۸ هزار و سیصد و هفتاد و چهار\n",
            "input:  1374-09-18   output:  1374-09-19\n",
            "چهارشنبه اردیبهشت ماه ۱۳ ۱۳۷۱\n",
            "input:  1371-02-13   output:  1371-02-11\n",
            "۶/۱/۶۴\n",
            "input:  1364-06-01   output:  1364-06-06\n",
            "چهارشنبه فروردین ماه ۲۱ هزار و سیصد و هشتاد و چهار\n",
            "input:  1384-01-21   output:  1384-01-22\n",
            "11/20/80\n",
            "input:  1380-11-20   output:  1380-10-11\n",
            "01.08.64\n",
            "input:  1364-08-01   output:  1384-08-10\n",
            "21 06 78\n",
            "input:  1378-06-21   output:  1388-07-22\n",
            "سی مهر ماه هزار و سیصد و هشتاد و چهار\n",
            "input:  1384-07-30   output:  1384-07-33\n",
            "دهم بهمن هزار و سیصد و هفتاد و شش\n",
            "input:  1376-11-10   output:  1376-11-07\n",
            "15.01.65\n",
            "input:  1365-01-15   output:  1365-11-15\n",
            "پنجشنبه مرداد 3 1370\n",
            "input:  1370-05-03   output:  1370-05-30\n",
            "۰۳.۰۱.۸۴\n",
            "input:  1384-01-03   output:  1384-01-33\n",
            "۳۱ فروردین ۱۳۸۶\n",
            "input:  1386-01-31   output:  1386-01-11\n",
            "یازده دی ماه 1379\n",
            "input:  1379-10-11   output:  1379-10-10\n",
            "۷/۱۳/۶۳\n",
            "input:  1363-07-13   output:  1363-07-03\n",
            "12/12/87\n",
            "input:  1387-12-12   output:  1387-12-22\n",
            "۴/۲۲/۸۷\n",
            "input:  1387-04-22   output:  1387-02-24\n",
            "چهارم فروردین ۱۴۰۱\n",
            "input:  1401-01-04   output:  1400-01-14\n",
            "۱۰ ۰۳ ۵۵\n",
            "input:  1355-03-10   output:  1355-00-30\n",
            "دوشنبه مهر ماه 31 هزار و سیصد و هفتاد و پنج\n",
            "input:  1375-07-31   output:  1375-07-13\n",
            "12/2/71\n",
            "input:  1371-12-02   output:  1371-02-02\n",
            "مرداد 12 هزار و سیصد و شصت و هشت\n",
            "input:  1368-05-12   output:  1368-05-21\n",
            "هشتم اسفند هزار و سیصد و هفتاد و چهار\n",
            "input:  1374-12-08   output:  1374-12-07\n",
            "سی تیر 1368\n",
            "input:  1368-04-30   output:  1388-04-30\n",
            "شنبه فروردین ماه 13 هزار و سیصد و نود و هشت\n",
            "input:  1398-01-13   output:  1398-01-33\n",
            "چهارشنبه بهمن ماه ۲۸ هفتاد\n",
            "input:  1370-11-28   output:  1370-12-08\n",
            "چهار‌شنبه آذر ماه ۱۳ ۱۳۸۰\n",
            "input:  1380-09-13   output:  1380-09-01\n",
            "31 07 91\n",
            "input:  1391-07-31   output:  1391-07-00\n",
            "یک شنبه اردیبهشت ماه 12 هزار و سیصد و هشتاد و شش\n",
            "input:  1386-02-12   output:  1386-02-11\n",
            "۲/۱۳/۸۲\n",
            "input:  1382-02-13   output:  1382-02-23\n",
            "۱۹.۰۵.۵۱\n",
            "input:  1351-05-19   output:  1351-09-15\n",
            "12/10/56\n",
            "input:  1356-12-10   output:  1356-11-21\n",
            "۲۷.۰۵.۵۱\n",
            "input:  1351-05-27   output:  1351-07-27\n",
            "ده تیر ماه هزار و سیصد و نود و شش\n",
            "input:  1396-04-10   output:  1396-04-00\n",
            "12.02.50\n",
            "input:  1350-02-12   output:  1350-02-22\n",
            "مرداد ۲۶ هشتاد و شش\n",
            "input:  1386-05-26   output:  1366-05-26\n",
            "۷ ۰۴ ۹۳\n",
            "input:  1393-04-07   output:  1393-07-04\n",
            "پنج‌شنبه شهریور ماه 12 1355\n",
            "input:  1355-06-12   output:  1355-06-11\n",
            "سه‌شنبه بهمن ماه ۲۱ ۱۳۵۲\n",
            "input:  1352-11-21   output:  1352-11-12\n",
            "۱۵.۰۶.۵۹\n",
            "input:  1359-06-15   output:  1355-06-15\n",
            "مرداد 17 1359\n",
            "input:  1359-05-17   output:  1355-05-17\n",
            "۲۱.۰۳.۷۱\n",
            "input:  1371-03-21   output:  1371-02-03\n",
            "۱۰/۸/۵۷\n",
            "input:  1357-10-08   output:  1377-05-00\n",
            "یکشنبه دی ماه 3 هزار و سیصد و شصت و دو\n",
            "input:  1362-10-03   output:  1362-10-33\n",
            "شنبه دی ماه ۱۲ هزار و سیصد و هفتاد و شش\n",
            "input:  1376-10-12   output:  1376-10-21\n",
            "۶ ۰۲ ۸۰\n",
            "input:  1380-02-06   output:  1380-06-06\n",
            "هفدهم اردیبهشت ماه هزار و سیصد و پنجاه و چهار\n",
            "input:  1354-02-17   output:  1354-01-17\n",
            "۲۹ ۰۷ ۹۶\n",
            "input:  1396-07-29   output:  1396-09-29\n",
            "4/30/62\n",
            "input:  1362-04-30   output:  1362-04-00\n",
            "آبان ماه 13 پنجاه و هفت\n",
            "input:  1357-08-13   output:  1357-08-31\n",
            "12 اردیبهشت ماه 1364\n",
            "input:  1364-02-12   output:  1364-02-22\n",
            "چهارشنبه مرداد 1 هزار و سیصد و نود و سه\n",
            "input:  1393-05-01   output:  1393-05-11\n",
            "شنبه مهر ماه 21 1364\n",
            "input:  1364-07-21   output:  1364-07-22\n",
            "8 05 81\n",
            "input:  1381-05-08   output:  1381-08-08\n",
            "نهم تیر ماه ۱۳۷۵\n",
            "input:  1375-04-09   output:  1375-04-19\n",
            "شنبه خرداد ماه ۳۱ ۱۳۹۸\n",
            "input:  1398-03-31   output:  1398-03-33\n",
            "18.11.71\n",
            "input:  1371-11-18   output:  1371-11-17\n",
            "جمعه خرداد ماه ۱۰ ۱۳۹۷\n",
            "input:  1397-03-10   output:  1377-03-10\n",
            "سه شنبه تیر ماه ۱۲ ۱۳۷۴\n",
            "input:  1374-04-12   output:  1374-04-11\n",
            "شش اردیبهشت ماه هزار و سیصد و نود و شش\n",
            "input:  1396-02-06   output:  1366-02-06\n",
            "چهارشنبه آبان ماه ۱ هزار و سیصد و شصت و چهار\n",
            "input:  1364-08-01   output:  1364-08-11\n",
            "5/19/78\n",
            "input:  1378-05-19   output:  1378-09-15\n",
            "سی بهمن ماه 1356\n",
            "input:  1356-11-30   output:  1356-11-03\n",
            "۱۱/۶/۵۸\n",
            "input:  1358-11-06   output:  1358-11-16\n",
            "سی ام بهمن ۱۳۸۳\n",
            "input:  1383-11-30   output:  1383-11-03\n",
            "دی ماه ۱۳ ۱۳۶۱\n",
            "input:  1361-10-13   output:  1361-10-11\n",
            "8/11/96\n",
            "input:  1396-08-11   output:  1396-11-11\n",
            "28.06.64\n",
            "input:  1364-06-28   output:  1364-08-28\n",
            "سه شنبه مرداد 12 شصت و هفت\n",
            "input:  1367-05-12   output:  1367-05-11\n",
            "ده مهر ماه 1383\n",
            "input:  1383-07-10   output:  1383-07-13\n",
            "یک خرداد ماه 1364\n",
            "input:  1364-03-01   output:  1364-03-03\n",
            "12/30/55\n",
            "input:  1355-12-30   output:  1355-10-20\n",
            "8 05 80\n",
            "input:  1380-05-08   output:  1380-08-08\n",
            "بیست و پنج آبان 1395\n",
            "input:  1395-08-25   output:  1355-08-25\n",
            "خرداد ماه 17 1367\n",
            "input:  1367-03-17   output:  1377-03-17\n",
            "۰۲.۰۷.۹۴\n",
            "input:  1394-07-02   output:  1394-07-00\n",
            "پنجشنبه بهمن ماه 27 شصت\n",
            "input:  1360-11-27   output:  1360-11-07\n",
            "دو‌شنبه خرداد ماه 21 پنجاه و هفت\n",
            "input:  1357-03-21   output:  1357-03-22\n",
            "سی و یک آبان ماه هزار و سیصد و هشتاد\n",
            "input:  1380-08-31   output:  1380-08-33\n",
            "۸ مهر ماه سیصد و شصت\n",
            "input:  1360-07-08   output:  1360-08-08\n",
            "۲۸ ۰۵ ۵۷\n",
            "input:  1357-05-28   output:  1357-05-25\n",
            "۲۰.۰۴.۶۹\n",
            "input:  1369-04-20   output:  1399-04-20\n",
            "12/26/67\n",
            "input:  1367-12-26   output:  1367-02-26\n",
            "31 اسفند ماه شصت و یک\n",
            "input:  1361-12-31   output:  1361-12-33\n",
            "۴/۷/۶۰\n",
            "input:  1360-04-07   output:  1360-07-04\n",
            "جمعه مهر ماه 13 نود و هشت\n",
            "input:  1398-07-13   output:  1398-07-11\n",
            "01.06.87\n",
            "input:  1387-06-01   output:  1387-07-10\n",
            "بیست اسفند 1397\n",
            "input:  1397-12-20   output:  1397-12-23\n",
            "شنبه فروردین ماه 9 چهارصد و یک\n",
            "input:  1401-01-09   output:  1390-01-09\n",
            "6 05 63\n",
            "input:  1363-05-06   output:  1363-06-06\n",
            "سهشنبه مهر ماه 1 1388\n",
            "input:  1388-07-01   output:  1388-07-11\n",
            "پنجشنبه تیر ماه ۱۶ ۱۳۵۰\n",
            "input:  1350-04-16   output:  1350-06-16\n",
            "بیست و ششم تیر نود و هشت\n",
            "input:  1398-04-26   output:  1398-06-26\n",
            "یک‌شنبه اردیبهشت ماه 13 هزار و سیصد و هشتاد\n",
            "input:  1380-02-13   output:  1380-02-03\n",
            "۱۶.۰۶.۹۵\n",
            "input:  1395-06-16   output:  1365-06-16\n",
            "۶ ۰۲ ۸۹\n",
            "input:  1389-02-06   output:  1389-06-06\n",
            "یک‌شنبه آذر ماه ۲۱ نود و چهار\n",
            "input:  1394-09-21   output:  1394-09-12\n",
            "بیست و نهم بهمن ماه هزار و سیصد و هفتاد و سه\n",
            "input:  1373-11-29   output:  1393-11-29\n",
            "16 09 80\n",
            "input:  1380-09-16   output:  1380-06-16\n",
            "30 07 75\n",
            "input:  1375-07-30   output:  1375-07-00\n",
            "هشت فروردین ماه هزار و سیصد و هفتاد و یک\n",
            "input:  1371-01-08   output:  1371-01-07\n",
            "چهار شنبه مرداد 13 1395\n",
            "input:  1395-05-13   output:  1395-05-11\n",
            "۴ ۰۸ ۶۸\n",
            "input:  1368-08-04   output:  1388-08-04\n",
            "دی ماه 31 1372\n",
            "input:  1372-10-31   output:  1372-10-11\n",
            "۲۴ ۰۵ ۷۹\n",
            "input:  1379-05-24   output:  1379-04-24\n",
            "۱/۸/۵۷\n",
            "input:  1357-01-08   output:  1357-03-08\n",
            "۰۳.۱۲.۸۶\n",
            "input:  1386-12-03   output:  1386-02-03\n",
            "سه‌شنبه اسفند ماه 1 1389\n",
            "input:  1389-12-01   output:  1389-12-11\n",
            "سهشنبه بهمن ماه 3 1350\n",
            "input:  1350-11-03   output:  1350-11-33\n",
            "20.02.83\n",
            "input:  1383-02-20   output:  1383-02-22\n",
            "بیست فروردین 1394\n",
            "input:  1394-01-20   output:  1394-01-24\n",
            "چهار شنبه آبان ماه 13 1360\n",
            "input:  1360-08-13   output:  1360-08-11\n",
            "مهر 24 1400\n",
            "input:  1400-07-24   output:  1400-04-24\n",
            "۱/۱۷/۵۷\n",
            "input:  1357-01-17   output:  1375-01-17\n",
            "جمعه بهمن ماه ۱۳ ۱۴۰۰\n",
            "input:  1400-11-13   output:  1400-11-11\n",
            "00.05.00\n",
            "input:  1400-05-20   output:  1400-00-00\n",
            "سی ام فروردین هفتاد و پنج\n",
            "input:  1375-01-30   output:  1375-01-00\n",
            "آذر ماه ۱۲ ۱۳۶۱\n",
            "input:  1361-09-12   output:  1361-09-11\n",
            "25 01 80\n",
            "input:  1380-01-25   output:  1380-02-25\n",
            "۱۱.۰۲.۶۸\n",
            "input:  1368-02-11   output:  1368-12-11\n",
            "16.03.85\n",
            "input:  1385-03-16   output:  1385-06-16\n",
            "شنبه دی ماه 21 1397\n",
            "input:  1397-10-21   output:  1397-10-12\n",
            "22 10 68\n",
            "input:  1368-10-22   output:  1386-02-22\n",
            "آبان 13 شصت و هفت\n",
            "input:  1367-08-13   output:  1367-08-11\n",
            "7/18/93\n",
            "input:  1393-07-18   output:  1393-07-17\n",
            "12/11/75\n",
            "input:  1375-12-11   output:  1375-11-21\n",
            "سه شنبه دی ماه 12 1379\n",
            "input:  1379-10-12   output:  1379-10-11\n",
            "بیستم مهر هزار و سیصد و هشتاد و هفت\n",
            "input:  1387-07-20   output:  1387-07-23\n",
            "شش خرداد 1376\n",
            "input:  1376-03-06   output:  1366-03-06\n",
            "سه شنبه مرداد 1 هشتاد و یک\n",
            "input:  1381-05-01   output:  1381-05-11\n",
            "9/3/94\n",
            "input:  1394-09-03   output:  1394-09-09\n",
            "03.08.86\n",
            "input:  1386-08-03   output:  1386-08-00\n",
            "شنبه مهر ماه 21 هزار و سیصد و هفتاد\n",
            "input:  1370-07-21   output:  1370-07-22\n",
            "4/18/77\n",
            "input:  1377-04-18   output:  1377-08-14\n",
            "۲۰.۰۳.۷۲\n",
            "input:  1372-03-20   output:  1372-02-20\n",
            "۲۹ ۰۳ ۶۰\n",
            "input:  1360-03-29   output:  1360-09-29\n",
            "پنج‌شنبه تیر ماه 2 1372\n",
            "input:  1372-04-02   output:  1372-04-22\n",
            "سی آبان هزار و سیصد و هفتاد و یک\n",
            "input:  1371-08-30   output:  1371-08-33\n",
            "6/22/69\n",
            "input:  1369-06-22   output:  1369-02-26\n",
            "نهم تیر ماه ۱۳۹۰\n",
            "input:  1390-04-09   output:  1390-04-19\n",
            "۰۱.۰۶.۹۹\n",
            "input:  1399-06-01   output:  1399-06-10\n",
            "شنبه تیر ماه ۶ ۱۳۶۵\n",
            "input:  1365-04-06   output:  1365-06-06\n",
            "۴/۳/۵۹\n",
            "input:  1359-04-03   output:  1359-03-04\n",
            "سیزده دی 1356\n",
            "input:  1356-10-13   output:  1356-10-11\n",
            "۹۹ ۰۷ ۹۹\n",
            "input:  1399-07-19   output:  1399-09-19\n",
            "07.11.52\n",
            "input:  1352-11-07   output:  1352-11-17\n",
            "چهار شنبه تیر ماه 26 هشتاد\n",
            "input:  1380-04-26   output:  1380-04-24\n",
            "دوشنبه دی ماه 2 1352\n",
            "input:  1352-10-02   output:  1352-10-22\n",
            "شهریور ماه ۱۳ ۱۳۸۰\n",
            "input:  1380-06-13   output:  1380-06-11\n",
            "چهار‌شنبه مهر ماه ۳۱ هزار و سیصد و پنجاه و چهار\n",
            "input:  1354-07-31   output:  1354-07-33\n",
            "چهارم بهمن ماه ۱۳۹۲\n",
            "input:  1392-11-04   output:  1392-11-00\n",
            "21 03 56\n",
            "input:  1356-03-21   output:  1356-02-23\n",
            "خرداد ماه 2 هزار و سیصد و هفتاد و هفت\n",
            "input:  1377-03-02   output:  1377-03-22\n",
            "۹ ۰۲ ۹۴\n",
            "input:  1394-02-09   output:  1394-09-09\n",
            "28 05 71\n",
            "input:  1371-05-28   output:  1371-08-28\n",
            "۴/۱۹/۸۶\n",
            "input:  1386-04-19   output:  1388-04-14\n",
            "5/14/95\n",
            "input:  1395-05-14   output:  1359-05-14\n",
            "12.08.66\n",
            "input:  1366-08-12   output:  1366-07-12\n",
            "دوازدهم شهریور هزار و سیصد و هفتاد و نه\n",
            "input:  1379-06-12   output:  1379-06-11\n",
            "پنجشنبه اسفند ماه ۳۱ هزار و سیصد و هشتاد و نه\n",
            "input:  1389-12-31   output:  1389-12-33\n",
            "10 بهمن هزار و سیصد و شصت و یک\n",
            "input:  1361-11-10   output:  1361-11-01\n",
            "پنج‌شنبه فروردین ماه ۱۲ ۱۳۶۴\n",
            "input:  1364-01-12   output:  1364-01-22\n",
            "دوازده خرداد 1360\n",
            "input:  1360-03-12   output:  1360-03-11\n",
            "یکشنبه اسفند ماه 17 1397\n",
            "input:  1397-12-17   output:  1377-12-17\n",
            "فروردین ماه 12 هزار و سیصد و پنجاه\n",
            "input:  1350-01-12   output:  1350-01-11\n",
            "بیست و دو اردیبهشت 1353\n",
            "input:  1353-02-22   output:  1353-02-23\n",
            "دو شنبه خرداد ماه ۳۱ ۱۳۸۳\n",
            "input:  1383-03-31   output:  1383-03-11\n",
            "۳۰.۱۲.۶۱\n",
            "input:  1361-12-30   output:  1361-01-03\n",
            "3/27/64\n",
            "input:  1364-03-27   output:  1364-07-23\n",
            "۲/۱۷/۹۹\n",
            "input:  1399-02-17   output:  1399-02-07\n",
            "۱۷ ۰۹ ۸۶\n",
            "input:  1386-09-17   output:  1398-07-17\n",
            "۳۰.۱۲.۱۵\n",
            "input:  1394-12-30   output:  1351-11-03\n",
            "بیست و ششم مرداد ۱۳۸۶\n",
            "input:  1386-05-26   output:  1366-05-26\n",
            "آبان ماه 12 1377\n",
            "input:  1377-08-12   output:  1377-08-11\n",
            "06.10.62\n",
            "input:  1362-10-06   output:  1362-00-06\n",
            "شنبه دی ماه 31 1367\n",
            "input:  1367-10-31   output:  1367-10-33\n",
            "25.02.88\n",
            "input:  1388-02-25   output:  1388-02-22\n",
            "15.05.94\n",
            "input:  1394-05-15   output:  1354-05-15\n",
            "۱۳ ۰۲ ۵۶\n",
            "input:  1356-02-13   output:  1356-02-03\n",
            "۱۰ ۰۷ ۰۰\n",
            "input:  1400-07-10   output:  1400-00-10\n",
            "29.05.81\n",
            "input:  1381-05-29   output:  1381-09-25\n",
            "سیزده مرداد 1359\n",
            "input:  1359-05-13   output:  1359-05-10\n",
            "پنج‌شنبه آذر ماه ۱۲ ۱۳۶۵\n",
            "input:  1365-09-12   output:  1365-09-11\n",
            "۲۸.۰۹.۶۷\n",
            "input:  1367-09-28   output:  1377-08-28\n",
            "سی ام بهمن ماه هزار و سیصد و هشتاد و شش\n",
            "input:  1386-11-30   output:  1386-11-00\n",
            "۲۲ ۱۰ ۶۴\n",
            "input:  1364-10-22   output:  1364-12-22\n",
            "14 09 50\n",
            "input:  1350-09-14   output:  1350-08-14\n",
            "مرداد ۱۳ هزار و سیصد و هفتاد و هشت\n",
            "input:  1378-05-13   output:  1378-05-33\n",
            "6 02 65\n",
            "input:  1365-02-06   output:  1365-06-06\n",
            "27.01.85\n",
            "input:  1385-01-27   output:  1385-12-27\n",
            "چهار شنبه مهر ماه 2 1360\n",
            "input:  1360-07-02   output:  1360-07-22\n",
            "۰۴.۰۹.۰۰\n",
            "input:  1400-09-04   output:  1400-00-00\n",
            "دو‌شنبه مرداد 31 1362\n",
            "input:  1362-05-31   output:  1362-05-11\n",
            "سی آبان ماه 1387\n",
            "input:  1387-08-30   output:  1387-08-33\n",
            "پنج‌شنبه تیر ماه ۲۷ ۱۳۹۷\n",
            "input:  1397-04-27   output:  1377-04-27\n",
            "شنبه آبان ماه 16 شصت\n",
            "input:  1360-08-16   output:  1360-08-01\n",
            "چهار‌شنبه فروردین ماه ۳۱ ۱۳۵۹\n",
            "input:  1359-01-31   output:  1359-01-13\n",
            "یک شنبه بهمن ماه 14 نود\n",
            "input:  1390-11-14   output:  1390-11-01\n",
            "07.01.69\n",
            "input:  1369-01-07   output:  1369-07-00\n",
            "7/3/72\n",
            "input:  1372-07-03   output:  1372-07-07\n",
            "02.05.69\n",
            "input:  1369-05-02   output:  1369-05-05\n",
            "جمعه خرداد ماه 3 هزار و سیصد و هشتاد\n",
            "input:  1380-03-03   output:  1380-03-33\n",
            "7/1/84\n",
            "input:  1384-07-01   output:  1384-07-07\n",
            "جمعه آذر ماه ۱ ۱۳۵۲\n",
            "input:  1352-09-01   output:  1352-09-11\n",
            "پانزده خرداد ماه 1356\n",
            "input:  1356-03-15   output:  1356-03-19\n",
            "25.04.69\n",
            "input:  1369-04-25   output:  1366-04-25\n",
            "12 05 71\n",
            "input:  1371-05-12   output:  1371-01-02\n",
            "09.08.74\n",
            "input:  1374-08-09   output:  1374-09-09\n",
            "هجدهم فروردین هزار و سیصد و هفتاد و هشت\n",
            "input:  1378-01-18   output:  1378-01-17\n",
            "دو‌شنبه دی ماه ۱۳ هزار و سیصد و پنجاه و پنج\n",
            "input:  1355-10-13   output:  1355-10-11\n",
            "آبان ماه 13 هزار و سیصد و پنجاه و یک\n",
            "input:  1351-08-13   output:  1351-08-31\n",
            "۰۳.۰۳.۵۸\n",
            "input:  1358-03-03   output:  1358-03-33\n",
            "27 آبان هفتاد\n",
            "input:  1370-08-27   output:  1370-08-07\n",
            "4 10 69\n",
            "input:  1369-10-04   output:  1369-04-00\n",
            "29.08.56\n",
            "input:  1356-08-29   output:  1356-09-29\n",
            "پانزدهم شهریور ماه ۱۳۹۰\n",
            "input:  1390-06-15   output:  1390-06-10\n",
            "چهار شنبه فروردین ماه ۲۷ نود\n",
            "input:  1390-01-27   output:  1390-01-07\n",
            "دوشنبه مهر ماه 31 هزار و سیصد و هشتاد و شش\n",
            "input:  1386-07-31   output:  1386-07-13\n",
            "دو شنبه آذر ماه 6 1384\n",
            "input:  1384-09-06   output:  1384-09-09\n",
            "سهشنبه مرداد 2 هزار و سیصد و شصت و نه\n",
            "input:  1369-05-02   output:  1369-05-22\n",
            "۱۳ تیر ۱۳۵۷\n",
            "input:  1357-04-13   output:  1357-04-31\n",
            "۱۲ خرداد ماه هزار و سیصد و شصت و پنج\n",
            "input:  1365-03-12   output:  1365-03-11\n",
            "7 11 68\n",
            "input:  1368-11-07   output:  1388-11-07\n",
            "04.10.88\n",
            "input:  1388-10-04   output:  1388-00-00\n",
            "۲۹ ۰۷ ۷۳\n",
            "input:  1373-07-29   output:  1373-09-29\n",
            "۱۰.۱۲.۵۶\n",
            "input:  1356-12-10   output:  1356-11-11\n",
            "3/18/80\n",
            "input:  1380-03-18   output:  1380-08-13\n",
            "مهر ماه ۱۳ ۱۳۹۱\n",
            "input:  1391-07-13   output:  1391-07-11\n",
            "شانزده تیر هزار و سیصد و پنجاه\n",
            "input:  1350-04-16   output:  1350-04-19\n",
            "8/27/59\n",
            "input:  1359-08-27   output:  1359-08-28\n",
            "جمعه خرداد ماه 31 1375\n",
            "input:  1375-03-31   output:  1375-03-33\n",
            "06.01.69\n",
            "input:  1369-01-06   output:  1369-06-00\n",
            "پنج شنبه اسفند ماه ۳۱ ۱۳۷۸\n",
            "input:  1378-12-31   output:  1378-12-13\n",
            "11/23/84\n",
            "input:  1384-11-23   output:  1384-12-13\n",
            "بهمن ماه ۲۱ هزار و سیصد و شصت\n",
            "input:  1360-11-21   output:  1360-11-22\n",
            "۲۶.۰۸.۸۴\n",
            "input:  1384-08-26   output:  1384-06-26\n",
            "نهم خرداد ماه هپنجاه و یک\n",
            "input:  1351-03-09   output:  1351-03-19\n",
            "23.07.68\n",
            "input:  1368-07-23   output:  1378-07-23\n",
            "18.10.92\n",
            "input:  1392-10-18   output:  1392-11-18\n",
            "۰۶.۰۲.۶۲\n",
            "input:  1362-02-06   output:  1362-06-06\n",
            "۳۰.۱۲.۸۶\n",
            "input:  1386-12-30   output:  1386-01-03\n",
            "26 تیر 1395\n",
            "input:  1395-04-26   output:  1355-04-26\n",
            "سی خرداد هزار و سیصد و هفتاد و چهار\n",
            "input:  1374-03-30   output:  1374-03-33\n",
            "۲۳ ۰۱ ۹۴\n",
            "input:  1394-01-23   output:  1394-12-23\n",
            "28 01 62\n",
            "input:  1362-01-28   output:  1362-02-28\n",
            "۱۷ ۰۴ ۵۱\n",
            "input:  1351-04-17   output:  1351-07-04\n",
            "24 02 00\n",
            "input:  1400-02-24   output:  1400-02-20\n",
            "آذر ماه ۱۸ ۱۳۹۴\n",
            "input:  1394-09-18   output:  1394-09-19\n",
            "21 04 69\n",
            "input:  1369-04-21   output:  1399-04-24\n",
            "یکشنبه اردیبهشت ماه ۲ ۱۳۸۳\n",
            "input:  1383-02-02   output:  1383-02-22\n",
            "سی ام خرداد ۱۳۷۳\n",
            "input:  1373-03-30   output:  1373-03-33\n",
            "دوازدهم مرداد ۱۳۸۸\n",
            "input:  1388-05-12   output:  1388-05-11\n",
            "29.04.62\n",
            "input:  1362-04-29   output:  1362-02-26\n",
            "۱۹.۰۲.۹۷\n",
            "input:  1397-02-19   output:  1397-01-19\n",
            "ده تیر هزار و سیصد و هفتاد و شش\n",
            "input:  1376-04-10   output:  1376-04-00\n",
            "هشتم اسفند هفتاد و پنج\n",
            "input:  1375-12-08   output:  1375-12-07\n",
            "جمعه اردیبهشت ماه 21 هزار و سیصد و هفتاد و دو\n",
            "input:  1372-02-21   output:  1372-02-22\n",
            "7/5/98\n",
            "input:  1398-07-05   output:  1398-07-07\n",
            "دوشنبه خرداد ماه ۲ شصت و یک\n",
            "input:  1361-03-02   output:  1361-03-22\n",
            "یک شنبه مهر ماه 5 1395\n",
            "input:  1395-07-05   output:  1355-07-05\n",
            "۴ ۱۱ ۶۳\n",
            "input:  1363-11-04   output:  1363-11-03\n",
            "نوزدهم اردیبهشت ماه هزار و سیصد و پنجاه و یک\n",
            "input:  1351-02-19   output:  1351-02-18\n",
            "۵ ۰۹ ۷۲\n",
            "input:  1372-09-05   output:  1372-07-05\n",
            "۱۶ ۱۰ ۶۴\n",
            "input:  1364-10-16   output:  1364-01-16\n",
            "چهارشنبه مرداد 12 هزار و سیصد و شصت و دو\n",
            "input:  1362-05-12   output:  1362-05-11\n",
            "۲۲.۰۶.۵۶\n",
            "input:  1356-06-22   output:  1356-02-22\n",
            "۱۴ ۰۲ ۰۰\n",
            "input:  1400-02-14   output:  1400-04-10\n",
            "پنج‌شنبه تیر ماه ۱۳ ۱۳۸۶\n",
            "input:  1386-04-13   output:  1386-04-11\n",
            "01 مهر نود و سه\n",
            "input:  1393-07-01   output:  1393-07-00\n",
            "۲۱.۰۴.۵۷\n",
            "input:  1357-04-21   output:  1357-04-24\n",
            "جمعه خرداد ماه 13 هزار و سیصد و پنجاه\n",
            "input:  1350-03-13   output:  1350-03-11\n",
            "هفتم شهریور چهارصد\n",
            "input:  1400-06-07   output:  1400-07-07\n",
            "اول فروردین ماه ۲۰۱۶\n",
            "input:  1397-01-01   output:  1360-01-11\n",
            "۲/۱۱/۶۱\n",
            "input:  1361-02-11   output:  1361-11-11\n",
            "30 07 73\n",
            "input:  1373-07-30   output:  1373-07-00\n",
            "شنبه دی ماه ۱۲ ۱۳۷۱\n",
            "input:  1371-10-12   output:  1371-10-11\n",
            "۳۰.۰۳.۹۲\n",
            "input:  1392-03-30   output:  1392-03-33\n",
            "نه مرداد 1377\n",
            "input:  1377-05-09   output:  1377-05-19\n",
            "سه‌شنبه آذر ماه ۱۸ ۱۳۹۸\n",
            "input:  1398-09-18   output:  1388-09-19\n",
            "۴/۱۲/۷۸\n",
            "input:  1378-04-12   output:  1378-12-14\n",
            "3/16/59\n",
            "input:  1359-03-16   output:  1359-03-33\n",
            "۲ ۰۳ ۸۵\n",
            "input:  1385-03-02   output:  1385-02-03\n",
            "یک‌شنبه مرداد 13 1398\n",
            "input:  1398-05-13   output:  1398-05-11\n",
            "نوزدهم دی ۱۳۶۸\n",
            "input:  1368-10-19   output:  1368-11-19\n",
            "سیزدهم مرداد ۱۳۵۱\n",
            "input:  1351-05-13   output:  1351-05-10\n",
            "27.10.59\n",
            "input:  1359-10-27   output:  1359-07-27\n",
            "۲۱ ۱۲ ۹۶\n",
            "input:  1396-12-21   output:  1396-11-22\n",
            "شنبه دی ماه 1 هزار و سیصد و نود و شش\n",
            "input:  1396-10-01   output:  1396-10-11\n",
            "۲ اردیبهشت ماه هشتاد و نه\n",
            "input:  1389-02-02   output:  1389-02-22\n",
            "۱۷ ۰۸ ۹۴\n",
            "input:  1394-08-17   output:  1384-07-17\n",
            "۴/۲۱/۸۲\n",
            "input:  1382-04-21   output:  1382-02-24\n",
            "23.10.62\n",
            "input:  1362-10-23   output:  1362-02-23\n",
            "4/18/83\n",
            "input:  1383-04-18   output:  1383-04-14\n",
            "چهار‌شنبه شهریور ماه ۲۱ هزار و سیصد و هفتاد و پنج\n",
            "input:  1375-06-21   output:  1375-06-22\n",
            "شنبه مهر ماه ۳۱ هزار و سیصد و هشتاد و چهار\n",
            "input:  1384-07-31   output:  1384-07-13\n",
            "خرداد ماه 31 پنجاه\n",
            "input:  1350-03-31   output:  1350-03-11\n",
            "سیزدهم اردیبهشت هزار و سیصد و شصت و یک\n",
            "input:  1361-02-13   output:  1361-02-11\n",
            "۵/۲۰/۵۱\n",
            "input:  1351-05-20   output:  1351-05-25\n",
            "2/12/74\n",
            "input:  1374-02-12   output:  1374-02-22\n",
            "۳/۱۴/۷۰\n",
            "input:  1370-03-14   output:  1370-03-13\n",
            "۲۸ ۰۴ ۶۲\n",
            "input:  1362-04-28   output:  1362-08-24\n",
            "۱۰ ۰۲ ۹۱\n",
            "input:  1391-02-10   output:  1391-01-00\n",
            "۷/۲۶/۹۳\n",
            "input:  1393-07-26   output:  1363-07-07\n",
            "۱۲ ۱۲ ۸۸\n",
            "input:  1388-12-12   output:  1388-11-12\n",
            "04 آبان 1390\n",
            "input:  1390-08-04   output:  1390-08-00\n",
            "8 83 83\n",
            "input:  1383-03-08   output:  1383-08-08\n",
            "بیست و هفتم مرداد ۱۳۶۱\n",
            "input:  1361-05-27   output:  1361-07-27\n",
            "چهار‌شنبه فروردین ماه 12 1391\n",
            "input:  1391-01-12   output:  1391-01-11\n",
            "۱۶ ۱۲ ۸۷\n",
            "input:  1387-12-16   output:  1387-11-16\n",
            "۰۸.۰۷.۷۱\n",
            "input:  1371-07-08   output:  1371-08-08\n",
            "چهار‌شنبه شهریور ماه ۲ ۱۳۸۴\n",
            "input:  1384-06-02   output:  1384-06-22\n",
            "جمعه خرداد ماه 13 1372\n",
            "input:  1372-03-13   output:  1372-03-11\n",
            "ششم خرداد ماه ۱۳۷۶\n",
            "input:  1376-03-06   output:  1366-03-06\n",
            "آبان 12 هزار و سیصد و پنجاه\n",
            "input:  1350-08-12   output:  1350-08-11\n",
            "18 10 85\n",
            "input:  1385-10-18   output:  1385-11-18\n",
            "شنبه شهریور ماه ۱۴ ۱۳۶۰\n",
            "input:  1360-06-14   output:  1360-06-16\n",
            "سهشنبه آبان ماه 31 هزار و سیصد و هشتاد و چهار\n",
            "input:  1384-08-31   output:  1384-08-33\n",
            "۸ ۰۳ ۹۰\n",
            "input:  1390-03-08   output:  1390-08-08\n",
            "فروردین ماه 13 هزار و سیصد و نود و یک\n",
            "input:  1391-01-13   output:  1391-01-31\n",
            "سه شنبه خرداد ماه ۲۵ هزار و سیصد و نود و چهار\n",
            "input:  1394-03-25   output:  1394-03-29\n",
            "دهم تیر هفتاد و هفت\n",
            "input:  1377-04-10   output:  1377-04-19\n",
            "چهار بهمن پنجاه و هفت\n",
            "input:  1357-11-04   output:  1357-11-05\n",
            "4/26/60\n",
            "input:  1360-04-26   output:  1360-06-26\n",
            "اسفند ۱۲ هزار و سیصد و شصت و پنج\n",
            "input:  1365-12-12   output:  1365-12-11\n",
            "۱۵.۰۶.۸۵\n",
            "input:  1385-06-15   output:  1385-05-15\n",
            "۶ ۰۳ ۷۱\n",
            "input:  1371-03-06   output:  1371-07-06\n",
            "۴/۲۲/۷۵\n",
            "input:  1375-04-22   output:  1375-02-24\n",
            "۲۲ ۰۹ ۶۷\n",
            "input:  1367-09-22   output:  1367-02-22\n",
            "۳۰.۱۱.۷۹\n",
            "input:  1379-11-30   output:  1379-11-01\n",
            "سیزدهم فروردین هزار و سیصد و شصت و چهار\n",
            "input:  1364-01-13   output:  1364-01-10\n",
            "۱۶.۰۴.۵۱\n",
            "input:  1351-04-16   output:  1351-04-14\n",
            "3/27/80\n",
            "input:  1380-03-27   output:  1380-07-23\n",
            "۲۱.۰۳.۷۵\n",
            "input:  1375-03-21   output:  1375-03-23\n",
            "پنجشنبه دی ماه ۲۲ ۱۴۰۰\n",
            "input:  1400-10-22   output:  1400-10-20\n",
            "11 05 87\n",
            "input:  1387-05-11   output:  1387-01-11\n",
            "۳/۱۷/۹۶\n",
            "input:  1396-03-17   output:  1396-07-11\n",
            "3/22/76\n",
            "input:  1376-03-22   output:  1376-02-22\n",
            "یازده آذر 1395\n",
            "input:  1395-09-11   output:  1355-09-11\n",
            "پنجشنبه مرداد ۱۲ نود و یک\n",
            "input:  1391-05-12   output:  1391-05-11\n",
            "6/19/96\n",
            "input:  1396-06-19   output:  1396-19-19\n",
            "نهم خرداد ۱۳۵۰\n",
            "input:  1350-03-09   output:  1350-03-19\n",
            "هفت اردیبهشت هزار و سیصد و نود و شش\n",
            "input:  1396-02-07   output:  1366-02-07\n",
            "شنبه دی ماه 1 1363\n",
            "input:  1363-10-01   output:  1363-10-11\n",
            "۱۶ ۰۳ ۸۶\n",
            "input:  1386-03-16   output:  1388-06-16\n",
            "1 دی ماه 1374\n",
            "input:  1374-10-01   output:  1374-10-13\n",
            "3/30/87\n",
            "input:  1387-03-30   output:  1387-03-33\n",
            "ده خرداد 1382\n",
            "input:  1382-03-10   output:  1382-03-03\n",
            "سی آبان 1386\n",
            "input:  1386-08-30   output:  1386-08-33\n",
            "مهر ماه ۱۳ هزار و سیصد و نود\n",
            "input:  1390-07-13   output:  1390-07-11\n",
            "پنج‌شنبه مهر ماه 12 هزار و سیصد و هشتاد و هفت\n",
            "input:  1387-07-12   output:  1387-07-11\n",
            "۱۷ ۰۴ ۵۹\n",
            "input:  1359-04-17   output:  1359-07-14\n",
            "82.10.82\n",
            "input:  1382-10-02   output:  1382-08-28\n",
            "دهم شهریور ماه ۱۳۷۸\n",
            "input:  1378-06-10   output:  1378-06-00\n",
            "چهارشنبه مرداد ۲۲ ۱۳۸۲\n",
            "input:  1382-05-22   output:  1382-02-22\n",
            "چهار شنبه بهمن ماه ۱۲ هزار و سیصد و پنجاه و پنج\n",
            "input:  1355-11-12   output:  1355-11-11\n",
            "4/30/57\n",
            "input:  1357-04-30   output:  1377-03-30\n",
            "۹/۲۷/۶۲\n",
            "input:  1362-09-27   output:  1362-06-27\n",
            "18 06 60\n",
            "input:  1360-06-18   output:  1360-08-18\n",
            "12 اسفند ماه هزار و سیصد و نود و چهار\n",
            "input:  1394-12-12   output:  1394-12-22\n",
            "۲۸ ۱۰ ۶۸\n",
            "input:  1368-10-28   output:  1386-08-28\n",
            "دهم خرداد ۱۳۵۰\n",
            "input:  1350-03-10   output:  1350-03-19\n",
            "ده شهریور هزار و سیصد و شصت و شش\n",
            "input:  1366-06-10   output:  1366-06-01\n",
            "۱۰.۰۳.۵۶\n",
            "input:  1356-03-10   output:  1356-03-30\n",
            "تیر ماه ۲ هزار و سیصد و شصت و سه\n",
            "input:  1363-04-02   output:  1363-04-22\n",
            "شنبه اسفند ماه 31 هزار و سیصد و شصت و هشت\n",
            "input:  1368-12-31   output:  1368-12-33\n",
            "8/28/98\n",
            "input:  1398-08-28   output:  1388-08-28\n",
            "آذر ماه 3 1400\n",
            "input:  1400-09-03   output:  1400-09-00\n",
            "شنبه آذر ماه 29 1359\n",
            "input:  1359-09-29   output:  1399-09-29\n",
            "09.03.64\n",
            "input:  1364-03-09   output:  1364-09-00\n",
            "آذر ۳۰ ۱۴۰۰\n",
            "input:  1400-09-30   output:  1400-09-00\n",
            "۱۱.۱۰.۷۱\n",
            "input:  1371-10-11   output:  1371-11-11\n",
            "۱۷ ۰۲ ۹۱\n",
            "input:  1391-02-17   output:  1391-11-07\n",
            "۹ ۰۶ ۵۹\n",
            "input:  1359-06-09   output:  1399-09-09\n",
            "آذر ماه 16 1375\n",
            "input:  1375-09-16   output:  1375-06-16\n",
            "هشتم بهمن ماه ۱۳۶۷\n",
            "input:  1367-11-08   output:  1367-11-06\n",
            "دوشنبه اسفند ماه ۱ هزار و سیصد و هشتاد و هشت\n",
            "input:  1388-12-01   output:  1388-12-11\n",
            "جمعه دی ماه ۱ ۱۳۷۹\n",
            "input:  1379-10-01   output:  1379-10-11\n",
            "پنج شنبه بهمن ماه ۲۱ هزار و سیصد و هفتاد و شش\n",
            "input:  1376-11-21   output:  1376-11-22\n",
            "10 بهمن 1398\n",
            "input:  1398-11-10   output:  1398-11-01\n",
            "پنجشنبه اردیبهشت ماه ۱۳ هزار و سیصد و هشتاد و سه\n",
            "input:  1383-02-13   output:  1383-02-33\n",
            "جمعه دی ماه ۱ هشتاد و چهار\n",
            "input:  1384-10-01   output:  1384-10-11\n",
            "۲۰ ۰۶ ۷۲\n",
            "input:  1372-06-20   output:  1372-06-26\n",
            "۲۶.۰۲.۵۷\n",
            "input:  1357-02-26   output:  1357-06-26\n",
            "دو‌شنبه مرداد 13 1376\n",
            "input:  1376-05-13   output:  1376-05-11\n",
            "11 01 54\n",
            "input:  1354-01-11   output:  1354-11-11\n",
            "سی ام مهر ماه ۱۳۸۱\n",
            "input:  1381-07-30   output:  1381-07-33\n",
            "۱ ۰۳ ۶۵\n",
            "input:  1365-03-01   output:  1365-03-03\n",
            "۰۲ اسفند هزار و سیصد و هشتاد و یک\n",
            "input:  1381-12-02   output:  1381-12-21\n",
            "شانزده مرداد هزار و سیصد و نود و دو\n",
            "input:  1392-05-16   output:  1392-05-19\n",
            "سی ام آذر ۱۳۸۴\n",
            "input:  1384-09-30   output:  1384-09-33\n",
            "یک شنبه شهریور ماه ۷ هزار و چهارصد\n",
            "input:  1400-06-07   output:  1400-07-07\n",
            "1 آبان هزار و سیصد و هشتاد و دو\n",
            "input:  1382-08-01   output:  1382-08-11\n",
            "18.09.80\n",
            "input:  1380-09-18   output:  1380-08-18\n",
            "12.08.54\n",
            "input:  1354-08-12   output:  1354-01-12\n",
            "3 06 55\n",
            "input:  1355-06-03   output:  1355-03-03\n",
            "19 12 86\n",
            "input:  1386-12-19   output:  1366-12-19\n",
            "سی ام اسفند ماه ۱۳۷۴\n",
            "input:  1374-12-30   output:  1374-12-33\n",
            "15.03.82\n",
            "input:  1382-03-15   output:  1382-05-05\n",
            "02 دی 1389\n",
            "input:  1389-10-02   output:  1389-10-20\n",
            "12/3/85\n",
            "input:  1385-12-03   output:  1385-02-03\n",
            "سه شنبه مرداد 12 نود و پنج\n",
            "input:  1395-05-12   output:  1395-05-11\n",
            "پنج شنبه مهر ماه ۲۱ هزار و سیصد و هشتاد و پنج\n",
            "input:  1385-07-21   output:  1385-07-22\n",
            "شانزده خرداد ماه 1368\n",
            "input:  1368-03-16   output:  1388-03-16\n",
            "شنبه فروردین ماه ۲۷ هشتاد و یک\n",
            "input:  1381-01-27   output:  1371-01-27\n",
            "۱۰/۴/۸۵\n",
            "input:  1385-10-04   output:  1385-00-04\n",
            "۱ خرداد ۱۳۶۴\n",
            "input:  1364-03-01   output:  1364-03-03\n",
            "11/2/69\n",
            "input:  1369-11-02   output:  1369-11-01\n",
            "۱۰ ۰۶ ۸۳\n",
            "input:  1383-06-10   output:  1386-00-10\n",
            "۱۱.۱۰.۸۷\n",
            "input:  1387-10-11   output:  1387-11-11\n",
            "هفدهم آبان هزار و سیصد و پنجاه و هفت\n",
            "input:  1357-08-17   output:  1357-08-18\n",
            "12 خرداد 1353\n",
            "input:  1353-03-12   output:  1353-03-21\n",
            "دو‌شنبه فروردین ماه ۸ ۱۳۵۹\n",
            "input:  1359-01-08   output:  1399-01-08\n",
            "سه‌شنبه مهر ماه ۴ ۱۳۹۷\n",
            "input:  1397-07-04   output:  1377-07-04\n",
            "دو‌شنبه مهر ماه 21 هزار و سیصد و هشتاد و سه\n",
            "input:  1383-07-21   output:  1383-07-22\n",
            "سه شنبه مهر ماه ۳۱ ۱۳۶۴\n",
            "input:  1364-07-31   output:  1364-07-13\n",
            "۱ ۰۳ ۸۴\n",
            "input:  1384-03-01   output:  1384-03-03\n",
            "۰۸.۰۲.۷۵\n",
            "input:  1375-02-08   output:  1375-08-00\n",
            "دوشنبه مرداد ۳۱ هپنجاه و یک\n",
            "input:  1351-05-31   output:  1351-05-11\n",
            "12 آبان هزار و سیصد و هشتاد و هفت\n",
            "input:  1387-08-12   output:  1387-08-11\n",
            "شنبه فروردین ماه 1 1380\n",
            "input:  1380-01-01   output:  1380-01-11\n",
            "4/17/87\n",
            "input:  1387-04-17   output:  1377-07-14\n",
            "26.10.89\n",
            "input:  1389-10-26   output:  1389-16-26\n",
            "08.05.51\n",
            "input:  1351-05-08   output:  1351-05-05\n",
            "سی خرداد 1387\n",
            "input:  1387-03-30   output:  1387-03-33\n",
            "3/2/78\n",
            "input:  1378-03-02   output:  1378-03-03\n",
            "بیست و نهم اسفند ۱۹۸۱\n",
            "input:  1360-12-29   output:  1391-12-28\n",
            "تیر ماه 21 هزار و سیصد و نود و پنج\n",
            "input:  1395-04-21   output:  1395-04-22\n",
            "22 10 71\n",
            "input:  1371-10-22   output:  1371-01-22\n",
            "نهم خرداد ۱۳۷۱\n",
            "input:  1371-03-09   output:  1371-03-19\n",
            "یازدهم آذر ماه شصت و سه\n",
            "input:  1363-09-11   output:  1363-09-10\n",
            "16.04.61\n",
            "input:  1361-04-16   output:  1361-06-16\n",
            "26.06.95\n",
            "input:  1395-06-26   output:  1365-06-26\n",
            "۲۱ آذر هزار و سیصد و هشتاد\n",
            "input:  1380-09-21   output:  1380-09-11\n",
            "۲/۴/۸۴\n",
            "input:  1384-02-04   output:  1384-04-04\n",
            "۲۷.۰۱.۷۶\n",
            "input:  1376-01-27   output:  1376-12-27\n",
            "۱/۴/۶۴\n",
            "input:  1364-01-04   output:  1364-04-04\n",
            "26.10.77\n",
            "input:  1377-10-26   output:  1377-16-26\n",
            "سی ام اسفند ماه ۱۳۷۸\n",
            "input:  1378-12-30   output:  1378-12-33\n",
            "۲۸ ۰۶ ۷۸\n",
            "input:  1378-06-28   output:  1388-08-28\n",
            "اسفند ماه ۱۲ نود و هفت\n",
            "input:  1397-12-12   output:  1397-12-21\n",
            "چهار‌شنبه فروردین ماه 12 هزار و سیصد و پنجاه و هفت\n",
            "input:  1357-01-12   output:  1357-01-11\n",
            "آذر ماه 27 هزار و سیصد و نود و چهار\n",
            "input:  1394-09-27   output:  1394-09-29\n",
            "۲۱.۱۰.۷۹\n",
            "input:  1379-10-21   output:  1379-11-11\n",
            "21.05.52\n",
            "input:  1352-05-21   output:  1352-05-22\n",
            "21.03.93\n",
            "input:  1393-03-21   output:  1393-02-23\n",
            "سه شنبه شهریور ماه ۱۲ هزار و سیصد و شصت و چهار\n",
            "input:  1364-06-12   output:  1364-06-11\n",
            "۲۴ ۱۰ ۶۸\n",
            "input:  1368-10-24   output:  1388-02-24\n",
            "۱۳ ۰۱ ۵۸\n",
            "input:  1358-01-13   output:  1358-11-31\n",
            "بیست و دو مرداد 1357\n",
            "input:  1357-05-22   output:  1357-05-25\n",
            "۱۱/۳/۵۷\n",
            "input:  1357-11-03   output:  1357-13-13\n",
            "۰۱ تیر ۱۳۵۱\n",
            "input:  1351-04-01   output:  1351-04-10\n",
            "پنج آذر 1395\n",
            "input:  1395-09-05   output:  1355-09-05\n",
            "07.03.65\n",
            "input:  1365-03-07   output:  1355-07-00\n",
            "پنجشنبه تیر ماه ۲ ۱۳۷۲\n",
            "input:  1372-04-02   output:  1372-04-22\n",
            "11 03 81\n",
            "input:  1381-03-11   output:  1381-11-03\n",
            "۲۵.۰۴.۹۲\n",
            "input:  1392-04-25   output:  1392-05-25\n",
            "یک اردیبهشت 1358\n",
            "input:  1358-02-01   output:  1358-02-03\n",
            "سه شنبه اسفند ماه 12 1380\n",
            "input:  1380-12-12   output:  1380-12-11\n",
            "1 فروردین ماه 1798\n",
            "input:  1379-01-01   output:  1398-06-11\n",
            "۴ ۰۷ ۵۸\n",
            "input:  1358-07-04   output:  1378-07-04\n",
            "سی مهر 1383\n",
            "input:  1383-07-30   output:  1383-07-33\n",
            "۹/۱۷/۷۵\n",
            "input:  1375-09-17   output:  1375-07-17\n",
            "دو شهریور 1367\n",
            "input:  1367-06-02   output:  1367-06-05\n",
            "یکشنبه دی ماه 20 هزار و سیصد و هفتاد و شش\n",
            "input:  1376-10-20   output:  1376-10020\n",
            "دوازدهم خرداد ۱۳۸۷\n",
            "input:  1387-03-12   output:  1387-03-19\n",
            "یک مهر ماه 1393\n",
            "input:  1393-07-01   output:  1393-07-03\n",
            "3/27/77\n",
            "input:  1377-03-27   output:  1377-07-27\n",
            "جمعه دی ماه 31 1388\n",
            "input:  1388-10-31   output:  1388-10-13\n",
            "اول فروردین ۱۹۸۸\n",
            "input:  1369-01-01   output:  1398-01-11\n",
            "30 04 52\n",
            "input:  1352-04-30   output:  1352-04-00\n",
            "سهشنبه آذر ماه ۱۲ هزار و سیصد و شصت و نه\n",
            "input:  1369-09-12   output:  1369-09-11\n",
            "02.09.51\n",
            "input:  1351-09-02   output:  1351-09-05\n",
            "چهار شنبه آذر ماه ۲ هزار و سیصد و نود و پنج\n",
            "input:  1395-09-02   output:  1395-09-22\n",
            "۸ ۰۹ ۸۷\n",
            "input:  1387-09-08   output:  1387-08-08\n",
            "یک‌شنبه آذر ماه ۱۳ ۱۳۷۸\n",
            "input:  1378-09-13   output:  1378-09-11\n",
            "۱۳.۰۱.۶۶\n",
            "input:  1366-01-13   output:  1366-01-33\n",
            "1 اردیبهشت ماه هزار و سیصد و هفتاد و سه\n",
            "input:  1373-02-01   output:  1373-02-02\n",
            "۹/۸/۶۹\n",
            "input:  1369-09-08   output:  1369-09-09\n",
            "4 اسفند ماه هزار و سیصد و هشتاد و نه\n",
            "input:  1389-12-04   output:  1389-12-10\n",
            "خرداد ماه 21 1364\n",
            "input:  1364-03-21   output:  1364-03-22\n",
            "شانزده تیر هزار و سیصد و پنجاه\n",
            "input:  1350-04-16   output:  1350-04-19\n",
            "12 خرداد ماه هزار و سیصد و شصت\n",
            "input:  1360-03-12   output:  1360-03-11\n",
            "9/12/53\n",
            "input:  1353-09-12   output:  1353-09-22\n",
            "۵/۲۶/۶۴\n",
            "input:  1364-05-26   output:  1364-06-26\n",
            "3/9/64\n",
            "input:  1364-03-09   output:  1364-03-03\n",
            "دو‌شنبه مهر ماه ۲۱ ۱۳۸۸\n",
            "input:  1388-07-21   output:  1388-07-12\n",
            "1/28/77\n",
            "input:  1377-01-28   output:  1377-02-18\n",
            "25 10 85\n",
            "input:  1385-10-25   output:  1385-01-25\n",
            "سهشنبه مرداد ۳ ۱۳۸۵\n",
            "input:  1385-05-03   output:  1385-03-03\n",
            "۱۳ دی ماه نود و یک\n",
            "input:  1391-10-13   output:  1391-10-11\n",
            "18 10 63\n",
            "input:  1363-10-18   output:  1363-18-18\n",
            "۳۰.۰۳.۷۵\n",
            "input:  1375-03-30   output:  1375-03-33\n",
            "5/15/90\n",
            "input:  1390-05-15   output:  1390-05-05\n",
            "بیست و هفتم دی هزار و سیصد و پنجاه و چهار\n",
            "input:  1354-10-27   output:  1354-11-27\n",
            "جمعه اسفند ماه 12 1394\n",
            "input:  1394-12-12   output:  1394-12-21\n",
            "7/11/59\n",
            "input:  1359-07-11   output:  1359-11-07\n",
            "8 01 59\n",
            "input:  1359-01-08   output:  1359-05-08\n",
            "یک شنبه اردیبهشت ماه 19 هزار و سیصد و هفتاد و پنج\n",
            "input:  1375-02-19   output:  1377-02-19\n",
            "دوشنبه فروردین ماه ۲۱ هزار و سیصد و هفتاد و یک\n",
            "input:  1371-01-21   output:  1371-01-22\n",
            "پنج شنبه اسفند ماه ۲ هزار و سیصد و پنجاه و شش\n",
            "input:  1356-12-02   output:  1356-12-22\n",
            "شنبه اسفند ماه 21 نود و سه\n",
            "input:  1393-12-21   output:  1393-12-22\n",
            "سه‌شنبه شهریور ماه ۲ ۱۳۸۹\n",
            "input:  1389-06-02   output:  1389-06-03\n",
            "۲۶.۰۸.۵۳\n",
            "input:  1353-08-26   output:  1353-06-26\n",
            "سهشنبه آبان ماه 1 هزار و سیصد و پنجاه و هشت\n",
            "input:  1358-08-01   output:  1358-08-11\n",
            "۱۲ بهمن ماه ۱۳۹۴\n",
            "input:  1394-11-12   output:  1394-11-11\n",
            "۱/۲۶/۸۳\n",
            "input:  1383-01-26   output:  1383-02-13\n",
            "جمعه آبان ماه 1 1360\n",
            "input:  1360-08-01   output:  1360-08-11\n",
            "ده آذر هزار و سیصد و شصت و دو\n",
            "input:  1362-09-10   output:  1362-09-19\n",
            "7/8/82\n",
            "input:  1382-07-08   output:  1382-08-08\n",
            "08.12.54\n",
            "input:  1354-12-08   output:  1354-02-18\n",
            "چهار آبان هزار و سیصد و پنجاه و هشت\n",
            "input:  1358-08-04   output:  1358-08-05\n",
            "۱۳ دی هزار و سیصد و نود و یک\n",
            "input:  1391-10-13   output:  1391-10-11\n",
            "۳/۹/۷۲\n",
            "input:  1372-03-09   output:  1372-03-02\n",
            "فروردین ماه 1 1987\n",
            "input:  1368-01-01   output:  1397-01-11\n",
            "۲۷ ۱۲ ۹۷\n",
            "input:  1397-12-27   output:  1392-12-27\n",
            "۲۶ ۰۱ ۶۷\n",
            "input:  1367-01-26   output:  1367-06-26\n",
            "۱۶ مهر ۱۳۶۰\n",
            "input:  1360-07-16   output:  1360-06-16\n",
            "یازده دی ماه 1389\n",
            "input:  1389-10-11   output:  1389-10-10\n",
            "شنبه بهمن ماه 21 هزار و سیصد و شصت و هفت\n",
            "input:  1367-11-21   output:  1367-11-22\n",
            "یک آذر 1387\n",
            "input:  1387-09-01   output:  1387-09-03\n",
            "92 04 92\n",
            "input:  1392-04-12   output:  1392-04-29\n",
            "دو‌شنبه آبان ماه 27 1387\n",
            "input:  1387-08-27   output:  1377-08-27\n",
            "چهار‌شنبه آذر ماه 12 پنجاه و نه\n",
            "input:  1359-09-12   output:  1359-09-11\n",
            "مرداد ۲۳ ۱۳۵۴\n",
            "input:  1354-05-23   output:  1354-03-23\n",
            "12.03.74\n",
            "input:  1374-03-12   output:  1374-02-03\n",
            "دوشنبه آبان ماه ۳۱ هزار و چهارصد\n",
            "input:  1400-08-31   output:  1400-08-13\n",
            "12 اسفند ماه هزار و سیصد و پنجاه و هفت\n",
            "input:  1357-12-12   output:  1357-12-22\n",
            "۵/۲۱/۸۳\n",
            "input:  1383-05-21   output:  1383-02-02\n",
            "9 02 91\n",
            "input:  1391-02-09   output:  1391-09-09\n",
            "5 شهریور ماه 1361\n",
            "input:  1361-06-05   output:  1361-06-06\n",
            "4/13/91\n",
            "input:  1391-04-13   output:  1391-03-13\n",
            "دو شنبه اسفند ماه ۱۳ پنجاه و شش\n",
            "input:  1356-12-13   output:  1356-12-11\n",
            "۲۶.۰۴.۵۴\n",
            "input:  1354-04-26   output:  1354-04-24\n",
            "پانزده خرداد ماه 1372\n",
            "input:  1372-03-15   output:  1372-03-19\n",
            "نه تیر ماه 1400\n",
            "input:  1400-04-09   output:  1400-04-14\n",
            "سی ام مهر ماه هزار و سیصد و هشتاد و هشت\n",
            "input:  1388-07-30   output:  1388-07-33\n",
            "شنبه مهر ماه 31 1356\n",
            "input:  1356-07-31   output:  1356-07-33\n",
            "۲۳.۰۶.۷۰\n",
            "input:  1370-06-23   output:  1370-03-23\n",
            "شنبه مرداد ۱۳ هزار و سیصد و هشتاد\n",
            "input:  1380-05-13   output:  1380-05-33\n",
            "3/8/00\n",
            "input:  1400-03-08   output:  1400-03-30\n",
            "۳/۲۷/۹۹\n",
            "input:  1399-03-27   output:  1399-07-23\n",
            "آبان ماه 2 1384\n",
            "input:  1384-08-02   output:  1384-08-22\n",
            "یکشنبه فروردین ماه 12 شصت و شش\n",
            "input:  1366-01-12   output:  1366-01-11\n",
            "دی ماه ۲۱ هزار و سیصد و شصت\n",
            "input:  1360-10-21   output:  1360-10-11\n",
            "دوازده مهر هزار و سیصد و هفتاد و پنج\n",
            "input:  1375-07-12   output:  1375-07-11\n",
            "۹/۱۱/۰۰\n",
            "input:  1400-09-11   output:  1400-11-11\n",
            "چهار شنبه شهریور ماه 24 چهارصد\n",
            "input:  1400-06-24   output:  1400-04-24\n",
            "10/24/64\n",
            "input:  1364-10-24   output:  1364-14-14\n",
            "هشت اسفند 1371\n",
            "input:  1371-12-08   output:  1371-12-07\n",
            "دو‌شنبه تیر ماه ۱۳ هزار و سیصد و پنجاه\n",
            "input:  1350-04-13   output:  1350-04-11\n",
            "17.06.79\n",
            "input:  1379-06-17   output:  1379-07-17\n",
            "جمعه آذر ماه 1 1352\n",
            "input:  1352-09-01   output:  1352-09-11\n",
            "۴/۱۶/۹۲\n",
            "input:  1392-04-16   output:  1392-06-14\n",
            "30 06 69\n",
            "input:  1369-06-30   output:  1369-06-00\n",
            "بیست و شش تیر ماه 1383\n",
            "input:  1383-04-26   output:  1383-04-24\n",
            "16 03 79\n",
            "input:  1379-03-16   output:  1379-06-06\n",
            "23.12.97\n",
            "input:  1397-12-23   output:  1377-12-23\n",
            "۰۹.۰۴.۷۴\n",
            "input:  1374-04-09   output:  1374-09-04\n",
            "۳۱ اسفند ۱۹۹۲\n",
            "input:  1371-12-31   output:  1392-12-13\n",
            "یکشنبه مهر ماه 28 هزار و سیصد و نود و نه\n",
            "input:  1399-07-28   output:  1399-08-28\n",
            "جمعه خرداد ماه 12 1351\n",
            "input:  1351-03-12   output:  1351-03-11\n",
            "30.07.74\n",
            "input:  1374-07-30   output:  1374-07-00\n",
            "پنج شنبه دی ماه 31 1393\n",
            "input:  1393-10-31   output:  1393-10-33\n",
            "7/15/91\n",
            "input:  1391-07-15   output:  1391-07-07\n",
            "بهمن ماه 12 1357\n",
            "input:  1357-11-12   output:  1357-11-11\n",
            "پنجشنبه دی ماه ۱۲ هزار و سیصد و هفتاد و پنج\n",
            "input:  1375-10-12   output:  1375-10-11\n",
            "یک شنبه دی ماه ۱ ۱۳۵۸\n",
            "input:  1358-10-01   output:  1358-10-11\n",
            "آبان ماه 27 1397\n",
            "input:  1397-08-27   output:  1377-08-27\n",
            "۱۰.۱۰.۹۱\n",
            "input:  1391-10-10   output:  1391-01-00\n",
            "۳۰.۱۲.۵۲\n",
            "input:  1352-12-30   output:  1352-12-03\n",
            "دو شنبه آذر ماه ۲۱ ۱۳۷۲\n",
            "input:  1372-09-21   output:  1372-09-22\n",
            "شنبه آبان ماه ۳۱ هزار و سیصد و هفتاد و شش\n",
            "input:  1376-08-31   output:  1376-08-33\n",
            "مهر ماه ۱۲ هزار و سیصد و نود و یک\n",
            "input:  1391-07-12   output:  1391-07-11\n",
            "چهارم آبان ماه ۱۳۵۸\n",
            "input:  1358-08-04   output:  1358-08-00\n",
            "خرداد ماه ۳۱ شصت و دو\n",
            "input:  1362-03-31   output:  1362-03-11\n",
            "نه دی ماه هزار و سیصد و هفتاد و پنج\n",
            "input:  1375-10-09   output:  1375-10-10\n",
            "12.08.59\n",
            "input:  1359-08-12   output:  1355-08-11\n",
            "1 03 80\n",
            "input:  1380-03-01   output:  1380-03-03\n",
            "اردیبهشت ماه ۱۳ ۱۳۶۷\n",
            "input:  1367-02-13   output:  1367-02-03\n",
            "دوازده مهر ماه هزار و سیصد و نود و چهار\n",
            "input:  1394-07-12   output:  1394-07-11\n",
            "چهار خرداد ماه 1367\n",
            "input:  1367-03-04   output:  1367-03-06\n",
            "12/12/50\n",
            "input:  1350-12-12   output:  1350-12-22\n",
            "دوم آذر ۱۳۸۸\n",
            "input:  1388-09-02   output:  1388-09-05\n",
            "دو‌شنبه خرداد ماه 25 1399\n",
            "input:  1399-03-25   output:  1399-03-29\n",
            "۱۲.۰۵.۸۳\n",
            "input:  1383-05-12   output:  1383-05-02\n",
            "4/16/83\n",
            "input:  1383-04-16   output:  1383-04-14\n",
            "سه‌شنبه دی ماه 2 1359\n",
            "input:  1359-10-02   output:  1359-10-22\n",
            "8/21/82\n",
            "input:  1382-08-21   output:  1382-08-22\n",
            "سی فروردین ماه 1383\n",
            "input:  1383-01-30   output:  1383-01-33\n",
            "نوزده شهریور ماه هزار و سیصد و شصت و یک\n",
            "input:  1361-06-19   output:  1361-06-16\n",
            "بیست و ششم آذر نود و شش\n",
            "input:  1396-09-26   output:  1366-09-26\n",
            "یکشنبه آبان ماه 23 1400\n",
            "input:  1400-08-23   output:  1400-08-20\n",
            "5/13/81\n",
            "input:  1381-05-13   output:  1381-03-03\n",
            "31 05 73\n",
            "input:  1373-05-31   output:  1353-03-33\n",
            "سهشنبه بهمن ماه 25 شصت\n",
            "input:  1360-11-25   output:  1360-11-05\n",
            "۱۱/۱۰/۹۵\n",
            "input:  1395-11-10   output:  1355-10-11\n",
            "31.05.90\n",
            "input:  1390-05-31   output:  1390-03-33\n",
            "۷/۱۶/۵۶\n",
            "input:  1356-07-16   output:  1366-07-07\n",
            "27 مهر پنجاه\n",
            "input:  1350-07-27   output:  1350-07-07\n",
            "سهشنبه دی ماه 2 1364\n",
            "input:  1364-10-02   output:  1364-10-23\n",
            "خرداد ماه ۲۱ هفتاد و دو\n",
            "input:  1372-03-21   output:  1372-03-22\n",
            "دو آذر ماه 1371\n",
            "input:  1371-09-02   output:  1371-09-01\n",
            "سهشنبه دی ماه ۱ هزار و سیصد و نود و نه\n",
            "input:  1399-10-01   output:  1399-10-11\n",
            "۲۳ ۰۲ ۶۴\n",
            "input:  1364-02-23   output:  1364-03-23\n",
            "سی ام بهمن نود\n",
            "input:  1390-11-30   output:  1390-11-00\n",
            "31.03.73\n",
            "input:  1373-03-31   output:  1373-03-33\n",
            "چهاردهم خرداد ماه ۱۳۶۰\n",
            "input:  1360-03-14   output:  1360-03-16\n",
            "۰۱ دی هزار و سیصد و پنجاه و سه\n",
            "input:  1353-10-01   output:  1353-10-10\n",
            "3/10/83\n",
            "input:  1383-03-10   output:  1383-00-33\n",
            "سه شنبه اردیبهشت ماه 12 شصت و پنج\n",
            "input:  1365-02-12   output:  1365-02-11\n",
            "پنج شنبه شهریور ماه ۱ ۱۳۷۵\n",
            "input:  1375-06-01   output:  1375-06-11\n",
            "11 آبان هشتاد و دو\n",
            "input:  1382-08-11   output:  1382-08-18\n",
            "27 05 84\n",
            "input:  1384-05-27   output:  1384-07-27\n",
            "جمعه آبان ماه 12 هزار و سیصد و پنجاه و هفت\n",
            "input:  1357-08-12   output:  1357-08-11\n",
            "1/13/72\n",
            "input:  1372-01-13   output:  1372-01-01\n",
            "دو‌شنبه اردیبهشت ماه ۲ نود و پنج\n",
            "input:  1395-02-02   output:  1395-02-22\n",
            "جمعه خرداد ماه 1 1354\n",
            "input:  1354-03-01   output:  1354-03-11\n",
            "سهشنبه آبان ماه ۲۱ ۱۳۸۱\n",
            "input:  1381-08-21   output:  1381-08-11\n",
            "۲۷ ۰۴ ۹۰\n",
            "input:  1390-04-27   output:  1390-07-27\n",
            "10 08 51\n",
            "input:  1351-08-10   output:  1351-08-00\n",
            "شهریور ماه ۱۳ ۱۳۷۹\n",
            "input:  1379-06-13   output:  1379-06-33\n",
            "28.09.67\n",
            "input:  1367-09-28   output:  1377-08-28\n",
            "۲۶.۰۱.۹۱\n",
            "input:  1391-01-26   output:  1391-11-26\n",
            "۰۴.۱۱.۵۰\n",
            "input:  1350-11-04   output:  1350-11-10\n",
            "نه فروردین ماه شصت و نه\n",
            "input:  1369-01-09   output:  1399-01-09\n",
            "15 05 89\n",
            "input:  1389-05-15   output:  1385-05-15\n",
            "یک شنبه شهریور ماه 1 1394\n",
            "input:  1394-06-01   output:  1394-06-11\n",
            "07.05.70\n",
            "input:  1370-05-07   output:  1370-07-07\n",
            "دهم مرداد هزار و چهارصد\n",
            "input:  1400-05-10   output:  1400-05-11\n",
            "۱۰/۸/۶۴\n",
            "input:  1364-10-08   output:  1384-01-00\n",
            "یک‌شنبه آذر ماه 18 1374\n",
            "input:  1374-09-18   output:  1374-09-19\n",
            "نه دی ماه 1390\n",
            "input:  1390-10-09   output:  1390-10-19\n",
            "۲/۲۲/۸۷\n",
            "input:  1387-02-22   output:  1382-02-22\n",
            "ده فروردین ماه هزار و سیصد و هشتاد و سه\n",
            "input:  1383-01-10   output:  1383-01-01\n",
            "12 اردیبهشت هفتاد\n",
            "input:  1370-02-12   output:  1370-02-02\n",
            "11.02.94\n",
            "input:  1394-02-11   output:  1394-11-11\n",
            "اسفند ماه ۱۳ هزار و سیصد و شصت و هشت\n",
            "input:  1368-12-13   output:  1368-12-33\n",
            "10/1/72\n",
            "input:  1372-10-01   output:  1372-10-11\n",
            "چهارم بهمن ماه نود و سه\n",
            "input:  1393-11-04   output:  1393-11-00\n",
            "شانزدهم تیر ماه هفتاد و سه\n",
            "input:  1373-04-16   output:  1373-04-19\n",
            "21 مهر ماه هزار و سیصد و هشتاد و هفت\n",
            "input:  1387-07-21   output:  1387-07-11\n",
            "۲۲.۰۳.۷۷\n",
            "input:  1377-03-22   output:  1377-02-23\n",
            "سه شنبه بهمن ماه 12 1376\n",
            "input:  1376-11-12   output:  1376-11-11\n",
            "مرداد ۱۳ ۱۳۹۹\n",
            "input:  1399-05-13   output:  1399-05-31\n",
            "30.06.79\n",
            "input:  1379-06-30   output:  1379-06-00\n",
            "آذر ماه 12 نود و هفت\n",
            "input:  1397-09-12   output:  1397-09-11\n",
            "دو‌شنبه فروردین ماه 12 1389\n",
            "input:  1389-01-12   output:  1389-01-11\n",
            "سه‌شنبه دی ماه 31 1386\n",
            "input:  1386-10-31   output:  1386-10-11\n",
            "دوشنبه اسفند ماه 12 هزار و سیصد و هشتاد و پنج\n",
            "input:  1385-12-12   output:  1385-12-22\n",
            "۱۱/۱/۵۵\n",
            "input:  1355-11-01   output:  1355-11-11\n",
            "ده تیر ماه 1389\n",
            "input:  1389-04-10   output:  1389-04-00\n",
            "دو‌شنبه بهمن ماه 12 هزار و سیصد و هشتاد و یک\n",
            "input:  1381-11-12   output:  1381-11-11\n",
            "۶/۲۳/۹۷\n",
            "input:  1397-06-23   output:  1377-09-26\n",
            "۱۰/۲۸/۷۸\n",
            "input:  1378-10-28   output:  1387-10-28\n",
            "02 مهر 1369\n",
            "input:  1369-07-02   output:  1369-07-22\n",
            "دوازدهم تیر ماه هزار و چهارصد\n",
            "input:  1400-04-12   output:  1400-07-11\n",
            "4/25/61\n",
            "input:  1361-04-25   output:  1361-05-24\n",
            "۱۷ ۱۲ ۹۴\n",
            "input:  1394-12-17   output:  1394-11-17\n",
            "29 12 78\n",
            "input:  1378-12-29   output:  1388-12-22\n",
            "9 12 85\n",
            "input:  1385-12-09   output:  1385-01-09\n",
            "01.01.66\n",
            "input:  1366-01-01   output:  1366-00-00\n",
            "۵/۲۹/۹۵\n",
            "input:  1395-05-29   output:  1395-05-25\n",
            "سه بهمن ماه 1400\n",
            "input:  1400-11-03   output:  1400-11-10\n",
            "17 08 68\n",
            "input:  1368-08-17   output:  1388-07-17\n",
            "۵/۲۱/۸۰\n",
            "input:  1380-05-21   output:  1380-05-22\n",
            "پنج شنبه مرداد ۱۳ هفتاد و نه\n",
            "input:  1379-05-13   output:  1379-05-11\n",
            "18 03 65\n",
            "input:  1365-03-18   output:  1365-08-18\n",
            "بیست و یکم آذر ماه ۱۳۵۳\n",
            "input:  1353-09-21   output:  1353-09-23\n",
            "۲۱ آبان ماه ۱۳۵۱\n",
            "input:  1351-08-21   output:  1351-08-11\n",
            "هجدهم آذر هزار و سیصد و نود و سه\n",
            "input:  1393-09-18   output:  1393-09-14\n",
            "جمعه بهمن ماه ۲۱ هزار و سیصد و هفتاد و هفت\n",
            "input:  1377-11-21   output:  1377-11-22\n",
            "دهم بهمن هزار و سیصد و شصت و هفت\n",
            "input:  1367-11-10   output:  1367-11-09\n",
            "۱۹.۱۲.۶۹\n",
            "input:  1369-12-19   output:  1369-11-19\n",
            "10.03.50\n",
            "input:  1350-03-10   output:  1350-00-30\n",
            "31.12.61\n",
            "input:  1361-12-31   output:  1361-11-03\n",
            "12 اردیبهشت ماه شصت\n",
            "input:  1360-02-12   output:  1360-02-02\n",
            "۴/۲۷/۹۵\n",
            "input:  1395-04-27   output:  1395-02-27\n",
            "۱/۲۴/۷۸\n",
            "input:  1378-01-24   output:  1378-02-14\n",
            "12/31/79\n",
            "input:  1379-12-31   output:  1379-11-21\n",
            "مهر ماه ۲۱ ۱۳۵۸\n",
            "input:  1358-07-21   output:  1358-07-22\n",
            "3/22/95\n",
            "input:  1395-03-22   output:  1355-02-23\n",
            "شنبه شهریور ماه 21 هزار و سیصد و هشتاد و هشت\n",
            "input:  1388-06-21   output:  1388-06-11\n",
            "اسفند ماه 1 هزار و سیصد و هفتاد و سه\n",
            "input:  1373-12-01   output:  1373-12-11\n",
            "14 03 54\n",
            "input:  1354-03-14   output:  1354-04-14\n",
            "شنبه تیر ماه ۲۵ ۱۳۹۵\n",
            "input:  1395-04-25   output:  1355-04-25\n",
            "سیزدهم تیر هزار و سیصد و هشتاد\n",
            "input:  1380-04-13   output:  1380-04-11\n",
            "سی فروردین 1395\n",
            "input:  1395-01-30   output:  1395-01-00\n",
            "5/19/78\n",
            "input:  1378-05-19   output:  1378-09-15\n",
            "دو شنبه تیر ماه ۲۷ هزار و چهارصد\n",
            "input:  1400-04-27   output:  1400-07-27\n",
            "مهر ۱۰ ۱۳۹۵\n",
            "input:  1395-07-10   output:  1395-07-11\n",
            "11.03.60\n",
            "input:  1360-03-11   output:  1360-01-13\n",
            "جمعه تیر ماه ۱۳ هزار و سیصد و پنجاه و نه\n",
            "input:  1359-04-13   output:  1359-04-11\n",
            "شنبه بهمن ماه 21 هزار و سیصد و هفتاد و هشت\n",
            "input:  1378-11-21   output:  1378-11-22\n",
            "بیست و نه دی ماه 1369\n",
            "input:  1369-10-29   output:  1399-10-29\n",
            "شنبه اردیبهشت ماه ۱۳ هزار و سیصد و نود\n",
            "input:  1390-02-13   output:  1390-02-03\n",
            "۰۱.۰۶.۸۷\n",
            "input:  1387-06-01   output:  1387-06-10\n",
            "۵/۲۹/۹۹\n",
            "input:  1399-05-29   output:  1399-09-25\n",
            "۰۱.۱۲.۶۹\n",
            "input:  1369-12-01   output:  1369-11-11\n",
            "۱/۱۰/۷۹\n",
            "input:  1379-01-10   output:  1379-10-11\n",
            "پنج شنبه خرداد ماه ۱۲ هزار و سیصد و پنجاه\n",
            "input:  1350-03-12   output:  1350-03-11\n",
            "13 05 55\n",
            "input:  1355-05-13   output:  1355-05-10\n",
            "چهار‌شنبه مرداد 23 1392\n",
            "input:  1392-05-23   output:  1392-02-23\n",
            "04.02.50\n",
            "input:  1350-02-04   output:  1350-04-00\n",
            "11 03 90\n",
            "input:  1390-03-11   output:  1390-11-11\n",
            "یک بهمن 1360\n",
            "input:  1360-11-01   output:  1360-11-03\n",
            "۸۵.۰۸.۸۵\n",
            "input:  1385-08-05   output:  1385-08-28\n",
            "چهار خرداد 1360\n",
            "input:  1360-03-04   output:  1360-03-06\n",
            "شنبه مهر ماه 21 1364\n",
            "input:  1364-07-21   output:  1364-07-22\n",
            "اول اردیبهشت ماه هزار و سیصد و نود و هشت\n",
            "input:  1398-02-01   output:  1398-02-02\n",
            "چهارشنبه آبان ماه 12 1378\n",
            "input:  1378-08-12   output:  1378-08-11\n",
            "یک شنبه فروردین ماه ۱۳ هزار و سیصد و نود و سه\n",
            "input:  1393-01-13   output:  1393-01-33\n",
            "پنج شنبه اسفند ماه 21 1375\n",
            "input:  1375-12-21   output:  1375-12-22\n",
            "پنج شنبه اسفند ماه 1 1357\n",
            "input:  1357-12-01   output:  1357-12-11\n",
            "۱۶.۰۴.۵۷\n",
            "input:  1357-04-16   output:  1357-06-16\n",
            "۱۹.۰۶.۸۸\n",
            "input:  1388-06-19   output:  1388-09-19\n",
            "یکشنبه مرداد 12 1382\n",
            "input:  1382-05-12   output:  1382-05-11\n",
            "۹/۱/۷۳\n",
            "input:  1373-09-01   output:  1373-07-11\n",
            "1 12 61\n",
            "input:  1361-12-01   output:  1361-11-01\n",
            "چهار آبان هزار و سیصد و شصت و هفت\n",
            "input:  1367-08-04   output:  1367-08-08\n",
            "۷/۱۶/۷۴\n",
            "input:  1374-07-16   output:  1374-07-17\n",
            "1/20/61\n",
            "input:  1361-01-20   output:  1361-00-00\n",
            "شنبه اسفند ماه ۵ ۱۳۹۵\n",
            "input:  1395-12-05   output:  1355-12-05\n",
            "04.10.93\n",
            "input:  1393-10-04   output:  1393-00-00\n",
            "۲۱.۰۵.۹۲\n",
            "input:  1392-05-21   output:  1352-02-22\n",
            "دی 31 1353\n",
            "input:  1353-10-31   output:  1353-10-33\n",
            "03 خرداد هزار و سیصد و هفتاد\n",
            "input:  1370-03-03   output:  1370-03-30\n",
            "26 06 81\n",
            "input:  1381-06-26   output:  1361-06-26\n",
            "4 05 71\n",
            "input:  1371-05-04   output:  1371-04-04\n",
            "۱۱/۱۰/۸۰\n",
            "input:  1380-11-10   output:  1380-11-11\n",
            "جمعه مرداد 13 1391\n",
            "input:  1391-05-13   output:  1391-05-31\n",
            "مهر 6 1396\n",
            "input:  1396-07-06   output:  1366-07-06\n",
            "چهارشنبه خرداد ماه 31 1362\n",
            "input:  1362-03-31   output:  1362-03-13\n",
            "7/21/98\n",
            "input:  1398-07-21   output:  1398-02-07\n",
            "بهمن 24 1364\n",
            "input:  1364-11-24   output:  1364-12-24\n",
            "۲۳ مهر ۱۳۹۷\n",
            "input:  1397-07-23   output:  1377-07-23\n",
            "08.08.62\n",
            "input:  1362-08-08   output:  1382-08-08\n",
            "بهمن ماه 21 1392\n",
            "input:  1392-11-21   output:  1392-11-22\n",
            "چهار شنبه مرداد ۳۱ ۱۳۹۷\n",
            "input:  1397-05-31   output:  1397-05-13\n",
            "سه‌شنبه فروردین ماه ۱۳ هزار و سیصد و هشتاد و چهار\n",
            "input:  1384-01-13   output:  1384-01-33\n",
            "شنبه اسفند ماه ۲۱ هزار و سیصد و هشتاد و دو\n",
            "input:  1382-12-21   output:  1382-12-22\n",
            "26.01.60\n",
            "input:  1360-01-26   output:  1360-12-26\n",
            "سی دی هزار و سیصد و نود و سه\n",
            "input:  1393-10-30   output:  1393-10-00\n",
            "۶/۲۱/۸۲\n",
            "input:  1382-06-21   output:  1382-16-26\n",
            "۱۸.۰۹.۹۵\n",
            "input:  1395-09-18   output:  1395-09-19\n",
            "اردیبهشت ۲۹ هشتاد\n",
            "input:  1380-02-29   output:  1380-02-09\n",
            "18 09 71\n",
            "input:  1371-09-18   output:  1391-08-18\n",
            "30 04 92\n",
            "input:  1392-04-30   output:  1392-04-00\n",
            "شش بهمن ماه 1384\n",
            "input:  1384-11-06   output:  1384-11-03\n",
            "11.12.65\n",
            "input:  1365-12-11   output:  1365-11-11\n",
            "یک‌شنبه مرداد ۱۲ نود و نه\n",
            "input:  1399-05-12   output:  1399-05-11\n",
            "هفدهم آذر ۱۳۹۳\n",
            "input:  1393-09-17   output:  1393-09-19\n",
            "10/18/58\n",
            "input:  1358-10-18   output:  1388-10-18\n",
            "15 08 90\n",
            "input:  1390-08-15   output:  1390-08-18\n",
            "جمعه شهریور ماه ۱۲ شصت و یک\n",
            "input:  1361-06-12   output:  1361-06-11\n",
            "پنج شنبه خرداد ماه ۱۲ ۱۳۶۱\n",
            "input:  1361-03-12   output:  1361-03-11\n",
            "۱۲ ۰۸ ۹۶\n",
            "input:  1396-08-12   output:  1396-08-22\n",
            "دهم مهر ماه ۱۳۹۸\n",
            "input:  1398-07-10   output:  1398-07-00\n",
            "14 10 69\n",
            "input:  1369-10-14   output:  1369-04-14\n",
            "پنج شنبه دی ماه ۲۲ ۱۴۰۰\n",
            "input:  1400-10-22   output:  1400-10-20\n",
            "۹/۷/۵۵\n",
            "input:  1355-09-07   output:  1355-09-09\n",
            "سیزدهم آبان هزار و سیصد و پنجاه و هشت\n",
            "input:  1358-08-13   output:  1358-08-10\n",
            "۱۷.۰۱.۷۳\n",
            "input:  1373-01-17   output:  1373-11-17\n",
            "۹/۱۴/۶۹\n",
            "input:  1369-09-14   output:  1399-09-14\n",
            "1/23/53\n",
            "input:  1353-01-23   output:  1353-03-33\n",
            "۰۸.۰۹.۸۲\n",
            "input:  1382-09-08   output:  1382-08-08\n",
            "سی و یک آبان ماه هزار و سیصد و شصت و چهار\n",
            "input:  1364-08-31   output:  1364-08-33\n",
            "۲۲.۱۲.۹۱\n",
            "input:  1391-12-22   output:  1391-11-22\n",
            "16 10 85\n",
            "input:  1385-10-16   output:  1385-16-16\n",
            "۲۵.۱۰.۶۷\n",
            "input:  1367-10-25   output:  1377-12-25\n",
            "2/24/94\n",
            "input:  1394-02-24   output:  1394-04-24\n",
            "جمعه دی ماه 10 شصت\n",
            "input:  1360-10-10   output:  1360-10-11\n",
            "شانزده آبان هفتاد و سه\n",
            "input:  1373-08-16   output:  1373-08-19\n",
            "۹/۵/۶۳\n",
            "input:  1363-09-05   output:  1363-09-09\n",
            "5/28/76\n",
            "input:  1376-05-28   output:  1376-08-25\n",
            "تیر ۱۳ پنجاه و هفت\n",
            "input:  1357-04-13   output:  1357-04-11\n",
            "13 02 62\n",
            "input:  1362-02-13   output:  1362-02-03\n",
            "23 04 56\n",
            "input:  1356-04-23   output:  1356-03-23\n",
            "00 04 00\n",
            "input:  1400-04-20   output:  1400-00-10\n",
            "3/25/71\n",
            "input:  1371-03-25   output:  1371-05-23\n",
            "سه شنبه اسفند ماه 18 شصت و چهار\n",
            "input:  1364-12-18   output:  1384-12-18\n",
            "یک‌شنبه دی ماه ۱۲ هزار و سیصد و پنجاه و پنج\n",
            "input:  1355-10-12   output:  1355-10-11\n",
            "دو‌شنبه مهر ماه ۱۳ ۱۴۰۰\n",
            "input:  1400-07-13   output:  1400-07-11\n",
            "۳۱ اسفند ماه ۱۹۸۰\n",
            "input:  1359-12-31   output:  1380-12-13\n",
            "۶/۳۰/۶۹\n",
            "input:  1369-06-30   output:  1369-06-36\n",
            "12 04 60\n",
            "input:  1360-04-12   output:  1360-04-11\n",
            "6/22/93\n",
            "input:  1393-06-22   output:  1393-09-26\n",
            "۱۹.۸۲.۸۲\n",
            "input:  1382-02-19   output:  1382-02-18\n",
            "8/23/90\n",
            "input:  1390-08-23   output:  1390-08-28\n",
            "دو شنبه خرداد ماه 20 هزار و سیصد و پنجاه و هشت\n",
            "input:  1358-03-20   output:  1358-03-22\n",
            "05.03.97\n",
            "input:  1397-03-05   output:  1377-05-05\n",
            "یازدهم آذر ماه هزار و سیصد و نود و سه\n",
            "input:  1393-09-11   output:  1393-09-10\n",
            "جمعه شهریور ماه ۱۲ ۱۳۵۰\n",
            "input:  1350-06-12   output:  1350-06-11\n",
            "29.04.00\n",
            "input:  1400-04-29   output:  1400-09-29\n",
            "پنج‌شنبه اسفند ماه 25 1360\n",
            "input:  1360-12-25   output:  1360-15-25\n",
            "سهشنبه تیر ماه 13 1390\n",
            "input:  1390-04-13   output:  1390-04-11\n",
            "۲۱.۰۸.۸۱\n",
            "input:  1381-08-21   output:  1381-01-22\n",
            "4/10/59\n",
            "input:  1359-04-10   output:  1359-00-10\n",
            "۱ ۰۲ ۵۴\n",
            "input:  1354-02-01   output:  1354-01-02\n",
            "مرداد 22 1362\n",
            "input:  1362-05-22   output:  1362-02-22\n",
            "بیست و شش آذر 1378\n",
            "input:  1378-09-26   output:  1388-09-26\n",
            "دوشنبه مهر ماه ۳۱ هزار و سیصد و شصت و نه\n",
            "input:  1369-07-31   output:  1369-07-13\n",
            "03.10.87\n",
            "input:  1387-10-03   output:  1387-00-33\n",
            "5/17/71\n",
            "input:  1371-05-17   output:  1371-07-17\n",
            "دی ماه 2 هزار و سیصد و هشتاد و شش\n",
            "input:  1386-10-02   output:  1386-10-22\n",
            "1 04 94\n",
            "input:  1394-04-01   output:  1394-04-04\n",
            "19.10.52\n",
            "input:  1352-10-19   output:  1352-11-19\n",
            "اردیبهشت ماه 7 1395\n",
            "input:  1395-02-07   output:  1355-02-07\n",
            "جمعه مرداد 12 1358\n",
            "input:  1358-05-12   output:  1358-05-11\n",
            "23.05.89\n",
            "input:  1389-05-23   output:  1389-03-23\n",
            "17.05.59\n",
            "input:  1359-05-17   output:  1355-07-17\n",
            "01.02.56\n",
            "input:  1356-02-01   output:  1356-02-11\n",
            "چهار‌شنبه بهمن ماه 12 1355\n",
            "input:  1355-11-12   output:  1355-11-11\n",
            "12/9/81\n",
            "input:  1381-12-09   output:  1381-02-19\n",
            "سی بهمن ماه هزار و سیصد و هشتاد و یک\n",
            "input:  1381-11-30   output:  1381-11-03\n",
            "9/15/75\n",
            "input:  1375-09-15   output:  1355-09-19\n",
            "یک‌شنبه خرداد ماه ۶ ۱۳۹۶\n",
            "input:  1396-03-06   output:  1366-03-06\n",
            "20 بهمن هزار و سیصد و هشتاد و یک\n",
            "input:  1381-11-20   output:  1381-11-22\n",
            "۱۱/۲۶/۶۲\n",
            "input:  1362-11-26   output:  1362-12-16\n",
            "۲۴ ۰۷ ۵۹\n",
            "input:  1359-07-24   output:  1399-04-24\n",
            "10/12/59\n",
            "input:  1359-10-12   output:  1355-10-21\n",
            "۱۹.۰۴.۸۰\n",
            "input:  1380-04-19   output:  1380-09-19\n",
            "11/27/95\n",
            "input:  1395-11-27   output:  1395-12-17\n",
            "۸/۱۴/۵۲\n",
            "input:  1352-08-14   output:  1352-08-18\n",
            "11/7/79\n",
            "input:  1379-11-07   output:  1379-01-17\n",
            "۲۷.۰۴.۶۲\n",
            "input:  1362-04-27   output:  1362-07-24\n",
            "۱۲.۰۸.۶۶\n",
            "input:  1366-08-12   output:  1366-02-12\n",
            "19 10 72\n",
            "input:  1372-10-19   output:  1372-11-19\n",
            "سی آبان 1355\n",
            "input:  1355-08-30   output:  1355-08-33\n",
            "یک شنبه آذر ماه ۱۸ ۱۳۷۴\n",
            "input:  1374-09-18   output:  1374-09-19\n",
            "۱۷ ۸۶ ۸۶\n",
            "input:  1386-06-17   output:  1386-07-17\n",
            "سوم دی ماه هزار و سیصد و هفتاد و نه\n",
            "input:  1379-10-03   output:  1379-11-00\n",
            "۱/۲۸/۵۸\n",
            "input:  1358-01-28   output:  1388-02-18\n",
            "8/21/99\n",
            "input:  1399-08-21   output:  1399-02-22\n",
            "هفدهم شهریور ماه هزار و سیصد و نود و هفت\n",
            "input:  1397-06-17   output:  1397-01-17\n",
            "دی ماه ۱۳ هزار و سیصد و نود و هفت\n",
            "input:  1397-10-13   output:  1397-10-31\n",
            "سی دی هزار و سیصد و شصت و شش\n",
            "input:  1366-10-30   output:  1366-10-00\n",
            "سه شنبه مهر ماه ۳۱ ۱۳۶۴\n",
            "input:  1364-07-31   output:  1364-07-13\n",
            "آبان ماه ۳۱ ۱۳۸۸\n",
            "input:  1388-08-31   output:  1388-08-33\n",
            "12/11/60\n",
            "input:  1360-12-11   output:  1360-11-11\n",
            "۶/۲۰/۹۷\n",
            "input:  1397-06-20   output:  1377-09-26\n",
            "۳۱ فروردین ۱۳۷۹\n",
            "input:  1379-01-31   output:  1379-01-11\n",
            "۳۰ ۰۸ ۶۳\n",
            "input:  1363-08-30   output:  1383-08-30\n",
            "فروردین ماه ۳۰ شصت و چهار\n",
            "input:  1364-01-30   output:  1364-01-00\n",
            "9/21/77\n",
            "input:  1377-09-21   output:  1377-09-12\n",
            "یک اردیبهشت ماه 1392\n",
            "input:  1392-02-01   output:  1392-02-02\n",
            "پنجشنبه آبان ماه ۱۲ ۱۳۸۴\n",
            "input:  1384-08-12   output:  1384-08-11\n",
            "4/15/86\n",
            "input:  1386-04-15   output:  1386-15-14\n",
            "دو شنبه آذر ماه 1 هزار و سیصد و هشتاد و هشت\n",
            "input:  1388-09-01   output:  1388-09-11\n",
            "10.05.75\n",
            "input:  1375-05-10   output:  1355-05-10\n",
            "17 06 69\n",
            "input:  1369-06-17   output:  1369-07-17\n",
            "دهم اردیبهشت ۱۳۷۲\n",
            "input:  1372-02-10   output:  1372-02-12\n",
            "۱۵ ۰۸ ۵۵\n",
            "input:  1355-08-15   output:  1355-05-15\n",
            "دهم بهمن هزار و سیصد و هفتاد و پنج\n",
            "input:  1375-11-10   output:  1375-11-17\n",
            "سوم آبان ماه هزار و چهارصد\n",
            "input:  1400-08-03   output:  1400-08-02\n",
            "07.05.91\n",
            "input:  1391-05-07   output:  1391-07-05\n",
            "هفده آبان 1389\n",
            "input:  1389-08-17   output:  1389-08-18\n",
            "01 شهریور هزار و سیصد و پنجاه و هشت\n",
            "input:  1358-06-01   output:  1358-06-10\n",
            "دو‌شنبه فروردین ماه 12 1378\n",
            "input:  1378-01-12   output:  1378-01-21\n",
            "28 10 96\n",
            "input:  1396-10-28   output:  1396-08-28\n",
            "خرداد ماه ۱ هزار و سیصد و پنجاه و دو\n",
            "input:  1352-03-01   output:  1352-03-11\n",
            "۶/۱۲/۹۷\n",
            "input:  1397-06-12   output:  1377-12-16\n",
            "۰۱.۰۴.۶۱\n",
            "input:  1361-04-01   output:  1361-04-10\n",
            "فروردین ماه ۱۳ ۱۳۹۶\n",
            "input:  1396-01-13   output:  1396-01-03\n",
            "۰۲ مرداد هزار و سیصد و پنجاه و دو\n",
            "input:  1352-05-02   output:  1352-05-20\n",
            "آذر ماه 21 1392\n",
            "input:  1392-09-21   output:  1392-09-22\n",
            "۰۲ فروردین هفتاد و هفت\n",
            "input:  1377-01-02   output:  1377-01-20\n",
            "چهارشنبه آبان ماه ۱ ۱۳۹۸\n",
            "input:  1398-08-01   output:  1398-08-11\n",
            "۱۵ ۰۹ ۸۴\n",
            "input:  1384-09-15   output:  1384-05-15\n",
            "نوزدهم اردیبهشت ماه هپنجاه و یک\n",
            "input:  1351-02-19   output:  1351-02-18\n",
            "12 02 75\n",
            "input:  1375-02-12   output:  1375-02-22\n",
            "دوشنبه شهریور ماه 11 1364\n",
            "input:  1364-06-11   output:  1364-01-11\n",
            "بیست و سه بهمن ماه چهارصد\n",
            "input:  1400-11-23   output:  1300-11-20\n",
            "۲۱ ۰۱ ۰۰\n",
            "input:  1400-01-21   output:  1400-11-11\n",
            "فروردین ماه 4 هزار و چهارصد و یک\n",
            "input:  1401-01-04   output:  1300-01-04\n",
            "دو‌شنبه تیر ماه 7 1360\n",
            "input:  1360-04-07   output:  1360-07-07\n",
            "تیر ماه ۲۶ پنجاه و شش\n",
            "input:  1356-04-26   output:  1356-06-26\n",
            "شنبه آذر ماه 23 پنجاه و دو\n",
            "input:  1352-09-23   output:  1352-09-22\n",
            "یکشنبه فروردین ماه 13 1360\n",
            "input:  1360-01-13   output:  1360-01-11\n",
            "23 12 97\n",
            "input:  1397-12-23   output:  1397-12-22\n",
            "چهارشنبه اردیبهشت ماه 12 1377\n",
            "input:  1377-02-12   output:  1377-02-21\n",
            "2 بهمن 1395\n",
            "input:  1395-11-02   output:  1355-11-02\n",
            "۷/۲۵/۹۶\n",
            "input:  1396-07-25   output:  1396-02-27\n",
            "چهارشنبه تیر ماه 12 1375\n",
            "input:  1375-04-12   output:  1375-04-11\n",
            "دی ماه 12 1360\n",
            "input:  1360-10-12   output:  1360-10-11\n",
            "سی ام مهر ماه ۱۳۶۷\n",
            "input:  1367-07-30   output:  1367-07-33\n",
            "۱۱ ۰۱ ۹۰\n",
            "input:  1390-01-11   output:  1390-11-11\n",
            "چهارشنبه شهریور ماه ۲ ۱۳۵۱\n",
            "input:  1351-06-02   output:  1351-06-23\n",
            "16.09.65\n",
            "input:  1365-09-16   output:  1365-06-16\n",
            "دو شنبه دی ماه ۱ هزار و سیصد و شصت و چهار\n",
            "input:  1364-10-01   output:  1364-10-11\n",
            "۸/۲۵/۵۶\n",
            "input:  1356-08-25   output:  1356-05-25\n",
            "6 02 98\n",
            "input:  1398-02-06   output:  1398-09-06\n",
            "فروردین ۳۱ ۱۳۵۵\n",
            "input:  1355-01-31   output:  1355-01-13\n",
            "28 02 77\n",
            "input:  1377-02-28   output:  1377-08-28\n",
            "12/9/86\n",
            "input:  1386-12-09   output:  1386-02-19\n",
            "بهمن 21 هزار و سیصد و هفتاد و دو\n",
            "input:  1372-11-21   output:  1372-11-22\n",
            "08.10.71\n",
            "input:  1371-10-08   output:  1371-01-08\n",
            "۷ ۱۱ ۵۲\n",
            "input:  1352-11-07   output:  1352-11-17\n",
            "۳/۲۲/۷۱\n",
            "input:  1371-03-22   output:  1371-02-22\n",
            "تیر ماه 1 هزار و سیصد و شصت و نه\n",
            "input:  1369-04-01   output:  1369-04-11\n",
            "سه‌شنبه تیر ماه 12 1396\n",
            "input:  1396-04-12   output:  1396-04-11\n",
            "۰۵.۰۳.۸۹\n",
            "input:  1389-03-05   output:  1389-05-05\n",
            "۱۲/۲۷/۵۷\n",
            "input:  1357-12-27   output:  1375-12-27\n",
            "14 03 54\n",
            "input:  1354-03-14   output:  1354-04-14\n",
            "9/28/73\n",
            "input:  1373-09-28   output:  1373-08-29\n",
            "۱۹.۰۸.۷۳\n",
            "input:  1373-08-19   output:  1373-09-19\n",
            "12/7/78\n",
            "input:  1378-12-07   output:  1378-01-17\n",
            "یکشنبه مرداد 3 هزار و سیصد و شصت و هفت\n",
            "input:  1367-05-03   output:  1367-05-33\n",
            "دی 2 1387\n",
            "input:  1387-10-02   output:  1387-10-22\n",
            "3/6/92\n",
            "input:  1392-03-06   output:  1392-03-03\n",
            "۱/۳۰/۸۴\n",
            "input:  1384-01-30   output:  1384-03-33\n",
            "دو شنبه خرداد ماه ۱۰ ۱۳۹۴\n",
            "input:  1394-03-10   output:  1394-03-11\n",
            "شش مهر 1391\n",
            "input:  1391-07-06   output:  1391-07-03\n",
            "8/8/74\n",
            "input:  1374-08-08   output:  1370-08-08\n",
            "15 01 87\n",
            "input:  1387-01-15   output:  1387-11-15\n",
            "۱۳ مرداد هزار و سیصد و شصت و هشت\n",
            "input:  1368-05-13   output:  1368-05-31\n",
            "۲۲.۱۰.۹۵\n",
            "input:  1395-10-22   output:  1355-12-22\n",
            "یک‌شنبه مهر ماه ۳۱ هزار و سیصد و هفتاد و چهار\n",
            "input:  1374-07-31   output:  1374-07-13\n",
            "۳/۹۳/۹۳\n",
            "input:  1393-03-13   output:  1393-03-33\n",
            "25 01 59\n",
            "input:  1359-01-25   output:  1359-02-25\n",
            "20 خرداد هزار و سیصد و شصت و یک\n",
            "input:  1361-03-20   output:  1361-03-23\n",
            "26.07.54\n",
            "input:  1354-07-26   output:  1354-06-26\n",
            "پنج شنبه تیر ماه 12 هزار و سیصد و هشتاد و هفت\n",
            "input:  1387-04-12   output:  1387-04-11\n",
            "۲/۵/۶۶\n",
            "input:  1366-02-05   output:  1366-02-02\n",
            "04.06.93\n",
            "input:  1393-06-04   output:  1363-04-04\n",
            "03.01.52\n",
            "input:  1352-01-03   output:  1352-00-33\n",
            "دو‌شنبه خرداد ماه ۱۲ هزار و سیصد و هشتاد و یک\n",
            "input:  1381-03-12   output:  1381-03-11\n",
            "30.06.66\n",
            "input:  1366-06-30   output:  1366-06-00\n",
            "۳۱ فروردین ماه هزار و سیصد و هفتاد و یک\n",
            "input:  1371-01-31   output:  1371-01-11\n",
            "فروردین 17 1359\n",
            "input:  1359-01-17   output:  1355-01-17\n",
            "دو‌شنبه خرداد ماه ۱۳ ۱۳۶۹\n",
            "input:  1369-03-13   output:  1369-03-11\n",
            "یکشنبه اسفند ماه 2 1359\n",
            "input:  1359-12-02   output:  1359-12-22\n",
            "یکشنبه خرداد ماه 12 1380\n",
            "input:  1380-03-12   output:  1380-03-11\n",
            "۸/۲۱/۵۰\n",
            "input:  1350-08-21   output:  1350-02-22\n",
            "سیزده شهریور 1379\n",
            "input:  1379-06-13   output:  1379-06-11\n",
            "۱۳ اسفند ماه ۱۴۰۰\n",
            "input:  1400-12-13   output:  1400-12-11\n",
            "هشتم بهمن ماه ۱۳۷۰\n",
            "input:  1370-11-08   output:  1370-17-07\n",
            "سی ام آبان هشتاد و نه\n",
            "input:  1389-08-30   output:  1389-08-33\n",
            "هشتم فروردین هفتاد و دو\n",
            "input:  1372-01-08   output:  1372-01-07\n",
            "چهارشنبه آذر ماه ۱ ۱۳۶۲\n",
            "input:  1362-09-01   output:  1362-09-11\n",
            "27.03.92\n",
            "input:  1392-03-27   output:  1392-07-27\n",
            "10/24/53\n",
            "input:  1353-10-24   output:  1353-14-24\n",
            "۸/۱/۶۲\n",
            "input:  1362-08-01   output:  1362-08-02\n",
            "نوزدهم اردیبهشت ۱۳۶۷\n",
            "input:  1367-02-19   output:  1367-02-16\n",
            "۱/۲۹/۸۴\n",
            "input:  1384-01-29   output:  1384-02-12\n",
            "21.11.96\n",
            "input:  1396-11-21   output:  1396-11-11\n",
            "7/22/86\n",
            "input:  1386-07-22   output:  1386-02-27\n",
            "۱۰.۰۸.۶۸\n",
            "input:  1368-08-10   output:  1388-08-10\n",
            "۶/۱۴/۶۵\n",
            "input:  1365-06-14   output:  1365-06-16\n",
            "۰۱ اسفند ۱۳۵۵\n",
            "input:  1355-12-01   output:  1355-12-10\n",
            "25 10 71\n",
            "input:  1371-10-25   output:  1371-01-25\n",
            "هشت دی ماه نود و هفت\n",
            "input:  1397-10-08   output:  1397-10-09\n",
            "نهم دی ماه هزار و سیصد و شصت\n",
            "input:  1360-10-09   output:  1360-10-19\n",
            "دوازده آذر 1391\n",
            "input:  1391-09-12   output:  1391-09-11\n",
            "سه شنبه فروردین ماه 9 1359\n",
            "input:  1359-01-09   output:  1399-01-09\n",
            "7 03 79\n",
            "input:  1379-03-07   output:  1379-07-00\n",
            "دوم مهر ماه ۱۳۹۰\n",
            "input:  1390-07-02   output:  1390-07-00\n",
            "شهریور ماه 13 نود و هفت\n",
            "input:  1397-06-13   output:  1397-06-33\n",
            "جمعه مهر ماه 1 هزار و سیصد و شصت و سه\n",
            "input:  1363-07-01   output:  1363-07-11\n",
            "جمعه دی ماه ۲ ۱۴۰۰\n",
            "input:  1400-10-02   output:  1400-10-20\n",
            "19.08.76\n",
            "input:  1376-08-19   output:  1366-09-19\n",
            "دو شنبه آبان ماه ۱۹ هزار و سیصد و هشتاد و دو\n",
            "input:  1382-08-19   output:  1382-09-19\n",
            "۰۳.۰۸.۹۰\n",
            "input:  1390-08-03   output:  1390-08-30\n",
            "یکشنبه شهریور ماه 12 شصت و سه\n",
            "input:  1363-06-12   output:  1363-06-11\n",
            "18.09.64\n",
            "input:  1364-09-18   output:  1364-08-18\n",
            "جمعه خرداد ماه ۱ ۱۳۷۶\n",
            "input:  1376-03-01   output:  1376-03-11\n",
            "دوشنبه آذر ماه ۱ ۱۳۷۷\n",
            "input:  1377-09-01   output:  1377-09-11\n",
            "10 02 51\n",
            "input:  1351-02-10   output:  1351-00-00\n",
            "6 06 76\n",
            "input:  1376-06-06   output:  1366-06-06\n",
            "بیست و سوم مرداد ۱۴۰۰\n",
            "input:  1400-05-23   output:  1400-05-22\n",
            "۱۳.۱۲.۶۱\n",
            "input:  1361-12-13   output:  1361-11-03\n",
            "بیست و یکم دی ۱۳۸۶\n",
            "input:  1386-10-21   output:  1386-11-21\n",
            "۹ ۱۱ ۹۷\n",
            "input:  1397-11-09   output:  1397-01-19\n",
            "شنبه بهمن ماه ۲۱ ۱۳۷۸\n",
            "input:  1378-11-21   output:  1378-11-22\n",
            "۱۵ ۰۶ ۷۶\n",
            "input:  1376-06-15   output:  1366-06-15\n",
            "دو‌شنبه اردیبهشت ماه 15 هزار و سیصد و هفتاد و نه\n",
            "input:  1379-02-15   output:  1372-02-15\n",
            "۴/۲۹/۸۳\n",
            "input:  1383-04-29   output:  1383-02-24\n",
            "9/13/98\n",
            "input:  1398-09-13   output:  1398-09-19\n",
            "چهار‌شنبه اسفند ماه 2 1372\n",
            "input:  1372-12-02   output:  1372-12-22\n",
            "خرداد ماه 15 هزار و سیصد و نود و یک\n",
            "input:  1391-03-15   output:  1391-05-15\n",
            "3/5/72\n",
            "input:  1372-03-05   output:  1372-03-03\n",
            "03.07.53\n",
            "input:  1353-07-03   output:  1353-07-00\n",
            "۰۱.۰۱.۱۵\n",
            "input:  1396-01-01   output:  1351-11-00\n",
            "۰۷.۱۱.۹۲\n",
            "input:  1392-11-07   output:  1391-11-17\n",
            "5/27/78\n",
            "input:  1378-05-27   output:  1378-08-27\n",
            "۱۶.۰۶.۸۹\n",
            "input:  1389-06-16   output:  1369-06-16\n",
            "10/6/73\n",
            "input:  1373-10-06   output:  1373-00-00\n",
            "20.81.81\n",
            "input:  1381-01-20   output:  1381-01-28\n",
            "۰۱.۰۵.۶۵\n",
            "input:  1365-05-01   output:  1355-05-10\n",
            "خرداد ۳۱ هزار و سیصد و پنجاه و دو\n",
            "input:  1352-03-31   output:  1352-03-33\n",
            "21 مهر 1395\n",
            "input:  1395-07-21   output:  1395-07-11\n",
            "بیست و هشت اردیبهشت ماه هزار و سیصد و هفتاد و سه\n",
            "input:  1373-02-28   output:  1383-02-28\n",
            "بیست و ششم آبان ماه ۱۳۷۶\n",
            "input:  1376-08-26   output:  1366-08-26\n",
            "یک‌شنبه فروردین ماه ۲ ۱۳۷۴\n",
            "input:  1374-01-02   output:  1374-01-22\n",
            "۰۳.۰۸.۸۵\n",
            "input:  1385-08-03   output:  1385-08-00\n",
            "سهشنبه دی ماه ۳ ۱۳۶۹\n",
            "input:  1369-10-03   output:  1369-10-33\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7954"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 332
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpU0RFSsjtLB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}